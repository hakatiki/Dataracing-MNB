{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni_D83AE5_uni (Baseline)\n",
    "#   {\"Logloss\": 0.02470931219446125, \"ROCAUC\": 0.9810926019123885}\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy.stats import gamma, kstest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    log_loss, \n",
    "    roc_auc_score, \n",
    "    roc_curve, \n",
    "    auc\n",
    ")\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')  # Use this line to suppress warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "training_data = pd.read_csv('training_data.csv')\n",
    "training_data = training_data[training_data['BORROWER_ID'] != 'xNullx']\n",
    "training_data = training_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data_submission_example = pd.read_csv('data_submission_example.csv')\n",
    "\n",
    "lognormal_variables = [\n",
    "    'CONTRACT_CREDIT_LOSS', 'CONTRACT_DEPT_SERVICE_TO_INCOME',\n",
    "    'CONTRACT_INCOME', 'CONTRACT_INSTALMENT_AMOUNT', 'CONTRACT_INSTALMENT_AMOUNT_2',\n",
    "    'CONTRACT_LOAN_AMOUNT', 'CONTRACT_MARKET_VALUE', 'CONTRACT_MORTGAGE_LENDING_VALUE', \n",
    "    'CONTRACT_LGD', 'CONTRACT_INCOME'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ADDITION 1\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "\n",
    "# numeric_columns = training_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# excluded_keywords = ['TARGET', 'event', 'binary', 'DATE', 'DAYS', 'YEARS', 'MATURITY', 'DAY', 'BORROWER']\n",
    "\n",
    "# X_columns = [col for col in numeric_columns if all(keyword.lower() not in col.lower() for keyword in excluded_keywords)]\n",
    "# X_columns.remove('CONTRACT_TYPE_OF_INTEREST_REPAYMENT')\n",
    "# X_columns.remove('CONTRACT_MORTGAGE_TYPE')\n",
    "# df_copy = training_data.copy()\n",
    "# print(df_copy.isna().sum().sum())\n",
    "# min_values = []\n",
    "# for i in X_columns:\n",
    "#     if i in lognormal_variables:\n",
    "#         min_values.append(1)\n",
    "#     else:\n",
    "#         min_values.append(-np.inf)\n",
    "# imputer = IterativeImputer(max_iter=30, random_state=42, min_value=min_values,)\n",
    "# imputed_values = imputer.fit_transform(df_copy[X_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ADDITION 2\n",
    "# imputed_df = pd.DataFrame(imputed_values, columns=X_columns)\n",
    "\n",
    "# for col in X_columns:\n",
    "#     if imputed_df[col].dtype == 'float64':\n",
    "#         imputed_df[col] = imputed_df[col].astype('float32')\n",
    "#     elif imputed_df[col].dtype == 'int64':\n",
    "#         imputed_df[col] = imputed_df[col].astype('int32')\n",
    "\n",
    "# training_data.update(imputed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.fillna(0, inplace=True)\n",
    "for col in training_data.columns:\n",
    "    try:\n",
    "        training_data[col] = pd.to_numeric(training_data[col], errors='ignore')\n",
    "    except:\n",
    "        continue\n",
    "training_data['TARGET_EVENT_BINARY'] = np.where(training_data['TARGET_EVENT'] == 'K', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CONTRACT_ID                            0\n",
       "BORROWER_ID                            0\n",
       "CONTRACT_BANK_ID                       0\n",
       "CONTRACT_CREDIT_INTERMEDIARY           0\n",
       "CONTRACT_CREDIT_LOSS                   0\n",
       "CONTRACT_CURRENCY                      0\n",
       "CONTRACT_DATE_OF_LOAN_AGREEMENT        0\n",
       "CONTRACT_DEPT_SERVICE_TO_INCOME        0\n",
       "CONTRACT_FREQUENCY_TYPE                0\n",
       "CONTRACT_INCOME                        0\n",
       "CONTRACT_INSTALMENT_AMOUNT             0\n",
       "CONTRACT_INSTALMENT_AMOUNT_2           0\n",
       "CONTRACT_INTEREST_PERIOD               0\n",
       "CONTRACT_INTEREST_RATE                 0\n",
       "CONTRACT_LGD                           0\n",
       "CONTRACT_LOAN_AMOUNT                   0\n",
       "CONTRACT_LOAN_CONTRACT_TYPE            0\n",
       "CONTRACT_LOAN_TO_VALUE_RATIO           0\n",
       "CONTRACT_LOAN_TYPE                     0\n",
       "CONTRACT_MARKET_VALUE                  0\n",
       "CONTRACT_MATURITY_DATE                 0\n",
       "CONTRACT_MORTGAGE_LENDING_VALUE        0\n",
       "CONTRACT_MORTGAGE_TYPE                 0\n",
       "CONTRACT_REFINANCED                    0\n",
       "CONTRACT_RISK_WEIGHTED_ASSETS          0\n",
       "CONTRACT_TYPE_OF_INTEREST_REPAYMENT    0\n",
       "BORROWER_BIRTH_YEAR                    0\n",
       "BORROWER_CITIZENSHIP                   0\n",
       "BORROWER_COUNTRY                       0\n",
       "BORROWER_COUNTY                        0\n",
       "BORROWER_TYPE_OF_CUSTOMER              0\n",
       "BORROWER_TYPE_OF_SETTLEMENT            0\n",
       "TARGET_EVENT                           0\n",
       "TARGET_EVENT_DAY                       0\n",
       "TARGET_EVENT_BINARY                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['TARGET_EVENT_E'] = np.where(training_data['TARGET_EVENT'] == 'E', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TARGET_EVENT_BINARY\n",
       "0    1590792\n",
       "1      10871\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['TARGET_EVENT_BINARY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date_variables = ['CONTRACT_DATE_OF_LOAN_AGREEMENT', 'CONTRACT_MATURITY_DATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['TARGET_EVENT_DAY'].replace(0.0, np.nan, inplace=True)\n",
    "training_data['TARGET_EVENT_DAY_JULIAN'] = pd.to_datetime(training_data['TARGET_EVENT_DAY'], origin='julian', unit='D', errors='coerce')\n",
    "training_data['TARGET_EVENT_DAY_DATETIME'] = pd.to_datetime(training_data['TARGET_EVENT_DAY_JULIAN'],  errors='coerce')\n",
    "\n",
    "training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN'] = pd.to_datetime(training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT'], origin='julian', unit='D')\n",
    "training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] = pd.to_datetime(training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN'],)\n",
    "\n",
    "training_data['CONTRACT_MATURITY_DATE_JULIAN'] = pd.to_datetime(training_data['CONTRACT_MATURITY_DATE'], origin='julian', unit='D')\n",
    "training_data['CONTRACT_MATURITY_DATE_DATETIME'] = pd.to_datetime(training_data['CONTRACT_MATURITY_DATE_JULIAN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['DAY_DIFF'] = (training_data['TARGET_EVENT_DAY_DATETIME'] - training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME']).dt.days\n",
    "training_data['DAYS_TO_END'] = (pd.Timestamp(\"2020-01-01\")- training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME']).dt.days\n",
    "training_data['YEARS_TO_END'] = training_data['DAYS_TO_END'] / 365\n",
    "training_data['DAYS_TO_2018'] = (pd.Timestamp(\"2018-01-01\")- training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME']).dt.days\n",
    "training_data['YEARS_TO_2018'] = training_data['DAYS_TO_2018'] / 365\n",
    "training_data['TIME_TO_MATURITY_DAYS'] = (training_data['CONTRACT_MATURITY_DATE']-training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT'])\n",
    "training_data['2020_OR_MATURITY'] = np.minimum(training_data['TIME_TO_MATURITY_DAYS'], training_data['DAYS_TO_END'])\n",
    "training_data['2020_OR_MATURITY_YEARS'] = training_data['2020_OR_MATURITY'] / 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_target_column(dataframe, column_name, event, day_diff_upper_limit):\n",
    "    dataframe[column_name] = np.where(\n",
    "        (dataframe['TARGET_EVENT'] == event) & \n",
    "        (dataframe['DAY_DIFF'] <= day_diff_upper_limit) & \n",
    "        (dataframe['DAY_DIFF'] >= 0), \n",
    "        1, \n",
    "        0\n",
    "    )\n",
    "\n",
    "timeframes = {\n",
    "    'TARGET_EVENT_BINARY_2Y': 730,\n",
    "    'TARGET_EVENT_BINARY_1Y': 365,\n",
    "    'TARGET_EVENT_BINARY_6M': 365//2,\n",
    "}\n",
    "\n",
    "for column_name, days in timeframes.items():\n",
    "    create_binary_target_column(training_data, column_name, 'K', days)\n",
    "\n",
    "\n",
    "# target_event_binary_columns = []\n",
    "# for i in range(6):\n",
    "#     start_day = 273 + i * 30\n",
    "#     column_header = 'TARGET_EVENT_BINARY_' + str(start_day) + 'D'\n",
    "#     training_data[column_header] = np.where(\n",
    "#         (training_data['TARGET_EVENT'] == 'K') & \n",
    "#         (training_data['DAY_DIFF'] <= start_day) & \n",
    "#         (training_data['DAY_DIFF'] >= 0), \n",
    "#         1, \n",
    "#         0\n",
    "#     )\n",
    "#     target_event_binary_columns.append(column_header)\n",
    "\n",
    "training_data.drop('DAY_DIFF', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET_EVENT_BINARY\n",
      "0    1590792\n",
      "1      10871\n",
      "Name: count, dtype: int64\n",
      "TARGET_EVENT_BINARY_2Y\n",
      "0    1591751\n",
      "1       9912\n",
      "Name: count, dtype: int64\n",
      "TARGET_EVENT_BINARY_1Y\n",
      "0    1596927\n",
      "1       4736\n",
      "Name: count, dtype: int64\n",
      "TARGET_EVENT_BINARY_6M\n",
      "0    1601267\n",
      "1        396\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(training_data['TARGET_EVENT_BINARY'].value_counts())\n",
    "print(training_data['TARGET_EVENT_BINARY_2Y'].value_counts())\n",
    "print(training_data['TARGET_EVENT_BINARY_1Y'].value_counts())\n",
    "print(training_data['TARGET_EVENT_BINARY_6M'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CONTRACT_CREDIT_INTERMEDIARY',\n",
       " 'CONTRACT_CREDIT_LOSS',\n",
       " 'CONTRACT_CURRENCY',\n",
       " 'CONTRACT_DEPT_SERVICE_TO_INCOME',\n",
       " 'CONTRACT_INCOME',\n",
       " 'CONTRACT_INSTALMENT_AMOUNT',\n",
       " 'CONTRACT_INSTALMENT_AMOUNT_2',\n",
       " 'CONTRACT_INTEREST_PERIOD',\n",
       " 'CONTRACT_INTEREST_RATE',\n",
       " 'CONTRACT_LGD',\n",
       " 'CONTRACT_LOAN_AMOUNT',\n",
       " 'CONTRACT_LOAN_CONTRACT_TYPE',\n",
       " 'CONTRACT_LOAN_TO_VALUE_RATIO',\n",
       " 'CONTRACT_MARKET_VALUE',\n",
       " 'CONTRACT_MORTGAGE_LENDING_VALUE',\n",
       " 'CONTRACT_MORTGAGE_TYPE',\n",
       " 'CONTRACT_REFINANCED',\n",
       " 'CONTRACT_RISK_WEIGHTED_ASSETS',\n",
       " 'CONTRACT_TYPE_OF_INTEREST_REPAYMENT',\n",
       " 'BORROWER_BIRTH_YEAR',\n",
       " 'BORROWER_CITIZENSHIP',\n",
       " 'BORROWER_COUNTRY',\n",
       " 'BORROWER_COUNTY',\n",
       " 'BORROWER_TYPE_OF_SETTLEMENT']"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_columns = training_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "excluded_keywords = ['TARGET', 'event', 'binary', 'DATE', 'DAYS', 'YEARS', 'MATURITY', 'DAY']\n",
    "\n",
    "X_columns = [col for col in numeric_columns if all(keyword.lower() not in col.lower() for keyword in excluded_keywords)]\n",
    "y_column = 'TARGET_EVENT_BINARY_2Y' \n",
    "X_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy_financial as npf\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_monthly_payment(row):\n",
    "    annual_interest_rate = row['CONTRACT_INTEREST_RATE'] / 100  # Convert percentage to decimal\n",
    "    monthly_interest_rate = annual_interest_rate / 12\n",
    "    term_in_months = ((pd.to_datetime(row['CONTRACT_MATURITY_DATE_DATETIME'], format='%d/%m/%Y') - \n",
    "                       pd.to_datetime(row['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'], format='%d/%m/%Y'))).days/30\n",
    "    principal = row['CONTRACT_LOAN_AMOUNT']\n",
    "    monthly_payment = npf.pmt(monthly_interest_rate, term_in_months, -principal)\n",
    "    return monthly_payment\n",
    "# ADDITION 3:\n",
    "\n",
    "# X_columns.append('TIME_TO_MATURITY_DAYS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_type_dummies = pd.get_dummies(training_data['CONTRACT_LOAN_TYPE'], prefix='LOAN_TYPE', drop_first=True)\n",
    "frequency_type_dummies = pd.get_dummies(training_data['CONTRACT_FREQUENCY_TYPE'], prefix='FREQ_TYPE', drop_first=True)\n",
    "interest_type_dummies = pd.get_dummies(training_data['CONTRACT_TYPE_OF_INTEREST_REPAYMENT'], prefix='INTEREST_TYPE', drop_first=True)\n",
    "mortgage_type_dummies =  pd.get_dummies(training_data['CONTRACT_MORTGAGE_TYPE'], prefix='MORTGAGE_TYPE', drop_first=True)\n",
    "\n",
    "training_data = pd.concat([training_data, loan_type_dummies, frequency_type_dummies,interest_type_dummies,mortgage_type_dummies ], axis=1)\n",
    "\n",
    "X_columns.extend(loan_type_dummies.columns)\n",
    "X_columns.extend(frequency_type_dummies.columns)\n",
    "X_columns.extend(interest_type_dummies.columns)\n",
    "X_columns.extend(mortgage_type_dummies.columns)\n",
    "# X_columns.remove('CONTRACT_LOAN_TYPE')\n",
    "# X_columns.remove('CONTRACT_FREQUENCY_TYPE')\n",
    "X_columns.remove('CONTRACT_TYPE_OF_INTEREST_REPAYMENT')\n",
    "X_columns.remove('CONTRACT_MORTGAGE_TYPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data.loc[training_data['BORROWER_CITIZENSHIP'] == 0, 'BORROWER_CITIZENSHIP'] = 98\n",
    "# citizenship_type_dummies = pd.get_dummies(training_data['BORROWER_CITIZENSHIP'], prefix='CITIZENSHIP_TYPE', drop_first=True)\n",
    "# training_data = pd.concat([training_data, citizenship_type_dummies ], axis=1)\n",
    "# X_columns.extend(citizenship_type_dummies.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['BORROWER_LOAN_COUNT'] = training_data.groupby('BORROWER_ID')['BORROWER_ID'].transform('count')\n",
    "training_data['LOAN_BORROWER_COUNT'] = training_data.groupby('CONTRACT_ID')['CONTRACT_ID'].transform('count')\n",
    "training_data['TOTAL_LOAN_AMOUNT'] = training_data.groupby('BORROWER_ID')['CONTRACT_LOAN_AMOUNT'].transform('sum')\n",
    "training_data['TOTAL_INSTALLMENT_AMOUNT_1'] = training_data.groupby('BORROWER_ID')['CONTRACT_INSTALMENT_AMOUNT'].transform('sum')\n",
    "training_data['TOTAL_INSTALLMENT_AMOUNT_2'] = training_data.groupby('BORROWER_ID')['CONTRACT_INSTALMENT_AMOUNT_2'].transform('sum')\n",
    "training_data['TOTAL_INSTALLMENT_AMOUNT'] = training_data['TOTAL_INSTALLMENT_AMOUNT_1'] + training_data['TOTAL_INSTALLMENT_AMOUNT_2']\n",
    "\n",
    "X_columns.extend(['BORROWER_LOAN_COUNT', 'TOTAL_LOAN_AMOUNT','TOTAL_INSTALLMENT_AMOUNT','LOAN_BORROWER_COUNT'])\n",
    "lognormal_variables.extend([ 'TOTAL_LOAN_AMOUNT','TOTAL_INSTALLMENT_AMOUNT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables removed: {'BORROWER_COUNTRY', 'CONTRACT_REFINANCED', 'FREQ_TYPE_2f88e16c'}\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.85\n",
    "correlation_matrix = training_data[X_columns].corr()\n",
    "highly_correlated_set = set()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            highly_correlated_set.add(colname)\n",
    "\n",
    "X_columns = [col for col in X_columns if col not in highly_correlated_set]\n",
    "print('Variables removed:', highly_correlated_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_percentage_generator_2016(df, percentage, target, random_sample=42):\n",
    "    df_copy = df.copy()\n",
    "    df_filtered = df_copy[df_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] < '2016-01-01']\n",
    "    df_mean = df_filtered[target].mean()\n",
    "    print(f\"Mean in year {df_mean}\")\n",
    "    df_defautled = df_filtered[target].sum()\n",
    "    df_not_defaulted = len(df_filtered) - df_defautled\n",
    "\n",
    "    required_val = (df_defautled - percentage * len(df_filtered))/(percentage - 1)\n",
    "    \n",
    "    df_filtered_after = df_copy[df_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] > '2016-01-01']\n",
    "    df_filtered_after = df_filtered_after[df_filtered_after[target]==1] \n",
    "    print(len(df_filtered_after))\n",
    "    print(required_val)\n",
    "    required_val = min(int(required_val) ,len(df_filtered_after))\n",
    "    df_filtered_after = df_filtered_after.sample(n=int(required_val),random_state=random_sample).reset_index(drop=True)\n",
    "\n",
    "    df_filtered = pd.concat([df_filtered, df_filtered_after])\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def default_percentage_generator_2016_maximal(df, percentage, target):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    defaulted = df_copy[(df_copy[target]==1) & (df_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] < '2018-01-01')]\n",
    "    total_needed = len(defaulted) / percentage * 100 - len(defaulted)\n",
    "\n",
    "\n",
    "    # print(len(defaulted))\n",
    "    # print(total_needed)\n",
    "\n",
    "    df_filtered_2016 = df_copy[(df_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] < '2016-01-01')& (df_copy[target]==0)]\n",
    "    # print(len(df_filtered_2016))\n",
    "    # print(total_needed)\n",
    "    max_needed = min(total_needed,len(df_filtered_2016))\n",
    "    df_filtered_2016 = df_filtered_2016.sample(n=int(max_needed),random_state=42).reset_index(drop=True)\n",
    "\n",
    "    extra_needed = total_needed - len(df_filtered_2016)\n",
    "\n",
    "    df_filtered_after = df_copy[df_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] > '2016-01-01']\n",
    "    df_filtered_after = df_filtered_after[df_filtered_after[target]==0]\n",
    "    df_filtered_after = df_filtered_after.sample(n=int(extra_needed),random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    df_filtered = pd.concat([df_filtered_2016, df_filtered_after,defaulted])\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# default_percentage_generator_2016_maximal(training_data, 1.48, 'TARGET_EVENT_BINARY')['TARGET_EVENT_BINARY'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities(data, column, time_factor):\n",
    "    lambdas = -np.log(1 - data[column]) / time_factor\n",
    "    probs_2y = 1 - np.exp(-2 * lambdas)\n",
    "    return probs_2y\n",
    "def calculate_probabilities_vec(data, time_factor):\n",
    "    lambdas = -np.log(1 - data) / time_factor\n",
    "    probs_2y = 1 - np.exp(-2 * lambdas)\n",
    "    return probs_2y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "\n",
    "def train_and_predict_two_halves(df, variables, target, model=LogisticRegression(), \n",
    "                                 scaler=StandardScaler(), augment_distribution=True,calibrate=True,\n",
    "                                 augment_distribution_percentage = 1.48, unique_loans=False,\n",
    "                                 should_smote =False,maximal_sample=False,random_sample=42, \n",
    "                                 show_curve=False, calib_method = 'isotonic'):\n",
    "    if lognormal_variables is not None:\n",
    "        df = df.copy()\n",
    "        \n",
    "        for var in lognormal_variables:\n",
    "            if var == 'CONTRACT_CREDIT_LOSS':\n",
    "                df[var] = np.log1p(np.abs(df[var]))*np.sign(df[var])\n",
    "            else:\n",
    "                df[var] = np.log1p(df[var])\n",
    "    if augment_distribution and not maximal_sample:\n",
    "        df_filtered = default_percentage_generator_2016(df, augment_distribution_percentage/100, target, random_sample)\n",
    "    else:\n",
    "        df_filtered = df[df['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] < '2016-01-01']\n",
    "    \n",
    "    if maximal_sample:\n",
    "        df_filtered = default_percentage_generator_2016_maximal(df, augment_distribution_percentage, target)\n",
    "        print(df_filtered[target].mean())\n",
    "\n",
    "    if unique_loans:\n",
    "        df_filtered = df_filtered.drop_duplicates(subset=['CONTRACT_ID'])\n",
    "\n",
    "    \n",
    "    X_scaled = scaler.fit_transform(df[variables])\n",
    "    X_filtered = scaler.transform(df_filtered[variables])\n",
    "\n",
    "    y = df[target] \n",
    "    y_filtered = df_filtered[target] \n",
    "    if should_smote:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_filtered, y_filtered = smote.fit_resample(X_filtered, y_filtered)\n",
    "\n",
    "    if calibrate:\n",
    "        model = CalibratedClassifierCV(base_estimator=model, method=calib_method, )\n",
    "\n",
    "    model.fit(X_filtered, y_filtered)\n",
    "    \n",
    "\n",
    "    print(\"Logloss:\")\n",
    "    test_proba = model.predict_proba(X_filtered)\n",
    "    print(log_loss(y_filtered,test_proba))\n",
    "    proba = model.predict_proba(X_scaled)[:, 1]\n",
    "    # Additional code for ROC AUC score\n",
    "    roc_auc = roc_auc_score(y_filtered,test_proba[:,1])\n",
    "    print(\"ROC AUC Score:\", roc_auc)\n",
    "    if show_curve:\n",
    "        true_probas, predicted_probas = calibration_curve(y_filtered, test_proba[:, 1], n_bins=10)\n",
    "\n",
    "        # Plotting the calibration curve\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(predicted_probas, true_probas, marker='o')\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Perfect calibration line\n",
    "        plt.xlabel('Mean Predicted Probability')\n",
    "        plt.ylabel('Fraction of Positives')\n",
    "        plt.title('Calibration Curve')\n",
    "        plt.show()\n",
    "\n",
    "    return proba, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significant_features(df, variables, target, model1=LogisticRegression(), scaler=StandardScaler()):\n",
    "    if lognormal_variables is not None:\n",
    "        df = df.copy()\n",
    "        print(df[variables].isna().sum().sum())\n",
    "        \n",
    "        for var in lognormal_variables:\n",
    "            if var == 'CONTRACT_CREDIT_LOSS':\n",
    "                df[var] = np.log1p(np.abs(df[var]))*np.sign(df[var])\n",
    "            else:\n",
    "                df[var] = np.log1p(df[var])\n",
    "    print(np.isinf(df[variables]).sum().sum())\n",
    "    X_scaled = scaler.fit_transform(df[variables])\n",
    "    print(np.isinf(X_scaled).sum().sum())\n",
    "    print(pd.DataFrame(X_scaled, columns=variables).isna().sum().sum())\n",
    "    X = sm.add_constant(pd.DataFrame(X_scaled, columns=variables))\n",
    "    y = df[target]\n",
    "    model = sm.Logit(y, X).fit()\n",
    "    print(model.summary())\n",
    "    p_values = model.pvalues\n",
    "    non_significant_vars = p_values[p_values > 0.05].index.tolist()\n",
    "\n",
    "    return model, non_significant_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_probability(s):\n",
    "    if len(s) == 2:\n",
    "        p_a, p_b = s.values\n",
    "        return p_a + p_b - p_a * p_b #- (-3.2357827075016176e-05)\n",
    "    else:\n",
    "        return 1 - np.prod(1 - s.values)\n",
    "\n",
    "def create_submission_file(df_preds, target, example, filename='submission.csv', testing=False):\n",
    "    # Filter the data to only include BORROWER_IDs that are in the submission example\n",
    "    df_preds.loc[df_preds['TARGET_EVENT'] == 'E', target] = 0\n",
    "\n",
    "    print(\"Logloss:\")\n",
    "    print(log_loss(df_preds['TARGET_EVENT_BINARY'], df_preds[target]))\n",
    "\n",
    "    filtered_training_data = df_preds[df_preds['BORROWER_ID'].isin(example['BORROWER_ID'])]\n",
    "\n",
    "    # Print warning if the row count is off\n",
    "    if not testing and len(filtered_training_data) != 1564601:\n",
    "        print('WARNING: The filtered data does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "        # raise ValueError('WARNING: The submission file does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "    # Group by BORROWER_ID and calculate the combined probability\n",
    "\n",
    "    #######################x########################\n",
    "    #CUTTING TAILS DID NOT SEEM TO WORK\n",
    "    #######################x########################\n",
    "    # filtered_training_data = cut_exponential_tails(filtered_training_data, target)\n",
    "    grouped_data = filtered_training_data.groupby('BORROWER_ID')[target].apply(combined_probability).reset_index()\n",
    "\n",
    "    # Create the submission DataFrame\n",
    "    df_submission = pd.DataFrame()\n",
    "    df_submission['BORROWER_ID'] = grouped_data['BORROWER_ID']\n",
    "    df_submission['PRED'] = grouped_data[target]\n",
    "    print('Before centering:')\n",
    "    print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "\n",
    "    print('Centering probabilities...')\n",
    "    # Center the probabilities around 1.48%\n",
    "    desired_mean = 0.0148  # 1.48% as a decimal\n",
    "    # while (df_submission['PRED'].max() > 1 or df_submission['PRED'].min() < 0 or abs(df_submission['PRED'].mean() -0.0148) > 0.0005):\n",
    "    #     # print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "    #     df_submission['PRED'] = df_submission['PRED'].clip(lower=0, upper=1)\n",
    "    #     # print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "\n",
    "    #     current_mean = df_submission['PRED'].mean()\n",
    "    #     adjustment_factor = desired_mean  - current_mean\n",
    "    #     df_submission['PRED'] += adjustment_factor\n",
    "    initial_guess = 2\n",
    "    probas_unscaled = df_submission['PRED'].values\n",
    "    new_proba = probas_unscaled.copy()\n",
    "    while abs(new_proba.mean() - desired_mean) > 0.00001:\n",
    "        \n",
    "        new_proba = calculate_probabilities_vec(probas_unscaled, initial_guess)\n",
    "        error = new_proba.mean() - desired_mean\n",
    "        if error > 0:\n",
    "            initial_guess += 0.001\n",
    "        else:\n",
    "            initial_guess -= 0.001\n",
    "        print(error, initial_guess)\n",
    "    df_submission['PRED'] = new_proba\n",
    "        \n",
    "        \n",
    "    \n",
    "    print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "    # Save the submission file\n",
    "    if  not testing and filename is not None:\n",
    "        df_submission.to_csv(filename, index=False)\n",
    "    print(f'Saved file: {filename}')\n",
    "    # if abs(df_submission['PRED'].mean() -0.0148) > 0.0005:\n",
    "    #    raise ValueError('WARNING: mean is bad')\n",
    "        \n",
    "    # Print warning if the row count is off\n",
    "    if not testing and len(df_submission) != 1117674:\n",
    "        print('WARNING: The submission file does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "        # raise ValueError('WARNING: The submission file does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "        \n",
    "    return df_submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_2y_1y(df, variables, target, model1=LogisticRegression(), model2=LogisticRegression()):\n",
    "    df = df.copy()\n",
    "    start_date = pd.Timestamp('2015-01-01')\n",
    "    end_date = pd.Timestamp('2017-01-01')\n",
    "\n",
    "    # Mask for rows with CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN between start_date and end_date\n",
    "    mask_date_range = (df['CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN'] >= start_date) & (df['CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN'] <= end_date)\n",
    "\n",
    "    df = df[mask_date_range]\n",
    "\n",
    "    probs = train_and_predict_two_halves(\n",
    "        df, \n",
    "        variables, \n",
    "        target, \n",
    "        model1=model1,\n",
    "        model2=model2,\n",
    "    )\n",
    "    df['2Y-1Y-PROBS'] = probs\n",
    "    \n",
    "    test_data = pd.read_csv('./data/1y-test.csv')\n",
    "\n",
    "\n",
    "    df_submission = create_submission_file(df, '2Y-1Y-PROBS', test_data, filename=None, testing=True)\n",
    "\n",
    "    merged_df = pd.merge(test_data, df_submission, on='BORROWER_ID')\n",
    "    true_labels = merged_df['TARGET_EVENT_BINARY_1Y']\n",
    "    predicted_probs = merged_df['PRED']\n",
    "    logloss = log_loss(true_labels, predicted_probs)\n",
    "\n",
    "    print(f'Log loss: {logloss}')\n",
    "        \n",
    "    return logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_yearly_proba(data, proba, targets =[ 0.0052, 0.0099, 0.0185], logging=False):\n",
    "    data = data.copy()\n",
    "    probs = data[proba]\n",
    "    starter_scales=[2.8, 1.2, 0.75]\n",
    "    new_proba = np.zeros(len(data))\n",
    "    mask_2016 = (data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] < '2016-01-01') & (data['TARGET_EVENT_BINARY'] != 1)\n",
    "    mask_2017 = (data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] > '2016-01-01') & (data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] < '2017-01-01')& (data['TARGET_EVENT_BINARY'] != 1)\n",
    "    mask_2018 = (data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] > '2017-01-01') & (data['TARGET_EVENT_BINARY'] != 1)\n",
    "\n",
    "    proba_2016 = probs\n",
    "    proba_2017 = probs\n",
    "    proba_2018 = probs\n",
    "    if logging:\n",
    "        print(\"Before scaling:\")\n",
    "        print(proba_2016[mask_2016].mean(), proba_2017[mask_2017].mean(), proba_2018[mask_2018].mean())\n",
    "        print(probs.mean())\n",
    "    calib_2016 = True\n",
    "    calib_2017 = True\n",
    "    calib_2018 = True\n",
    "\n",
    "    while calib_2016 or calib_2017 or calib_2018:    \n",
    "        proba_2016 = probs\n",
    "        proba_2017 = probs\n",
    "        proba_2018 = probs\n",
    "        proba_2016 = calculate_probabilities_vec(proba_2016, starter_scales[0])\n",
    "        proba_2017 = calculate_probabilities_vec(proba_2017, starter_scales[1])\n",
    "        proba_2018 = calculate_probabilities_vec(proba_2018, starter_scales[2])\n",
    "        \n",
    "        new_proba[mask_2016] = proba_2016[mask_2016]\n",
    "        new_proba[mask_2017] = proba_2017[mask_2017]\n",
    "        new_proba[mask_2018] = proba_2018[mask_2018]\n",
    "        if logging:\n",
    "            print('Adter scaling:')\n",
    "            print(proba_2016[mask_2016].mean(), proba_2017[mask_2017].mean(), proba_2018[mask_2018].mean())\n",
    "            print(new_proba.mean())\n",
    "\n",
    "        mean_2016 = proba_2016[mask_2016].mean()\n",
    "        mean_2017 = proba_2017[mask_2017].mean()\n",
    "        mean_2018 = proba_2018[mask_2018].mean()\n",
    "\n",
    "        diff_2016 = mean_2016 - targets[0]\n",
    "        diff_2017 = mean_2017 - targets[1]\n",
    "        diff_2018 = mean_2018 - targets[2]\n",
    "        if diff_2016 > 0.0001:\n",
    "            starter_scales[0] += 0.01\n",
    "        elif diff_2016 < -0.0001:\n",
    "            starter_scales[0] -= 0.01\n",
    "        else:\n",
    "            calib_2016 = False\n",
    "        if diff_2017 > 0.0001:\n",
    "            starter_scales[1] += 0.01\n",
    "        elif diff_2017 < -0.0001:\n",
    "            starter_scales[1] -= 0.01\n",
    "        else:\n",
    "            calib_2017 = False\n",
    "        if diff_2018 > 0.0001:\n",
    "            starter_scales[2] += 0.01\n",
    "        elif diff_2018 < -0.0001:\n",
    "            starter_scales[2] -= 0.01\n",
    "        else:\n",
    "            calib_2018 = False\n",
    "    return new_proba\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: Maximum number of iterations has been exceeded.\n",
    "#          Current function value: 0.014855\n",
    "#          Iterations: 35\n",
    "#                              Logit Regression Results                             \n",
    "# ==================================================================================\n",
    "# Dep. Variable:     TARGET_EVENT_BINARY_2Y   No. Observations:              1601663\n",
    "# Model:                              Logit   Df Residuals:                  1601611\n",
    "# Method:                               MLE   Df Model:                           51\n",
    "# Date:                    Wed, 08 Nov 2023   Pseudo R-squ.:                  0.6053\n",
    "# Time:                            17:03:09   Log-Likelihood:                -23792.\n",
    "# converged:                          False   LL-Null:                       -60284.\n",
    "# Covariance Type:                nonrobust   LLR p-value:                     0.000\n",
    "\n",
    "#  No dummies and feature engineering:\n",
    "# Optimization terminated successfully.\n",
    "#          Current function value: 0.017321\n",
    "#          Iterations 13\n",
    "#                              Logit Regression Results                             \n",
    "# ==================================================================================\n",
    "# Dep. Variable:     TARGET_EVENT_BINARY_2Y   No. Observations:              1601663\n",
    "# Model:                              Logit   Df Residuals:                  1601638\n",
    "# Method:                               MLE   Df Model:                           24\n",
    "# Date:                    Wed, 08 Nov 2023   Pseudo R-squ.:                  0.5398\n",
    "# Time:                            20:38:24   Log-Likelihood:                -27743.\n",
    "# converged:                           True   LL-Null:                       -60284.\n",
    "# Covariance Type:                nonrobust   LLR p-value:                     0.000\n",
    "# Two more variables added: LOAN_TYPE, FREQ_TYPE, MORGTAGE_TYPE, INTEREST_TYPE\n",
    "# Warning: Maximum number of iterations has been exceeded.\n",
    "#          Current function value: 0.014613\n",
    "#          Iterations: 35\n",
    "#                              Logit Regression Results                             \n",
    "# ==================================================================================\n",
    "# Dep. Variable:     TARGET_EVENT_BINARY_2Y   No. Observations:              1601663\n",
    "# Model:                              Logit   Df Residuals:                  1601589\n",
    "# Method:                               MLE   Df Model:                           73\n",
    "# Date:                    Thu, 09 Nov 2023   Pseudo R-squ.:                  0.6118\n",
    "# Time:                            18:28:24   Log-Likelihood:                -23405.\n",
    "# converged:                          False   LL-Null:                       -60284.\n",
    "# Covariance Type:                nonrobust   LLR p-value:                     0.000\n",
    "# Adding monthly to LOAN_TYPE, FREQ_TYPE, MORGTAGE_TYPE, INTEREST_TYPE\n",
    "# Warning: Maximum number of iterations has been exceeded.\n",
    "#          Current function value: 0.014232\n",
    "#          Iterations: 35\n",
    "#                              Logit Regression Results                             \n",
    "# ==================================================================================\n",
    "# Dep. Variable:     TARGET_EVENT_BINARY_2Y   No. Observations:              1601663\n",
    "# Model:                              Logit   Df Residuals:                  1601584\n",
    "# Method:                               MLE   Df Model:                           78\n",
    "# Date:                    Thu, 09 Nov 2023   Pseudo R-squ.:                  0.6219\n",
    "# Time:                            18:35:37   Log-Likelihood:                -22795.\n",
    "# converged:                          False   LL-Null:                       -60284.\n",
    "# Covariance Type:                nonrobust   LLR p-value:                     0.000\n",
    "#  ADDING TIME TO MATURITY in days\n",
    "# Warning: Maximum number of iterations has been exceeded.\n",
    "#          Current function value: 0.014143\n",
    "#          Iterations: 35\n",
    "#                              Logit Regression Results                             \n",
    "# ==================================================================================\n",
    "# Dep. Variable:     TARGET_EVENT_BINARY_2Y   No. Observations:              1601663\n",
    "# Model:                              Logit   Df Residuals:                  1601586\n",
    "# Method:                               MLE   Df Model:                           76\n",
    "# Date:                    Mon, 13 Nov 2023   Pseudo R-squ.:                  0.6242\n",
    "# Time:                            17:00:42   Log-Likelihood:                -22653.\n",
    "# converged:                          False   LL-Null:                       -60284.\n",
    "# Covariance Type:                nonrobust   LLR p-value:                     0.000\n",
    "# Best score:\n",
    "# Warning: Maximum number of iterations has been exceeded.\n",
    "#          Current function value: 0.014634\n",
    "#          Iterations: 35\n",
    "#                              Logit Regression Results                             \n",
    "# ==================================================================================\n",
    "# Dep. Variable:     TARGET_EVENT_BINARY_2Y   No. Observations:              1601663\n",
    "# Model:                              Logit   Df Residuals:                  1601605\n",
    "# Method:                               MLE   Df Model:                           57\n",
    "# Date:                    Mon, 27 Nov 2023   Pseudo R-squ.:                  0.6112\n",
    "# Time:                            10:45:49   Log-Likelihood:                -23439.\n",
    "# converged:                          False   LL-Null:                       -60284.\n",
    "# Covariance Type:                nonrobust   LLR p-value:                     0.000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns.remove('CONTRACT_CURRENCY')\n",
    "X_columns.remove('BORROWER_COUNTY')\n",
    "# X_columns.remove('CONTRACT_MORTGAGE_TYPE')\n",
    "X_columns.remove('BORROWER_TYPE_OF_SETTLEMENT')\n",
    "X_columns.remove('BORROWER_CITIZENSHIP')\n",
    "X_columns.remove('LOAN_BORROWER_COUNT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def variance_threshold_selector(data, threshold=0.5):\n",
    "    # https://stackoverflow.com/a/39813304/1956309\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "    return data.columns[selector.get_support(indices=True)]\n",
    "min_variance = 0.0001\n",
    "low_variance = variance_threshold_selector(training_data[X_columns], min_variance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAN_TYPE_47693941\n",
      "LOAN_TYPE_8fe006f1\n",
      "LOAN_TYPE_cde77491\n",
      "FREQ_TYPE_bd092d5a\n",
      "MORTGAGE_TYPE_3.0\n",
      "MORTGAGE_TYPE_10.0\n",
      "MORTGAGE_TYPE_41.0\n",
      "MORTGAGE_TYPE_42.0\n",
      "MORTGAGE_TYPE_43.0\n",
      "MORTGAGE_TYPE_47.0\n"
     ]
    }
   ],
   "source": [
    "for i in X_columns:\n",
    "    if i not in low_variance:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns = low_variance\n",
    "X_columns= list(X_columns)\n",
    "# X_columns.append('TIME_TO_MATURITY_DAYS')\n",
    "# training_data['TIME_TO_MATURITY_DAYS_SQRT'] = np.sqrt(training_data['TIME_TO_MATURITY_DAYS'])\n",
    "# X_columns.append('TIME_TO_MATURITY_DAYS_SQRT')\n",
    "\n",
    "# training_data['MONTHLY_PAYMENT'] = training_data.apply(calculate_monthly_payment, axis=1)\n",
    "# training_data['MONTHLY_PAYMENT'] = training_data['MONTHLY_PAYMENT'].apply(lambda x: 0 if not np.isfinite(x) else x)\n",
    "# X_columns.extend(['MONTHLY_PAYMENT'])\n",
    "# lognormal_variables.extend(['MONTHLY_PAYMENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = training_data['CONTRACT_LOAN_TYPE'].value_counts() < 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LOAN_TYPE_694cbaee',\n",
       " 'LOAN_TYPE_0aeb4094',\n",
       " 'LOAN_TYPE_eab72d7a',\n",
       " 'LOAN_TYPE_95c4f8fb',\n",
       " 'LOAN_TYPE_f792971b',\n",
       " 'LOAN_TYPE_cde77491',\n",
       " 'LOAN_TYPE_47693941',\n",
       " 'LOAN_TYPE_8fe006f1']"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_list = vals.index[vals].tolist()\n",
    "small_header = ['LOAN_TYPE_' + i for i in small_list]\n",
    "print(small_header)\n",
    "[X_columns.remove(i) for i in small_header] \n",
    "X_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.014651\n",
      "         Iterations: 35\n",
      "                             Logit Regression Results                             \n",
      "==================================================================================\n",
      "Dep. Variable:     TARGET_EVENT_BINARY_2Y   No. Observations:              1601663\n",
      "Model:                              Logit   Df Residuals:                  1601606\n",
      "Method:                               MLE   Df Model:                           56\n",
      "Date:                    Mon, 27 Nov 2023   Pseudo R-squ.:                  0.6108\n",
      "Time:                            12:22:39   Log-Likelihood:                -23465.\n",
      "converged:                          False   LL-Null:                       -60284.\n",
      "Covariance Type:                nonrobust   LLR p-value:                     0.000\n",
      "===================================================================================================\n",
      "                                      coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------------\n",
      "const                             -10.1744      9.844     -1.034      0.301     -29.468       9.119\n",
      "CONTRACT_CREDIT_INTERMEDIARY        0.0747      0.004     16.818      0.000       0.066       0.083\n",
      "CONTRACT_CREDIT_LOSS                4.4896      0.055     81.555      0.000       4.382       4.597\n",
      "CONTRACT_DEPT_SERVICE_TO_INCOME    -0.0759      0.028     -2.689      0.007      -0.131      -0.021\n",
      "CONTRACT_INCOME                     0.2224      0.028      7.922      0.000       0.167       0.277\n",
      "CONTRACT_INSTALMENT_AMOUNT          0.0055      0.013      0.421      0.674      -0.020       0.031\n",
      "CONTRACT_INSTALMENT_AMOUNT_2       -0.1242      0.023     -5.395      0.000      -0.169      -0.079\n",
      "CONTRACT_INTEREST_PERIOD           -0.1253      0.031     -4.090      0.000      -0.185      -0.065\n",
      "CONTRACT_INTEREST_RATE              0.2319      0.018     12.670      0.000       0.196       0.268\n",
      "CONTRACT_LGD                       -0.4404      0.035    -12.641      0.000      -0.509      -0.372\n",
      "CONTRACT_LOAN_AMOUNT               -0.0128      0.043     -0.295      0.768      -0.098       0.072\n",
      "CONTRACT_LOAN_CONTRACT_TYPE         0.7965      0.062     12.809      0.000       0.675       0.918\n",
      "CONTRACT_LOAN_TO_VALUE_RATIO       -0.5815      0.040    -14.361      0.000      -0.661      -0.502\n",
      "CONTRACT_MARKET_VALUE               0.2886      0.106      2.714      0.007       0.080       0.497\n",
      "CONTRACT_MORTGAGE_LENDING_VALUE    -1.0274      0.091    -11.331      0.000      -1.205      -0.850\n",
      "CONTRACT_RISK_WEIGHTED_ASSETS       0.0317      0.005      6.568      0.000       0.022       0.041\n",
      "BORROWER_BIRTH_YEAR                -0.1797      0.039     -4.625      0.000      -0.256      -0.104\n",
      "LOAN_TYPE_1f951336                 -0.9141      0.061    -15.011      0.000      -1.033      -0.795\n",
      "LOAN_TYPE_2f88e16c                 -1.2088      0.191     -6.344      0.000      -1.582      -0.835\n",
      "LOAN_TYPE_5a06241e                 -0.0728      0.018     -3.937      0.000      -0.109      -0.037\n",
      "LOAN_TYPE_69f70539                 -0.5690      0.039    -14.672      0.000      -0.645      -0.493\n",
      "LOAN_TYPE_7e2065f4                 -0.5193      0.040    -12.939      0.000      -0.598      -0.441\n",
      "LOAN_TYPE_83910425                 -0.0165      0.011     -1.527      0.127      -0.038       0.005\n",
      "LOAN_TYPE_955ae3ef                 -0.4535      0.048     -9.490      0.000      -0.547      -0.360\n",
      "LOAN_TYPE_95c4f8fb                 -0.0524      0.016     -3.227      0.001      -0.084      -0.021\n",
      "LOAN_TYPE_b503a0de                 -1.4296      0.077    -18.486      0.000      -1.581      -1.278\n",
      "LOAN_TYPE_cf07c2dd                 -0.3451      0.027    -12.823      0.000      -0.398      -0.292\n",
      "LOAN_TYPE_d3aaffde                 -0.2191      0.039     -5.627      0.000      -0.295      -0.143\n",
      "LOAN_TYPE_eab72d7a                 -0.0737      0.007    -10.684      0.000      -0.087      -0.060\n",
      "LOAN_TYPE_f792971b                 -0.2097     49.645     -0.004      0.997     -97.511      97.092\n",
      "FREQ_TYPE_3265c5b7                 -0.3892      0.105     -3.710      0.000      -0.595      -0.184\n",
      "FREQ_TYPE_479a2e13                 -1.0459      0.270     -3.873      0.000      -1.575      -0.517\n",
      "FREQ_TYPE_87db11f5                 -0.8752      0.172     -5.098      0.000      -1.212      -0.539\n",
      "FREQ_TYPE_89efd382                 -0.0380      0.017     -2.196      0.028      -0.072      -0.004\n",
      "FREQ_TYPE_ad534644                 -0.1066      0.033     -3.274      0.001      -0.170      -0.043\n",
      "INTEREST_TYPE_100001.0             -0.1650      0.011    -14.663      0.000      -0.187      -0.143\n",
      "INTEREST_TYPE_100002.0             -0.6075      0.027    -22.398      0.000      -0.661      -0.554\n",
      "INTEREST_TYPE_100003.0             -1.1221      0.019    -58.169      0.000      -1.160      -1.084\n",
      "INTEREST_TYPE_100004.0             -0.9006      0.025    -35.736      0.000      -0.950      -0.851\n",
      "INTEREST_TYPE_110001.0             -0.1901      0.026     -7.255      0.000      -0.241      -0.139\n",
      "INTEREST_TYPE_140001.0             -0.1870      0.008    -23.849      0.000      -0.202      -0.172\n",
      "INTEREST_TYPE_140002.0             -0.0751      0.004    -20.409      0.000      -0.082      -0.068\n",
      "INTEREST_TYPE_140003.0             -0.2498      0.010    -26.137      0.000      -0.268      -0.231\n",
      "MORTGAGE_TYPE_1.0                  -0.0705      0.040     -1.766      0.077      -0.149       0.008\n",
      "MORTGAGE_TYPE_4.0                   0.0899      0.074      1.212      0.225      -0.055       0.235\n",
      "MORTGAGE_TYPE_5.0                   0.0619      0.013      4.596      0.000       0.035       0.088\n",
      "MORTGAGE_TYPE_6.0                   0.0817      0.022      3.755      0.000       0.039       0.124\n",
      "MORTGAGE_TYPE_7.0                   0.1101      0.032      3.388      0.001       0.046       0.174\n",
      "MORTGAGE_TYPE_8.0                   0.0194      0.016      1.196      0.232      -0.012       0.051\n",
      "MORTGAGE_TYPE_13.0                 -0.3410    586.069     -0.001      1.000   -1149.016    1148.334\n",
      "MORTGAGE_TYPE_44.0                 -0.0411      0.029     -1.410      0.159      -0.098       0.016\n",
      "MORTGAGE_TYPE_45.0                  0.0405      0.031      1.292      0.196      -0.021       0.102\n",
      "MORTGAGE_TYPE_46.0                  0.3459      0.052      6.591      0.000       0.243       0.449\n",
      "MORTGAGE_TYPE_48.0                  0.0442      0.027      1.618      0.106      -0.009       0.098\n",
      "BORROWER_LOAN_COUNT                -0.1259      0.021     -6.116      0.000      -0.166      -0.086\n",
      "TOTAL_LOAN_AMOUNT                  -0.4569      0.042    -10.762      0.000      -0.540      -0.374\n",
      "TOTAL_INSTALLMENT_AMOUNT            0.1152      0.018      6.440      0.000       0.080       0.150\n",
      "===================================================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.50 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "model, non_significant_vars = significant_features(training_data, X_columns, y_column,LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['const',\n",
       " 'CONTRACT_INSTALMENT_AMOUNT',\n",
       " 'CONTRACT_LOAN_AMOUNT',\n",
       " 'LOAN_TYPE_83910425',\n",
       " 'LOAN_TYPE_f792971b',\n",
       " 'MORTGAGE_TYPE_1.0',\n",
       " 'MORTGAGE_TYPE_4.0',\n",
       " 'MORTGAGE_TYPE_8.0',\n",
       " 'MORTGAGE_TYPE_13.0',\n",
       " 'MORTGAGE_TYPE_44.0',\n",
       " 'MORTGAGE_TYPE_45.0',\n",
       " 'MORTGAGE_TYPE_48.0']"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_significant_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have a pandas DataFrame named training_data\n",
    "# unique_training_data = training_data.drop_duplicates(subset='CONTRACT_ID', keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean in year 0.011345816067598232\n",
      "5206\n",
      "1750.5931790499392\n",
      "Logloss:\n",
      "0.023952328423164044\n",
      "ROC AUC Score: 0.9884292196483921\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014295684813192064\n"
     ]
    }
   ],
   "source": [
    "# Adding calib (isotonic)\n",
    "# Logloss: 0.02385418586559387\n",
    "# Logloss: imputed\n",
    "# 0.02372888852503455\n",
    "# Logloss: imputed and removed a few unnecessary variables\n",
    "# 0.023720691340422\n",
    "# Logloss: imouted and added monthly payment\n",
    "# 0.02294559159914238\n",
    "# Logloss: best sub:\n",
    "# 0.023917118107561128\n",
    "predicted_probs = 'LOGISTIC_REG'\n",
    "training_data['LOGISTIC_REG_CUMM'] = 0\n",
    "iterations = 1\n",
    "for i in range(iterations):\n",
    "    probs,_ = train_and_predict_two_halves(\n",
    "        training_data, \n",
    "        X_columns, \n",
    "        'TARGET_EVENT_BINARY',\n",
    "        model=LogisticRegression(max_iter=400, random_state=42,solver='lbfgs'),\n",
    "        augment_distribution=True,\n",
    "        augment_distribution_percentage=1.48,\n",
    "        calibrate=True,\n",
    "        unique_loans=False,\n",
    "        should_smote=False,\n",
    "        maximal_sample=False,\n",
    "        random_sample=i,\n",
    "        calib_method='isotonic',\n",
    "        show_curve=(i==0),\n",
    "    )\n",
    "    training_data[predicted_probs] = probs\n",
    "    training_data['LOGISTIC_REG_CUMM'] += probs\n",
    "training_data['LOGISTIC_REG_CUMM'] /= iterations\n",
    "print(probs.mean())\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'LOGISTIC_REG'] = 0\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'LOGISTIC_REG_CUMM'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011512021132794568\n",
      "Logloss:\n",
      "0.26221023356337625\n",
      "Before centering:\n",
      "0.999999981807983 2.26756847829801e-05 0.015053032575025972\n",
      "Centering probabilities...\n",
      "0.00025303257502597154 2.001\n",
      "0.0002481812283485745 2.002\n",
      "0.0002433332195526782 2.0029999999999997\n",
      "0.0002384885449246768 2.0039999999999996\n",
      "0.0002336472007568209 2.0049999999999994\n",
      "0.0002288091833474639 2.0059999999999993\n",
      "0.0002239744890006612 2.0069999999999992\n",
      "0.00021914311402654145 2.007999999999999\n",
      "0.00021431505474107417 2.008999999999999\n",
      "0.00020949030746609393 2.009999999999999\n",
      "0.00020466886852927267 2.010999999999999\n",
      "0.00019985073426425325 2.0119999999999987\n",
      "0.00019503590101037707 2.0129999999999986\n",
      "0.00019022436511290444 2.0139999999999985\n",
      "0.00018541612292291218 2.0149999999999983\n",
      "0.00018061117079725027 2.0159999999999982\n",
      "0.00017580950509855225 2.016999999999998\n",
      "0.00017101112219530465 2.017999999999998\n",
      "0.00016621601846167344 2.018999999999998\n",
      "0.0001614241902776082 2.019999999999998\n",
      "0.0001566356340288507 2.0209999999999977\n",
      "0.00015185034610680492 2.0219999999999976\n",
      "0.00014706832290868263 2.0229999999999975\n",
      "0.00014228956083730052 2.0239999999999974\n",
      "0.00013751405630121546 2.0249999999999972\n",
      "0.0001327418057146968 2.025999999999997\n",
      "0.00012797280549767427 2.026999999999997\n",
      "0.00012320705207568422 2.027999999999997\n",
      "0.00011844454187996852 2.028999999999997\n",
      "0.00011368527134739993 2.0299999999999967\n",
      "0.00010892923692044226 2.0309999999999966\n",
      "0.00010417643504719021 2.0319999999999965\n",
      "9.942686218137112e-05 2.0329999999999964\n",
      "9.468051478223397e-05 2.0339999999999963\n",
      "8.993738931464997e-05 2.034999999999996\n",
      "8.51974822490692e-05 2.035999999999996\n",
      "8.046079006144777e-05 2.036999999999996\n",
      "7.572730923332584e-05 2.037999999999996\n",
      "7.099703625173918e-05 2.0389999999999957\n",
      "6.626996760923307e-05 2.0399999999999956\n",
      "6.154609980395069e-05 2.0409999999999955\n",
      "5.682542933940597e-05 2.0419999999999954\n",
      "5.2107952724688225e-05 2.0429999999999953\n",
      "4.739366647431818e-05 2.043999999999995\n",
      "4.26825671082931e-05 2.044999999999995\n",
      "3.7974651152038186e-05 2.045999999999995\n",
      "3.3269915136425673e-05 2.046999999999995\n",
      "2.8568355597797385e-05 2.0479999999999947\n",
      "2.3869969077843292e-05 2.0489999999999946\n",
      "1.917475212367091e-05 2.0499999999999945\n",
      "1.4482701287876418e-05 2.0509999999999944\n",
      "9.793813128286186e-06 2.0519999999999943\n",
      "0.9999999716635116 2.2111839274363554e-05 0.014809793813128287\n",
      "Saved file: ./predictions/lr.csv\n"
     ]
    }
   ],
   "source": [
    "new_proba = scale_yearly_proba(training_data, 'LOGISTIC_REG')\n",
    "print(new_proba.mean())\n",
    "training_data['YEAR_SCALED_PROBA'] = new_proba\n",
    "submission = create_submission_file(training_data, 'YEAR_SCALED_PROBA', data_submission_example, filename='./predictions/lr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean in year 0.011345816067598232\n",
      "5206\n",
      "2769.8234336859236\n",
      "Logloss:\n",
      "0.01794931514821946\n",
      "ROC AUC Score: 0.9965531784500443\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012938618691090552\n"
     ]
    }
   ],
   "source": [
    "predicted_probs = 'XGB'\n",
    "training_data['XGB_CUMM'] = 0\n",
    "iterations = 1\n",
    "for i in range(iterations):\n",
    "    probs,_ = train_and_predict_two_halves(\n",
    "        training_data, \n",
    "        X_columns, \n",
    "        'TARGET_EVENT_BINARY',\n",
    "        model=xgb.XGBClassifier(max_depth=2, n_estimators=70, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "        augment_distribution=True,\n",
    "        augment_distribution_percentage=1.68,\n",
    "        calibrate=True,\n",
    "        random_sample=i,\n",
    "        calib_method='isotonic',\n",
    "        show_curve=(i==0),\n",
    "    )\n",
    "    training_data[predicted_probs] = probs\n",
    "    training_data['XGB_CUMM'] += probs\n",
    "training_data['XGB_CUMM'] /= iterations\n",
    "print(probs.mean())\n",
    "# this helps\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'XGB'] = 0\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'XGB_CUMM'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# # 100 minleaf 0.1 lr maxdepth 4 n_est 100 ll: 0.0157 tll: 0.0252 (25, 25, 50)\n",
    "\n",
    "# predicted_probs = 'GBM'\n",
    "# training_data['GBM_CUMM'] = 0\n",
    "# iterations = 1\n",
    "# for i in range(iterations):\n",
    "#     probs,_ = train_and_predict_two_halves(\n",
    "#         training_data, \n",
    "#         X_columns, \n",
    "#         'TARGET_EVENT_BINARY',\n",
    "#         model=GradientBoostingClassifier(min_samples_leaf=100,learning_rate=0.1,max_depth=3, n_estimators=200, random_state=42,),\n",
    "#         augment_distribution=True,\n",
    "#         augment_distribution_percentage=1.68,\n",
    "#         calibrate=True,\n",
    "#         random_sample=i,\n",
    "#     )\n",
    "#     training_data[predicted_probs] = probs\n",
    "#     training_data['GBM_CUMM'] += probs\n",
    "# training_data['GBM_CUMM'] /= iterations\n",
    "# print(probs.mean())\n",
    "# # this helps\n",
    "# training_data.loc[training_data['TARGET_EVENT'] == 'E', 'GBM'] = 0\n",
    "# training_data.loc[training_data['TARGET_EVENT'] == 'E', 'GBM_CUMM'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean in year 0.011345816067598232\n",
      "5206\n",
      "2259.690510058932\n",
      "Logloss:\n",
      "0.0185215773165008\n",
      "ROC AUC Score: 0.9963544470227873\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01268609266870293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Logloss:\n",
    "# 0.02484109588442129 RF 200-3\n",
    "# Logloss:\n",
    "# 0.023534501248296504 RF 200-4-10\n",
    "# Logloss:\n",
    "# 0.02304888836026815 400-4- log_loss 25 \n",
    "# Logloss:\n",
    "# 0.0216212401349101 400-5- log_loss 25\n",
    "# Logloss:\n",
    "# 0.020320409332841818 400-6- log_loss 25\n",
    "# Logloss:\n",
    "# 0.018208761775049223 400-8- log_loss 30\n",
    "# Logloss:\n",
    "# 0.019426191288290054 200-8- log_loss 100\n",
    "# Logloss:\n",
    "# 0.01860565128621486 400 10 Logloss 100\n",
    "# Logloss:\n",
    "# 0.018445624629156568 400 11 Logloss 100\n",
    "\n",
    "# Logloss: with 600:{\"Logloss\": 0.024925861276495972, \"ROCAUC\": 0.9808195618274818}\n",
    "# 0.01839314824358871\n",
    "# Logloss: with 1200 40 min {\"Logloss\": 0.024940499198232467, \"ROCAUC\": 0.9807876124372913}\n",
    "# 0.01838510029059968\n",
    "# Logloss: current\n",
    "# 0.018835162103137956\n",
    "predicted_probs = 'NN'\n",
    "training_data['RF_CUMM'] = 0\n",
    "iterations = 1\n",
    "for i in range(iterations):\n",
    "    probs,_ = train_and_predict_two_halves(\n",
    "        training_data, \n",
    "        X_columns, \n",
    "        'TARGET_EVENT_BINARY',\n",
    "        model= RandomForestClassifier(n_estimators=600, max_depth=11, random_state=42, min_samples_leaf=100, criterion=\"log_loss\", n_jobs=-1   ),\n",
    "        augment_distribution=True,\n",
    "        augment_distribution_percentage=1.58,\n",
    "        calibrate=True,\n",
    "        random_sample=i,\n",
    "        calib_method='isotonic',\n",
    "        show_curve=(i==0),\n",
    "    )\n",
    "    training_data[predicted_probs] = probs\n",
    "    training_data['RF_CUMM'] += probs\n",
    "training_data['RF_CUMM'] /= iterations\n",
    "print(probs.mean())\n",
    "# this helps\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'NN'] = 0\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'RF_CUMM'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightgbm import LGBMClassifier\n",
    "\n",
    "# predicted_probs = 'LGBM'\n",
    "# probs,_ = train_and_predict_two_halves(\n",
    "#     training_data, \n",
    "#     X_columns, \n",
    "#     'TARGET_EVENT_BINARY',\n",
    "#     model= LGBMClassifier(n_estimators=100, max_depth=3,boosting_type=\"dart\", random_state=42),\n",
    "#     augment_distribution=True,\n",
    "#     augment_distribution_percentage=1.48,\n",
    "#     calibrate=True,\n",
    "# )\n",
    "# training_data[predicted_probs] = probs\n",
    "# print(probs.mean())\n",
    "# # this helps\n",
    "# training_data.loc[training_data['TARGET_EVENT'] == 'E', 'LGBM'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logreg : 0.02079401800946978\n",
      "XGB : 0.016443337709751576\n",
      "RF : 0.016325035491740362\n"
     ]
    }
   ],
   "source": [
    "print(f\"Logreg : { log_loss(training_data['TARGET_EVENT_BINARY'], training_data['LOGISTIC_REG'])}\")\n",
    "# print(f\"XGB+ : {log_loss(training_data['TARGET_EVENT_BINARY'], training_data['ENSEMBLE'])}\")\n",
    "# print(f\"RF+ : {log_loss(training_data['TARGET_EVENT_BINARY'], training_data['ENSEMBLE_NN'])}\")\n",
    "print(f\"XGB : {log_loss(training_data['TARGET_EVENT_BINARY'], training_data['XGB'])}\")\n",
    "print(f\"RF : {log_loss(training_data['TARGET_EVENT_BINARY'], training_data['NN'])}\")\n",
    "# print(f\"LGBM : {log_loss(training_data['TARGET_EVENT_BINARY'], training_data['LGBM'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.5 0.2]\n",
      "CUSTOM+ : 0.016725658535945646\n"
     ]
    }
   ],
   "source": [
    "values = [30, 50, 20]\n",
    "# [0.25 0.25 0.25 0.25]\n",
    "# CUSTOM+ : 0.01588790730454689 og\n",
    "# CUSTOM+ : 0.016648811980442747\n",
    "# [25, 30,45]: CUSTOM+ : 0.016663738940216465\n",
    "\n",
    "values = values/np.sum(values)\n",
    "print(values)\n",
    "custom = training_data['LOGISTIC_REG']*values[0] + training_data['NN']*values[1]+training_data['XGB']*values[2]\n",
    "\n",
    "print(f\"CUSTOM+ : {log_loss(training_data['TARGET_EVENT_BINARY'], custom)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling:\n",
      "0.006214119511649236 0.007859660483801038 0.012185346778937478\n",
      "0.013071934804282903\n",
      "Adter scaling:\n",
      "0.004791433685417807 0.011179788696999845 0.021618468688174425\n",
      "0.012861879584596603\n",
      "Adter scaling:\n",
      "0.0048051563577822184 0.011120151162410797 0.021476931500519002\n",
      "0.012796016355506128\n",
      "Adter scaling:\n",
      "0.004818958364186874 0.011061183786978802 0.021337585319482027\n",
      "0.012731179625389319\n",
      "Adter scaling:\n",
      "0.004832840400794665 0.011002874510504782 0.021200368769076012\n",
      "0.012667343740418663\n",
      "Adter scaling:\n",
      "0.004846803172034055 0.01094521158203499 0.021065223070965682\n",
      "0.012604484078171711\n",
      "Adter scaling:\n",
      "0.004860847390723996 0.01088818354939121 0.02093209189953441\n",
      "0.012542576992490598\n",
      "Adter scaling:\n",
      "0.004874973778201043 0.01083177924914234 0.020800921246792045\n",
      "0.01248159976200059\n",
      "Adter scaling:\n",
      "0.004889183064448765 0.010775987796995271 0.020671659296344336\n",
      "0.012421530542002086\n",
      "Adter scaling:\n",
      "0.0049034759882298825 0.010720798578584363 0.020544256305714628\n",
      "0.012362348319476218\n",
      "Adter scaling:\n",
      "0.004917853297220466 0.010666201240639415 0.020418664496372068\n",
      "0.01230403287096659\n",
      "Adter scaling:\n",
      "0.004932315748146984 0.010612185682514041 0.02029483795087681\n",
      "0.012246564723121089\n",
      "Adter scaling:\n",
      "0.004946864106925668 0.010558742048056582 0.02017273251660465\n",
      "0.012189925115695811\n",
      "Adter scaling:\n",
      "0.004961499148804781 0.01050586071780713 0.020052305715559278\n",
      "0.012134095966840248\n",
      "Adter scaling:\n",
      "0.004976221658509342 0.010453532301505017 0.01993351665982259\n",
      "0.012079059840498004\n",
      "Adter scaling:\n",
      "0.004991032430388723 0.010401747630892138 0.019816325972231253\n",
      "0.012024799915771342\n",
      "Adter scaling:\n",
      "0.005005932268567037 0.010350497752798064 0.019700695711901473\n",
      "0.01197129995810989\n",
      "Adter scaling:\n",
      "0.0050209219870964395 0.01029977392249369 0.019586589304255844\n",
      "0.011918544292195866\n",
      "Adter scaling:\n",
      "0.005036002410113401 0.01024956759730116 0.01947397147523388\n",
      "0.01186651777640768\n",
      "Adter scaling:\n",
      "0.0050511743719977836 0.010199870430448047 0.019362808189393486\n",
      "0.011815205778753928\n",
      "Adter scaling:\n",
      "0.005066438717535349 0.010150674265154865 0.01925306659163385\n",
      "0.011764594154177794\n",
      "Adter scaling:\n",
      "0.0050817963020831775 0.01010197112894501 0.01914471495229204\n",
      "0.011714669223139792\n",
      "Adter scaling:\n",
      "0.005097247991738342 0.010053753228167444 0.019037722615384296\n",
      "0.011665417751394076\n",
      "Adter scaling:\n",
      "0.005112794663510017 0.010053753228167444 0.018932059949780856\n",
      "0.011632484925894596\n",
      "Adter scaling:\n",
      "0.005112794663510017 0.010053753228167444 0.018827698303119783\n",
      "0.011595225061253852\n",
      "Adter scaling:\n",
      "0.005112794663510017 0.010053753228167444 0.018724609958279296\n",
      "0.011558419798998187\n",
      "Adter scaling:\n",
      "0.005112794663510017 0.010053753228167444 0.018622768092241913\n",
      "0.011522059562601791\n",
      "Adter scaling:\n",
      "0.005112794663510017 0.010053753228167444 0.018522146737196606\n",
      "0.01148613508087018\n"
     ]
    }
   ],
   "source": [
    "training_data['CUSTOM'] = custom\n",
    "new_proba = scale_yearly_proba(training_data, 'CUSTOM', targets =[ 0.0052, 0.0100, 0.0185], logging=True)\n",
    "training_data['CUSTOM'] = new_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss:\n",
      "0.26437266266112935\n",
      "Before centering:\n",
      "0.9999999999999687 7.830987203027995e-06 0.01470541689373807\n",
      "Centering probabilities...\n",
      "-9.458310626193032e-05 1.999\n",
      "-9.021411453142246e-05 1.9980000000000002\n",
      "-8.584220112534124e-05 1.9970000000000003\n",
      "-8.146736276927816e-05 1.9960000000000004\n",
      "-7.708959618347132e-05 1.9950000000000006\n",
      "-7.270889808283845e-05 1.9940000000000007\n",
      "-6.832526517689536e-05 1.9930000000000008\n",
      "-6.393869416983916e-05 1.9920000000000009\n",
      "-5.95491817604616e-05 1.991000000000001\n",
      "-5.515672464215249e-05 1.990000000000001\n",
      "-5.076131950292745e-05 1.9890000000000012\n",
      "-4.6362963025365495e-05 1.9880000000000013\n",
      "-4.196165188660554e-05 1.9870000000000014\n",
      "-3.755738275838111e-05 1.9860000000000015\n",
      "-3.315015230696307e-05 1.9850000000000017\n",
      "-2.873995719314751e-05 1.9840000000000018\n",
      "-2.4326794072262667e-05 1.9830000000000019\n",
      "-1.9910659594151586e-05 1.982000000000002\n",
      "-1.5491550403168644e-05 1.981000000000002\n",
      "-1.1069463138144858e-05 1.9800000000000022\n",
      "-6.6443944323930915e-06 1.9790000000000023\n",
      "0.9999999999999771 7.910087771012542e-06 0.014793355605567608\n",
      "Saved file: ./predictions/nn-xgb-log-custom.csv\n"
     ]
    }
   ],
   "source": [
    "submission = create_submission_file(training_data, 'CUSTOM', data_submission_example, filename='./predictions/nn-xgb-log-custom.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BORROWER_ID    0\n",
       "PRED           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logreg : 0.020134263366506807\n",
      "XGB : 0.01771778774236349\n",
      "RF : 0.01637282709272169\n"
     ]
    }
   ],
   "source": [
    "print(f\"Logreg : { log_loss(training_data['TARGET_EVENT_BINARY'], training_data['LOGISTIC_REG_CUMM'])}\")\n",
    "print(f\"XGB : {log_loss(training_data['TARGET_EVENT_BINARY'], training_data['XGB_CUMM'])}\")\n",
    "print(f\"RF : {log_loss(training_data['TARGET_EVENT_BINARY'], training_data['RF_CUMM'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25 0.25 0.5 ]\n",
      "CUSTOM+ : 0.016533410454972034\n"
     ]
    }
   ],
   "source": [
    "values = [25, 25,50]\n",
    "values = values/np.sum(values)\n",
    "print(values)\n",
    "custom = training_data['LOGISTIC_REG_CUMM']*values[0] + training_data['XGB_CUMM']*values[1]+ training_data['RF_CUMM']*values[2]\n",
    "\n",
    "print(f\"CUSTOM+ : {log_loss(training_data['TARGET_EVENT_BINARY'], custom)}\")\n",
    "training_data['CUSTOM'] = custom\n",
    "new_proba = scale_yearly_proba(training_data, 'CUSTOM', targets =[ 0.0053, 0.0101, 0.0185])\n",
    "training_data['CUSTOM'] = new_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss:\n",
      "0.2675654201754927\n",
      "Before centering:\n",
      "1.0000130795158815 3.6340871196749447e-06 0.01470103385508877\n",
      "Centering probabilities...\n",
      "nan 1.999\n",
      "0.9999999999999828 3.6340871196749447e-06 0.014700152280562354\n",
      "Saved file: ./predictions/nn-xgb-log-custom-cumm.csv\n"
     ]
    }
   ],
   "source": [
    "submission = create_submission_file(training_data, 'CUSTOM', data_submission_example, filename='./predictions/nn-xgb-log-custom-cumm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean in year 0.011345816067598232\n",
      "5206\n",
      "1750.5931790499392\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394910\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 400842, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198296\n",
      "[LightGBM] [Info] Start training from score -4.198296\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394910\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4102\n",
      "[LightGBM] [Info] Number of data points in the train set: 400842, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198296\n",
      "[LightGBM] [Info] Start training from score -4.198296\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394910\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 400842, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198296\n",
      "[LightGBM] [Info] Start training from score -4.198296\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394911\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 400843, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198299\n",
      "[LightGBM] [Info] Start training from score -4.198299\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394911\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4100\n",
      "[LightGBM] [Info] Number of data points in the train set: 400843, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198299\n",
      "[LightGBM] [Info] Start training from score -4.198299\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Logloss:\n",
      "0.017873672206961056\n",
      "ROC AUC Score: 0.9960305120601326\n",
      "0.011525585386129645\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011387661618943183\n"
     ]
    }
   ],
   "source": [
    "new_proba = scale_yearly_proba(training_data, 'ENSEMBLE_LGBM')\n",
    "print(new_proba.mean())\n",
    "training_data['NN_LOG_XGB_LGBM'] = new_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss:\n",
      "0.26410894477695007\n",
      "Before centering:\n",
      "0.9999999999999206 2.404636646247127e-06 0.014602542785973009\n",
      "Centering probabilities...\n",
      "-0.00019745721402699215 1.999\n",
      "-0.00019307935449725672 1.9980000000000002\n",
      "-0.00018869853882500995 1.9970000000000003\n",
      "-0.00018431476366658102 1.9960000000000004\n",
      "-0.00017992802567283646 1.9950000000000006\n",
      "-0.0001755383214891021 1.9940000000000007\n",
      "-0.00017114564775523762 1.9930000000000008\n",
      "-0.00016675000110553083 1.9920000000000009\n",
      "-0.00016235137816872185 1.991000000000001\n",
      "-0.00015794977556803265 1.990000000000001\n",
      "-0.00015354518992110114 1.9890000000000012\n",
      "-0.0001491376178399586 1.9880000000000013\n",
      "-0.00014472705593112332 1.9870000000000014\n",
      "-0.00014031350079539429 1.9860000000000015\n",
      "-0.00013589694902806267 1.9850000000000017\n",
      "-0.00013147739721871762 1.9840000000000018\n",
      "-0.00012705484195137812 1.9830000000000019\n",
      "-0.00012262927980433158 1.982000000000002\n",
      "-0.00011820070735028658 1.981000000000002\n",
      "-0.00011376912115618372 1.9800000000000022\n",
      "-0.00010933451778334137 1.9790000000000023\n",
      "-0.00010489689378735331 1.9780000000000024\n",
      "-0.00010045624571809915 1.9770000000000025\n",
      "-9.601257011973736e-05 1.9760000000000026\n",
      "-9.156586353065675e-05 1.9750000000000028\n",
      "-8.71161224835458e-05 1.9740000000000029\n",
      "-8.266334350529557e-05 1.973000000000003\n",
      "-7.820752311702393e-05 1.972000000000003\n",
      "-7.374865783403572e-05 1.9710000000000032\n",
      "-6.928674416589557e-05 1.9700000000000033\n",
      "-6.482177861627351e-05 1.9690000000000034\n",
      "-6.035375768308378e-05 1.9680000000000035\n",
      "-5.5882677858346e-05 1.9670000000000036\n",
      "-5.1408535628235524e-05 1.9660000000000037\n",
      "-4.6931327473104245e-05 1.9650000000000039\n",
      "-4.2451049867350474e-05 1.964000000000004\n",
      "-3.7967699279529976e-05 1.963000000000004\n",
      "-3.3481272172284846e-05 1.9620000000000042\n",
      "-2.8991765002326156e-05 1.9610000000000043\n",
      "-2.4499174220442635e-05 1.9600000000000044\n",
      "-2.0003496271460766e-05 1.9590000000000045\n",
      "-1.5504727594288156e-05 1.9580000000000046\n",
      "-1.100286462179731e-05 1.9570000000000047\n",
      "-6.497903780941852e-06 1.9560000000000048\n",
      "0.999999999999959 2.4574722358172707e-06 0.014793502096219059\n",
      "Saved file: ./predictions/nn-xgb-log-lgbm.csv\n"
     ]
    }
   ],
   "source": [
    "# Logloss:\n",
    "# 0.2627792536515289: \n",
    "submission = create_submission_file(training_data, 'NN_LOG_XGB_LGBM', data_submission_example, filename='./predictions/nn-xgb-log-lgbm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean in year 0.011345816067598232\n",
      "5206\n",
      "1750.5931790499392\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394910\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 400842, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198296\n",
      "[LightGBM] [Info] Start training from score -4.198296\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042335 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394910\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4102\n",
      "[LightGBM] [Info] Number of data points in the train set: 400842, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198296\n",
      "[LightGBM] [Info] Start training from score -4.198296\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047329 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394910\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056967 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 400842, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198296\n",
      "[LightGBM] [Info] Start training from score -4.198296\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045143 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4100\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394911\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 400843, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198299\n",
      "[LightGBM] [Info] Start training from score -4.198299\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4103\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4102\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198426\n",
      "[LightGBM] [Info] Start training from score -4.198426\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198426\n",
      "[LightGBM] [Info] Start training from score -4.198426\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 320675, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198215\n",
      "[LightGBM] [Info] Start training from score -4.198215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045488 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 320675, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198215\n",
      "[LightGBM] [Info] Start training from score -4.198215\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394911\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4100\n",
      "[LightGBM] [Info] Number of data points in the train set: 400843, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198299\n",
      "[LightGBM] [Info] Start training from score -4.198299\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4102\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044339 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4100\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198426\n",
      "[LightGBM] [Info] Start training from score -4.198426\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198426\n",
      "[LightGBM] [Info] Start training from score -4.198426\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042846 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4100\n",
      "[LightGBM] [Info] Number of data points in the train set: 320675, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198215\n",
      "[LightGBM] [Info] Start training from score -4.198215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 320675, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198215\n",
      "[LightGBM] [Info] Start training from score -4.198215\n",
      "Logloss:\n",
      "0.016883891111904976\n",
      "ROC AUC Score: 0.9967732751830992\n",
      "0.011843919228928952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "# Logloss:\n",
    "# 0.01727067871139171\n",
    "\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, max_depth=11, random_state=42, min_samples_leaf=100, criterion=\"log_loss\", n_jobs=-1   )),\n",
    "    ('lgb', LGBMClassifier(n_estimators=60, max_depth=4,boosting_type=\"dart\", random_state=42)),\n",
    "    ('xgb', xgb.XGBClassifier(max_depth=2, n_estimators=70, random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
    "]\n",
    "meta_model = LogisticRegression()\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "\n",
    "predicted_probs = 'STACKED'\n",
    "probs, trained_model = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    'TARGET_EVENT_BINARY',\n",
    "    model= stacked_model,\n",
    "    augment_distribution=True,\n",
    "    augment_distribution_percentage=1.48,\n",
    "    calibrate=True,\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "print(probs.mean())\n",
    "# this helps\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'STACKED'] = 0\n",
    "\n",
    "# training_data['ENSEMBLE_STACKED'] = 0.9*training_data['ENSEMBLE_NN']+ 0.1*training_data['STACKED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['ENSEMBLE_STACKED'] = 0.5*training_data['LOGISTIC_REG']+ 0.5*training_data['STACKED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011389968960770625\n"
     ]
    }
   ],
   "source": [
    "new_proba = scale_yearly_proba(training_data, 'ENSEMBLE_STACKED')\n",
    "print(new_proba.mean())\n",
    "training_data['ENSEMBLE_STACKED_SCALED'] = new_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss:\n",
      "0.26356365759030304\n",
      "Before centering:\n",
      "0.9999999999993184 3.5809547743737014e-06 0.014705217226777713\n",
      "Centering probabilities...\n",
      "-9.47827732222916e-05 1.999\n",
      "-9.029485818285408e-05 1.9980000000000002\n",
      "-8.580389676962619e-05 1.9970000000000003\n",
      "-8.130988554106225e-05 1.9960000000000004\n",
      "-7.68128210499458e-05 1.9950000000000006\n",
      "-7.23126998434017e-05 1.9940000000000007\n",
      "-6.780951846291003e-05 1.9930000000000008\n",
      "-6.330327344424187e-05 1.9920000000000009\n",
      "-5.879396131748711e-05 1.991000000000001\n",
      "-5.4281578607054404e-05 1.990000000000001\n",
      "-4.976612183158098e-05 1.9890000000000012\n",
      "-4.5247587504062756e-05 1.9880000000000013\n",
      "-4.072597213164095e-05 1.9870000000000014\n",
      "-3.6201272215749536e-05 1.9860000000000015\n",
      "-3.167348425213434e-05 1.9850000000000017\n",
      "-2.7142604730655273e-05 1.9840000000000018\n",
      "-2.2608630135407745e-05 1.9830000000000019\n",
      "-1.8071556944701875e-05 1.982000000000002\n",
      "-1.3531381631010433e-05 1.981000000000002\n",
      "-8.988100660989665e-06 1.9800000000000022\n",
      "0.999999999999479 3.615300063430915e-06 0.014791011899339011\n",
      "Saved file: ./predictions/stacked-nn-xgb-log-lgbm.csv\n"
     ]
    }
   ],
   "source": [
    "submission = create_submission_file(training_data, 'ENSEMBLE_STACKED_SCALED', data_submission_example, filename='./predictions/stacked-nn-xgb-log-lgbm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean in year 0.011345816067598232\n",
      "5206\n",
      "1750.5931790499392\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4100\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033512 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4093\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4091\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4102\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4103\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034402 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4092\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 60\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4100\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4094\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033592 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4094\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045677 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4100\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4100\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033786 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4094\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039176 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035482 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038995 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4091\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040085 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4094\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4100\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041771 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4100\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033394 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032533 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256538, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198421\n",
      "[LightGBM] [Info] Start training from score -4.198421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032818 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4100\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038728 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4093\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 60\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4094\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4092\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 60\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4094\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4094\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038527 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4094\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4103\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4102\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198426\n",
      "[LightGBM] [Info] Start training from score -4.198426\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198426\n",
      "[LightGBM] [Info] Start training from score -4.198426\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 320675, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198215\n",
      "[LightGBM] [Info] Start training from score -4.198215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4101\n",
      "[LightGBM] [Info] Number of data points in the train set: 320675, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198215\n",
      "[LightGBM] [Info] Start training from score -4.198215\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4103\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4100\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4092\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037968 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252744\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198429\n",
      "[LightGBM] [Info] Start training from score -4.198429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4093\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4093\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033658 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4094\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252744\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198429\n",
      "[LightGBM] [Info] Start training from score -4.198429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4092\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 60\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252744\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4094\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198429\n",
      "[LightGBM] [Info] Start training from score -4.198429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4094\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252744\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198429\n",
      "[LightGBM] [Info] Start training from score -4.198429\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4102\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4100\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198426\n",
      "[LightGBM] [Info] Start training from score -4.198426\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198426\n",
      "[LightGBM] [Info] Start training from score -4.198426\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4100\n",
      "[LightGBM] [Info] Number of data points in the train set: 320675, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198215\n",
      "[LightGBM] [Info] Start training from score -4.198215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046958 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 320675, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198215\n",
      "[LightGBM] [Info] Start training from score -4.198215\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252742\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198158\n",
      "[LightGBM] [Info] Start training from score -4.198158\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035329 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4093\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4094\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252744\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4099\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198429\n",
      "[LightGBM] [Info] Start training from score -4.198429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4092\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4094\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4094\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256539, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198425\n",
      "[LightGBM] [Info] Start training from score -4.198425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252744\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4098\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198429\n",
      "[LightGBM] [Info] Start training from score -4.198429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4094\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4097\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4091\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 60\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252744\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198429\n",
      "[LightGBM] [Info] Start training from score -4.198429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031607 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4094\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3797, number of negative: 252743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4093\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014801 -> initscore=-4.198162\n",
      "[LightGBM] [Info] Start training from score -4.198162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3796, number of negative: 252744\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 256540, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198429\n",
      "[LightGBM] [Info] Start training from score -4.198429\n",
      "Logloss:\n",
      "0.01656447479879821\n",
      "ROC AUC Score: 0.9955872269259793\n",
      "0.011491446827399386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "# Logloss:\n",
    "# 0.01727067871139171\n",
    "model\n",
    "base_models = [\n",
    "    ('lr', CalibratedClassifierCV(base_estimator=LogisticRegression(max_iter=400, random_state=42,solver='lbfgs'), method='isotonic', )),\n",
    "    ('rf', CalibratedClassifierCV(base_estimator= RandomForestClassifier(n_estimators=100, max_depth=11, random_state=42, min_samples_leaf=100, criterion=\"log_loss\", n_jobs=-1   ), method='isotonic', )),\n",
    "    ('lgb', CalibratedClassifierCV(base_estimator= LGBMClassifier(n_estimators=60, max_depth=4,boosting_type=\"dart\", random_state=42), method='isotonic', )),\n",
    "    ('xgb', CalibratedClassifierCV(base_estimator= xgb.XGBClassifier(max_depth=2, n_estimators=70, random_state=42, use_label_encoder=False, eval_metric='logloss'), method='isotonic', ))\n",
    "]\n",
    "meta_model = LogisticRegression()\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "\n",
    "predicted_probs = 'STACKED'\n",
    "probs,trained_model_2 = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    'TARGET_EVENT_BINARY',\n",
    "    model= stacked_model,\n",
    "    augment_distribution=True,\n",
    "    augment_distribution_percentage=1.48,\n",
    "    calibrate=True,\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "print(probs.mean())\n",
    "# this helps\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'STACKED'] = 0\n",
    "\n",
    "# training_data['ENSEMBLE_STACKED'] = 0.9*training_data['ENSEMBLE_NN']+ 0.1*training_data['STACKED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011369975080190843\n",
      "Logloss:\n",
      "0.2670883780636673\n",
      "Before centering:\n",
      "1.0 2.8514065779416953e-05 0.01450183927509637\n",
      "Centering probabilities...\n",
      "-0.0002981607249036289 1.999\n",
      "-0.0002941216245565814 1.9980000000000002\n",
      "-0.000290079834252565 1.9970000000000003\n",
      "-0.0002860353509213133 1.9960000000000004\n",
      "-0.0002819881714875433 1.9950000000000006\n",
      "-0.000277938292870851 1.9940000000000007\n",
      "-0.0002738857119857271 1.9930000000000008\n",
      "-0.0002698304257416021 1.9920000000000009\n",
      "-0.0002657724310427545 1.991000000000001\n",
      "-0.00026171172478834694 1.990000000000001\n",
      "-0.0002576483038724299 1.9890000000000012\n",
      "-0.0002535821651838792 1.9880000000000013\n",
      "-0.0002495133056063959 1.9870000000000014\n",
      "-0.00024544172201856375 1.9860000000000015\n",
      "-0.00024136741129374487 1.9850000000000017\n",
      "-0.00023729037030010072 1.9840000000000018\n",
      "-0.00023321059590063198 1.9830000000000019\n",
      "-0.00022912808495307094 1.982000000000002\n",
      "-0.00022504283430997522 1.981000000000002\n",
      "-0.00022095484081863584 1.9800000000000022\n",
      "-0.0002168641013210841 1.9790000000000023\n",
      "-0.00021277061265411595 1.9780000000000024\n",
      "-0.0002086743716492312 1.9770000000000025\n",
      "-0.00020457537513265435 1.9760000000000026\n",
      "-0.00020047361992535026 1.9750000000000028\n",
      "-0.00019636910284288878 1.9740000000000029\n",
      "-0.00019226182069562864 1.973000000000003\n",
      "-0.00018815177028850237 1.972000000000003\n",
      "-0.00018403894842116374 1.9710000000000032\n",
      "-0.00017992335188790794 1.9700000000000033\n",
      "-0.00017580497747762824 1.9690000000000034\n",
      "-0.00017168382197384723 1.9680000000000035\n",
      "-0.00016755988215472542 1.9670000000000036\n",
      "-0.00016343315479302664 1.9660000000000037\n",
      "-0.00015930363665605206 1.9650000000000039\n",
      "-0.0001551713245057304 1.964000000000004\n",
      "-0.00015103621509852255 1.963000000000004\n",
      "-0.00014689830518546318 1.9620000000000042\n",
      "-0.00014275759151211216 1.9610000000000043\n",
      "-0.00013861407081853724 1.9600000000000044\n",
      "-0.0001344677398393765 1.9590000000000045\n",
      "-0.0001303185953037099 1.9580000000000046\n",
      "-0.00012616663393514964 1.9570000000000047\n",
      "-0.0001220118524517845 1.9560000000000048\n",
      "-0.0001178542475661331 1.955000000000005\n",
      "-0.0001136938159851976 1.954000000000005\n",
      "-0.00010953055441045335 1.9530000000000052\n",
      "-0.000105364459537724 1.9520000000000053\n",
      "-0.00010119552805731846 1.9510000000000054\n",
      "-9.70237566539512e-05 1.9500000000000055\n",
      "-9.284914200667974e-05 1.9490000000000056\n",
      "-8.867168078897057e-05 1.9480000000000057\n",
      "-8.449136966868186e-05 1.9470000000000058\n",
      "-8.030820530797317e-05 1.946000000000006\n",
      "-7.61221843634096e-05 1.945000000000006\n",
      "-7.193330348584034e-05 1.9440000000000062\n",
      "-6.774155932042988e-05 1.9430000000000063\n",
      "-6.354694850667884e-05 1.9420000000000064\n",
      "-5.9349467678370196e-05 1.9410000000000065\n",
      "-5.5149113463550184e-05 1.9400000000000066\n",
      "-5.094588248456301e-05 1.9390000000000067\n",
      "-4.673977135797279e-05 1.9380000000000068\n",
      "-4.2530776694586075e-05 1.937000000000007\n",
      "-3.8318895099477904e-05 1.936000000000007\n",
      "-3.410412317189637e-05 1.9350000000000072\n",
      "-2.9886457505323358e-05 1.9340000000000073\n",
      "-2.5665894687396454e-05 1.9330000000000074\n",
      "-2.144243129996795e-05 1.9320000000000075\n",
      "-1.7216063919016367e-05 1.9310000000000076\n",
      "-1.2986789114715833e-05 1.9300000000000077\n",
      "-8.754603451302523e-06 1.9290000000000078\n",
      "1.0 2.9548239414367572e-05 0.014791245396548698\n",
      "Saved file: ./predictions/stacked-nn-xgb-log-lgbm-v2.csv\n"
     ]
    }
   ],
   "source": [
    "training_data['ENSEMBLE_STACKED'] =0.5*training_data['ENSEMBLE_NN']+0.5* training_data['STACKED']\n",
    "new_proba = scale_yearly_proba(training_data, 'ENSEMBLE_STACKED')\n",
    "print(new_proba.mean())\n",
    "training_data['ENSEMBLE_STACKED_SCALED'] = new_proba\n",
    "submission = create_submission_file(training_data, 'ENSEMBLE_STACKED_SCALED', data_submission_example, filename='./predictions/stacked-nn-xgb-log-lgbm-v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_2.base_estimator.final_estimator.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CalibratedClassifierCV' object has no attribute 'final_estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\takat\\Documents\\Egyetem\\Gpi tanulsi esettanulmny labor\\Defaults\\two-more-dummies.ipynb Cell 51\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y213sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trained_model\u001b[39m.\u001b[39;49mfinal_estimator\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CalibratedClassifierCV' object has no attribute 'final_estimator'"
     ]
    }
   ],
   "source": [
    "trained_model.final_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean in year 0.011345816067598232\n",
      "5206\n",
      "1750.5931790499392\n",
      "Logloss:\n",
      "0.03345960035278355\n",
      "ROC AUC Score: 0.9690328434892286\n",
      "0.019327983772770608\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "\n",
    "predicted_probs = 'QDA'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    'TARGET_EVENT_BINARY',\n",
    "    model= QuadraticDiscriminantAnalysis(),\n",
    "    augment_distribution=True,\n",
    "    augment_distribution_percentage=1.48,\n",
    "    calibrate=True,\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "print(probs.mean())\n",
    "# this helps\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'QDA'] = 0\n",
    "\n",
    "training_data['ENSEMBLE_QDA'] = 0.9*training_data['ENSEMBLE_NN']+ 0.1*training_data['QDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean in year 0.011345816067598232\n",
      "5206\n",
      "1750.5931790499392\n",
      "Logloss:\n",
      "0.029537779758320133\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 880. MiB for an array with shape (1601663, 72) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 589, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py\", line 206, in _parallel_decision_function\n    return sum(\n           ^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py\", line 207, in <genexpr>\n    estimator.decision_function(X[:, features])\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 783, in decision_function\n    dec = self._decision_function(X)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 532, in _decision_function\n    X = self._validate_for_predict(X)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 611, in _validate_for_predict\n    X = self._validate_data(\n        ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 604, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 880. MiB for an array with shape (1601663, 72) and data type float64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\takat\\Documents\\Egyetem\\Gpi tanulsi esettanulmny labor\\Defaults\\two-more-dummies.ipynb Cell 49\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Logloss:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# 0.025080414519332874 1% & 10 estimators\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m predicted_probs \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSVC\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m probs \u001b[39m=\u001b[39m train_and_predict_two_halves(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     training_data, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     X_columns, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mTARGET_EVENT_BINARY\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     model\u001b[39m=\u001b[39;49m BaggingClassifier(SVC(kernel\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrbf\u001b[39;49m\u001b[39m'\u001b[39;49m, probability\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), max_samples\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m, n_estimators\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     augment_distribution\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     augment_distribution_percentage\u001b[39m=\u001b[39;49m\u001b[39m1.48\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     calibrate\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m training_data[predicted_probs] \u001b[39m=\u001b[39m probs\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(probs\u001b[39m.\u001b[39mmean())\n",
      "\u001b[1;32mc:\\Users\\takat\\Documents\\Egyetem\\Gpi tanulsi esettanulmny labor\\Defaults\\two-more-dummies.ipynb Cell 49\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m test_proba \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_proba(X_filtered)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(log_loss(y_filtered,test_proba))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m proba \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_proba(X_scaled)[:, \u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Additional code for ROC AUC score\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m roc_auc \u001b[39m=\u001b[39m roc_auc_score(y_filtered,test_proba[:,\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\calibration.py:500\u001b[0m, in \u001b[0;36mCalibratedClassifierCV.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    498\u001b[0m mean_proba \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((_num_samples(X), \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_)))\n\u001b[0;32m    499\u001b[0m \u001b[39mfor\u001b[39;00m calibrated_classifier \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalibrated_classifiers_:\n\u001b[1;32m--> 500\u001b[0m     proba \u001b[39m=\u001b[39m calibrated_classifier\u001b[39m.\u001b[39;49mpredict_proba(X)\n\u001b[0;32m    501\u001b[0m     mean_proba \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m proba\n\u001b[0;32m    503\u001b[0m mean_proba \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalibrated_classifiers_)\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\calibration.py:792\u001b[0m, in \u001b[0;36m_CalibratedClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    790\u001b[0m n_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses)\n\u001b[0;32m    791\u001b[0m pred_method, method_name \u001b[39m=\u001b[39m _get_prediction_method(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator)\n\u001b[1;32m--> 792\u001b[0m predictions \u001b[39m=\u001b[39m _compute_predictions(pred_method, method_name, X, n_classes)\n\u001b[0;32m    794\u001b[0m label_encoder \u001b[39m=\u001b[39m LabelEncoder()\u001b[39m.\u001b[39mfit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses)\n\u001b[0;32m    795\u001b[0m pos_class_indices \u001b[39m=\u001b[39m label_encoder\u001b[39m.\u001b[39mtransform(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator\u001b[39m.\u001b[39mclasses_)\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\calibration.py:683\u001b[0m, in \u001b[0;36m_compute_predictions\u001b[1;34m(pred_method, method_name, X, n_classes)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_compute_predictions\u001b[39m(pred_method, method_name, X, n_classes):\n\u001b[0;32m    660\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return predictions for `X` and reshape binary outputs to shape\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[39m    (n_samples, 1).\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[39m        (X.shape[0], 1).\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 683\u001b[0m     predictions \u001b[39m=\u001b[39m pred_method(X\u001b[39m=\u001b[39;49mX)\n\u001b[0;32m    685\u001b[0m     \u001b[39mif\u001b[39;00m method_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdecision_function\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    686\u001b[0m         \u001b[39mif\u001b[39;00m predictions\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:970\u001b[0m, in \u001b[0;36mBaggingClassifier.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[39m# Parallel loop\u001b[39;00m\n\u001b[0;32m    968\u001b[0m n_jobs, _, starts \u001b[39m=\u001b[39m _partition_estimators(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n\u001b[1;32m--> 970\u001b[0m all_decisions \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose)(\n\u001b[0;32m    971\u001b[0m     delayed(_parallel_decision_function)(\n\u001b[0;32m    972\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimators_[starts[i] : starts[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]],\n\u001b[0;32m    973\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimators_features_[starts[i] : starts[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]],\n\u001b[0;32m    974\u001b[0m         X,\n\u001b[0;32m    975\u001b[0m     )\n\u001b[0;32m    976\u001b[0m     \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(n_jobs)\n\u001b[0;32m    977\u001b[0m )\n\u001b[0;32m    979\u001b[0m \u001b[39m# Reduce\u001b[39;00m\n\u001b[0;32m    980\u001b[0m decisions \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(all_decisions) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1693\u001b[0m \n\u001b[0;32m   1694\u001b[0m     \u001b[39m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m     \u001b[39m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[39m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     \u001b[39m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aborting:\n\u001b[1;32m-> 1699\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_error_fast()\n\u001b[0;32m   1700\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m     \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m     \u001b[39m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[39m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[39m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m \u001b[39m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[39mif\u001b[39;00m error_job \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1734\u001b[0m     error_job\u001b[39m.\u001b[39;49mget_result(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    730\u001b[0m backend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel\u001b[39m.\u001b[39m_backend\n\u001b[0;32m    732\u001b[0m \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    733\u001b[0m     \u001b[39m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[39m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[39m# be returned.\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_return_or_raise()\n\u001b[0;32m    738\u001b[0m \u001b[39m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 754\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[0;32m    756\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 880. MiB for an array with shape (1601663, 72) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Logloss:\n",
    "# 0.025080414519332874 1% & 10 estimators\n",
    "predicted_probs = 'SVC'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    'TARGET_EVENT_BINARY',\n",
    "    model= BaggingClassifier(SVC(kernel='rbf', probability=True), max_samples=2000, n_estimators=100, n_jobs=-1, random_state=42),\n",
    "    augment_distribution=True,\n",
    "    augment_distribution_percentage=1.48,\n",
    "    calibrate=True,\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "print(probs.mean())\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'SVC'] = 0\n",
    "training_data['ENSEMBLE_SVC'] = 0.9*training_data['ENSEMBLE_LGBM']+ 0.1*training_data['SVC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_proba = scale_yearly_proba(training_data, 'ENSEMBLE_SVC')\n",
    "print(new_proba.mean())\n",
    "training_data['NN_LOG_XGB_LGBM_SVC'] = new_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logloss:\n",
    "# 0.2627792536515289: \n",
    "submission = create_submission_file(training_data, 'NN_LOG_XGB_LGBM_SVC', data_submission_example, filename='./predictions/nn-xgb-log-lgbm-svc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "# Other necessary imports, like pandas, if you're using a DataFrame\n",
    "\n",
    "def train_and_evaluate(df, variables, target, model=LogisticRegression(), scaler=StandardScaler(), cv_folds=5, augment_distribution=False):\n",
    "    # Your lognormal_variables transformation code remains the same\n",
    "    if lognormal_variables is not None:\n",
    "        df = df.copy()\n",
    "        for var in lognormal_variables:\n",
    "            if var == 'CONTRACT_CREDIT_LOSS':\n",
    "                df[var] = np.log1p(np.abs(df[var]))*np.sign(df[var])\n",
    "            else:\n",
    "                df[var] = np.log1p(df[var])\n",
    "\n",
    "    # Split the data before the given date for training \n",
    "    if augment_distribution:\n",
    "        df_filtered = default_percentage_generator_2016(df, 1.48/100, target)\n",
    "    else:\n",
    "        df_filtered = df[df['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] < '2016-01-01']\n",
    "    X_filtered = df_filtered[variables]\n",
    "    y_filtered = df_filtered[target]\n",
    "    \n",
    "    # Scale the filtered features\n",
    "    X_filtered_scaled = scaler.fit_transform(X_filtered)\n",
    "    \n",
    "    # Perform cross-validation and fit the model\n",
    "    cv_scores = cross_val_score(model, X_filtered_scaled, y_filtered, cv=cv_folds, scoring='neg_log_loss')\n",
    "    \n",
    "    print(f\"CV Log Loss Scores: {-cv_scores}\")\n",
    "    print(f\"Mean CV Log Loss: {-np.mean(cv_scores)}\")\n",
    "    \n",
    "    # Now fit the model to the entire dataset\n",
    "    model.fit(X_filtered_scaled, y_filtered)\n",
    "    \n",
    "    # Scale the entire dataset to predict probabilities for all records\n",
    "    X_scaled = scaler.transform(df[variables])\n",
    "    \n",
    "    # Predict probabilities using the trained model\n",
    "    proba = model.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "    print(\"Logloss:\")\n",
    "    print(log_loss(y_filtered, model.predict_proba(X_filtered_scaled)))\n",
    "    # Calculate log loss on the full dataset\n",
    "    full_log_loss = log_loss(df[target], proba)\n",
    "\n",
    "    print(f\"Full Dataset Log Loss: {full_log_loss}\")\n",
    "\n",
    "    return proba, cv_scores, full_log_loss\n",
    "\n",
    "# You might call the function like this\n",
    "# probabilities, cv_scores, full_log_loss = train_and_evaluate(df, variables, target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logloss xgb :\n",
    "# 0.016317211616370328\n",
    "# Logloss logistic regression:\n",
    "# 0.02385418586559387\n",
    "predicted_probs = 'XGBOOST'\n",
    "probs, _, _ = train_and_evaluate(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    'TARGET_EVENT_BINARY',\n",
    "    model=xgb.XGBClassifier(max_depth=3, reg_lambda=1, n_estimators=60, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    augment_distribution=True\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "print(probs.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_number = 1.6\n",
    "experiment_probs = calculate_probabilities(training_data, 'XGBOOST', magic_number)\n",
    "print(experiment_probs.mean(), experiment_probs.max(), experiment_probs.min(), experiment_probs.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['EXPERIMENT_PROBS_XGB'] = experiment_probs\n",
    "submission_exp = create_submission_file(training_data, 'EXPERIMENT_PROBS_XGB', data_submission_example, filename='./predictions/xgb-prediction-with-time-param-0-585.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(experiment_probs.mean(), experiment_probs.max(), experiment_probs.min(), experiment_probs.std())\n",
    "sns.kdeplot(submission, label='Submission OG', lw=3, alpha=1)\n",
    "sns.kdeplot(training_data['LOGISTIC_REG'], label='PredictionOG', lw=3, alpha=1)\n",
    "sns.kdeplot(experiment_probs, label='Fine tuned solution', lw=3, alpha=1)\n",
    "sns.kdeplot(submission_exp, label='Fine tuned solution per borrower', lw=3, alpha=1)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('Overlayed Histogram of Submission and Predicted Probabilities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (submission_exp['PRED'] > 0.9999)# | (submission_exp['PRED'] < 0.01)\n",
    "ensemble_submission = submission.copy()\n",
    "ensemble_submission.loc[condition,'PRED'] = submission_exp.loc[condition,'PRED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_submission['PRED'].mean()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_probabilities_vec(ensemble_submission['PRED'].values, 2.029).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_submission['PRED'] = calculate_probabilities_vec(ensemble_submission['PRED'].values, 2.029)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_submission.to_csv('./predictions/ensemble-submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(experiment_probs.mean(), experiment_probs.max(), experiment_probs.min(), experiment_probs.std())\n",
    "sns.kdeplot(submission, label='Submission OG', lw=3, alpha=1, color='r')\n",
    "# sns.kdeplot(training_data['LOGISTIC_REG'], label='PredictionOG', lw=3, alpha=1)\n",
    "sns.kdeplot(ensemble_submission, label='Fine tuned solution', lw=3, alpha=1, color='g')\n",
    "# sns.kdeplot(submission_exp, label='Fine tuned solution per borrower', lw=3, alpha=1)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('Overlayed Histogram of Submission and Predicted Probabilities')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
