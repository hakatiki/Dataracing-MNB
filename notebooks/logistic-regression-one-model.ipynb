{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni_D83AE5_uni (Baseline)\n",
    "#  {\"Logloss\": 0.04508655474248735, \"ROCAUC\": 0.9744860696967259}\n",
    "import pandas as pd\n",
    "# Importing necessary libraries for logistic regression and scaling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, log_loss, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "# Load the data\n",
    "training_data = pd.read_csv('training_data.csv')\n",
    "\n",
    "# Drop rows where BORROWER_ID is 'xNullx'\n",
    "training_data = training_data[training_data['BORROWER_ID'] != 'xNullx']\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "training_data = training_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "data_submission_example = pd.read_csv('data_submission_example.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Assuming training_data =  training_data.sample(n=10000, random_state=1)\n",
    "\n",
    "# Filling NaN values with 0\n",
    "training_data.fillna(0, inplace=True)\n",
    "\n",
    "# Converting columns to numeric where possible\n",
    "for col in training_data.columns:\n",
    "    try:\n",
    "        training_data[col] = pd.to_numeric(training_data[col], errors='ignore')\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Creating a sample target variable\n",
    "training_data['TARGET_EVENT_BINARY'] = np.where(training_data['TARGET_EVENT'] == 'K', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lognormal_variables = [\n",
    "    'CONTRACT_CREDIT_LOSS', 'CONTRACT_DEPT_SERVICE_TO_INCOME',\n",
    "    'CONTRACT_INCOME', 'CONTRACT_INSTALMENT_AMOUNT', 'CONTRACT_INSTALMENT_AMOUNT_2',\n",
    "    'CONTRACT_LOAN_AMOUNT', 'CONTRACT_MARKET_VALUE', 'CONTRACT_MORTGAGE_LENDING_VALUE', \n",
    "    'CONTRACT_LGD', 'CONTRACT_INCOME'\n",
    "]\n",
    "date_variables = ['CONTRACT_DATE_OF_LOAN_AGREEMENT', 'CONTRACT_MATURITY_DATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['TARGET_EVENT_DAY_DATETIME'] = pd.to_datetime(training_data['TARGET_EVENT_DAY'])\n",
    "training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] = pd.to_datetime(training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT'])\n",
    "\n",
    "# Calculate the day difference\n",
    "training_data['DAY_DIFF'] = (training_data['TARGET_EVENT_DAY_DATETIME'] - training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME']).dt.days\n",
    "\n",
    "# Create TARGET_EVENT_BINARY_2Y based on conditions\n",
    "training_data['TARGET_EVENT_BINARY_2Y'] = np.where(\n",
    "    (training_data['TARGET_EVENT'] == 'K') & \n",
    "    (training_data['DAY_DIFF'] <= 730) & \n",
    "    (training_data['DAY_DIFF'] >= 0), \n",
    "    1, \n",
    "    0\n",
    ")\n",
    "training_data['TARGET_EVENT_BINARY_1Y'] = np.where(\n",
    "        (training_data['TARGET_EVENT'] == 'K') & \n",
    "        (training_data['DAY_DIFF'] <= 365) & \n",
    "        (training_data['DAY_DIFF'] >= 0), \n",
    "        1, \n",
    "        0\n",
    "    )\n",
    "# Drop the temporary 'DAY_DIFF' column if needed\n",
    "training_data.drop('DAY_DIFF', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CONTRACT_CREDIT_INTERMEDIARY',\n",
       " 'CONTRACT_CREDIT_LOSS',\n",
       " 'CONTRACT_CURRENCY',\n",
       " 'CONTRACT_DEPT_SERVICE_TO_INCOME',\n",
       " 'CONTRACT_INCOME',\n",
       " 'CONTRACT_INSTALMENT_AMOUNT',\n",
       " 'CONTRACT_INSTALMENT_AMOUNT_2',\n",
       " 'CONTRACT_INTEREST_PERIOD',\n",
       " 'CONTRACT_INTEREST_RATE',\n",
       " 'CONTRACT_LGD',\n",
       " 'CONTRACT_LOAN_AMOUNT',\n",
       " 'CONTRACT_LOAN_CONTRACT_TYPE',\n",
       " 'CONTRACT_LOAN_TO_VALUE_RATIO',\n",
       " 'CONTRACT_MARKET_VALUE',\n",
       " 'CONTRACT_MORTGAGE_LENDING_VALUE',\n",
       " 'CONTRACT_MORTGAGE_TYPE',\n",
       " 'CONTRACT_REFINANCED',\n",
       " 'CONTRACT_RISK_WEIGHTED_ASSETS',\n",
       " 'CONTRACT_TYPE_OF_INTEREST_REPAYMENT',\n",
       " 'BORROWER_BIRTH_YEAR',\n",
       " 'BORROWER_CITIZENSHIP',\n",
       " 'BORROWER_COUNTRY',\n",
       " 'BORROWER_COUNTY',\n",
       " 'BORROWER_TYPE_OF_SETTLEMENT']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify numeric columns\n",
    "numeric_columns = training_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Identify target columns that shouldn't be in the X variables\n",
    "excluded_keywords = ['TARGET', 'event', 'binary', 'DATE']\n",
    "\n",
    "# Create lists for X variable columns and target column\n",
    "X_columns = [col for col in numeric_columns if all(keyword.lower() not in col.lower() for keyword in excluded_keywords)]\n",
    "y_column = 'TARGET_EVENT_BINARY_2Y' \n",
    "X_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in date_variables:\n",
    "    training_data[var+ '_JULIAN'] = pd.to_datetime(training_data[var], origin='julian', unit='D')\n",
    "\n",
    "training_data['TIME_TO_MATURITY'] = (training_data['CONTRACT_MATURITY_DATE_JULIAN'] - training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN']).dt.days\n",
    "\n",
    "year_2018_date = pd.Timestamp('2018-01-01')\n",
    "training_data['TIME_TO_MATURITY'] = (training_data['CONTRACT_MATURITY_DATE_JULIAN'] - year_2018_date).dt.days\n",
    "training_data['ADJUSTED_TIME_TO_MATURITY'] = training_data['TIME_TO_MATURITY'].apply(lambda x: max(min(730, x),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the 'CONTRACT_LOAN_TYPE' and 'CONTRACT_FREQUENCY_TYPE' columns\n",
    "loan_type_dummies = pd.get_dummies(training_data['CONTRACT_LOAN_TYPE'], prefix='LOAN_TYPE', drop_first=True)\n",
    "frequency_type_dummies = pd.get_dummies(training_data['CONTRACT_FREQUENCY_TYPE'], prefix='FREQ_TYPE', drop_first=True)\n",
    "\n",
    "# Concatenate the original DataFrame with the one-hot encoded columns\n",
    "training_data = pd.concat([training_data, loan_type_dummies, frequency_type_dummies], axis=1)\n",
    "\n",
    "# Add the names of the one-hot encoded columns to X_columns\n",
    "X_columns.extend(loan_type_dummies.columns)\n",
    "X_columns.extend(frequency_type_dummies.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['BORROWER_LOAN_COUNT'] = training_data.groupby('BORROWER_ID')['BORROWER_ID'].transform('count')\n",
    "training_data['TOTAL_LOAN_AMOUNT'] = training_data.groupby('BORROWER_ID')['CONTRACT_LOAN_AMOUNT'].transform('sum')\n",
    "training_data['TOTAL_INSTALLMENT_AMOUNT_1'] = training_data.groupby('BORROWER_ID')['CONTRACT_INSTALMENT_AMOUNT'].transform('sum')\n",
    "training_data['TOTAL_INSTALLMENT_AMOUNT_2'] = training_data.groupby('BORROWER_ID')['CONTRACT_INSTALMENT_AMOUNT_2'].transform('sum')\n",
    "training_data['TOTAL_INSTALLMENT_AMOUNT'] = training_data['TOTAL_INSTALLMENT_AMOUNT_1'] + training_data['TOTAL_INSTALLMENT_AMOUNT_2']\n",
    "# training_data['AVERAGE_LOAN_AMOUNT'] = training_data['TOTAL_LOAN_AMOUNT'] / training_data['BORROWER_LOAN_COUNT']\n",
    "# training_data['AVERAGE_LOAN_TERM'] = training_data.groupby('BORROWER_ID')['CONTRACT_MATURITY_DATE'].transform('mean')\n",
    "# training_data['AVERAGE_INTEREST_RATE'] = training_data.groupby('BORROWER_ID')['CONTRACT_INTEREST_RATE'].transform('mean')\n",
    "# training_data['MAX_DEBT_TO_INCOME'] = training_data.groupby('BORROWER_ID')['CONTRACT_DEPT_SERVICE_TO_INCOME'].transform('max')\n",
    "# training_data['HAS_MULTIPLE_LOAN_TYPES'] = training_data.groupby('BORROWER_ID')['CONTRACT_LOAN_TYPE'].transform('nunique').apply(lambda x: 1 if x > 1 else 0)\n",
    "# training_data['HAS_REFINANCED'] = training_data.groupby('BORROWER_ID')['CONTRACT_REFINANCED'].transform('max')\n",
    "\n",
    "# # # training_data['RELATIVE_LOAN_AMOUNT'] = training_data['CONTRACT_LOAN_AMOUNT'] / training_data['TOTAL_LOAN_AMOUNT']\n",
    "# training_data['IS_FIXED_RATE'] = training_data['CONTRACT_LOAN_CONTRACT_TYPE'].apply(lambda x: 1 if x == 'fixed' else 0)\n",
    "# # Ez még nem tudom hogyan de hasznos lehet \n",
    "# # Assuming CONTRACT_MATURITY_DATE is a datetime object and 'current_date' is today's date\n",
    "# # training_data['DAYS_TO_MATURITY'] = (training_data['CONTRACT_MATURITY_DATE'] - current_date).dt.days\n",
    "X_columns.extend(['BORROWER_LOAN_COUNT', 'TOTAL_LOAN_AMOUNT','TOTAL_INSTALLMENT_AMOUNT'])\n",
    "lognormal_variables.extend([ 'TOTAL_LOAN_AMOUNT','TOTAL_INSTALLMENT_AMOUNT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables removed: {'FREQ_TYPE_2f88e16c', 'BORROWER_COUNTRY', 'CONTRACT_REFINANCED'}\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.85  # Set your own threshold\n",
    "correlation_matrix = training_data[X_columns].corr()\n",
    "# Get pairs of highly correlated features\n",
    "highly_correlated_set = set()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            highly_correlated_set.add(colname)\n",
    "\n",
    "# Remove highly correlated features from X_columns\n",
    "X_columns = [col for col in X_columns if col not in highly_correlated_set]\n",
    "print('Variables removed:', highly_correlated_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, log_loss, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def filter_2016(df1, df2, variables):\n",
    "    # Create deep copies to avoid modifying original data\n",
    "    df1_copy = df1.copy()\n",
    "    df2_copy = df2.copy()\n",
    "\n",
    "    # Convert to datetime format if not already\n",
    "    df1_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'] = pd.to_datetime(df1_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'])\n",
    "    df2_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'] = pd.to_datetime(df2_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'])\n",
    "    \n",
    "    # Filter data where CONTRACT_DATE_OF_LOAN_AGREEMENT is before 2016\n",
    "    df1_filtered = df1_copy[df1_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'].dt.year < 2016]\n",
    "    df2_filtered = df2_copy[df2_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'].dt.year < 2016]\n",
    "    \n",
    "    df1_filtered['CONTRACT_DATE_OF_LOAN_AGREEMENT'] = df1_filtered['CONTRACT_DATE_OF_LOAN_AGREEMENT'].apply(lambda x: x.toordinal())\n",
    "    df2_filtered['CONTRACT_DATE_OF_LOAN_AGREEMENT'] = df2_filtered['CONTRACT_DATE_OF_LOAN_AGREEMENT'].apply(lambda x: x.toordinal())\n",
    "\n",
    "    return df1_filtered, df2_filtered\n",
    "\n",
    "def train_and_predict_two_halves(df, variables, target, model1=LogisticRegression(), scaler=StandardScaler()):\n",
    "    # Split the dataframe into two halves\n",
    "    if lognormal_variables is not None:\n",
    "        df = df.copy()\n",
    "        \n",
    "        for var in lognormal_variables:\n",
    "            if var == 'CONTRACT_CREDIT_LOSS':\n",
    "                df[var] = np.log1p(np.abs(df[var]))*np.sign(df[var])\n",
    "            else:\n",
    "                df[var] = np.log1p(df[var])\n",
    "\n",
    "    half = len(df) // 2\n",
    "    df1, df2 = df.iloc[:half], df.iloc[half:]\n",
    "    #######################x########################\n",
    "    # FILTERING TO 2016 MAKES IT A TINY BIT WORSE\n",
    "    #######################x########################\n",
    "    # df1, df2 = filter_2016(df1, df2, variables)\n",
    "    # Scale the entire dataset using a single scaler\n",
    "    X_scaled = scaler.fit_transform(df[variables])\n",
    "    # X1_scaled = scaler.transform(df1[variables])\n",
    "    # X2_scaled = scaler.transform(df2[variables])\n",
    "    \n",
    "    # Prepare target variables\n",
    "    y = df[target]\n",
    "    # y1 = df1[target]\n",
    "    # y2 = df2[target]\n",
    "    model1.fit(X_scaled, y)\n",
    "    proba = model1.predict_proba(X_scaled)[:, 1]\n",
    "    # y2_prob = model1.predict_proba(X2_scaled)[:, 1]\n",
    "    # Train model1 on df1 and get probabilities on df2\n",
    "    # model1.fit(X1_scaled, y1)\n",
    "    # y2_prob = model1.predict_proba(X2_scaled)[:, 1]\n",
    "    \n",
    "    # # Train model2 on df2 and get probabilities on df1\n",
    "    # model2.fit(X2_scaled, y2)\n",
    "    # y1_prob = model2.predict_proba(X1_scaled)[:, 1]\n",
    "    \n",
    "    # Evaluate model1 on df2\n",
    "    # print(\"Evaluation of Model 1 on df2:\")\n",
    "    # print(\"Classification Report:\")\n",
    "    # print(classification_report(y2, y2_prob >= 0.5))\n",
    "    # print(\"Log Loss:\")\n",
    "    # print(log_loss(y2, y2_prob))\n",
    "    # print(\"Confusion Matrix:\")\n",
    "    # print(confusion_matrix(y2, y2_prob >= 0.5))\n",
    "\n",
    "    # # Evaluate model2 on df1\n",
    "    # print(\"Evaluation of Model 2 on df1:\")\n",
    "    # print(\"Classification Report:\")\n",
    "    # print(classification_report(y1, y1_prob >= 0.5))\n",
    "    # print(\"Log Loss:\")\n",
    "    # print(log_loss(y1, y1_prob))\n",
    "    # print(\"Confusion Matrix:\")\n",
    "    # print(confusion_matrix(y1, y1_prob >= 0.5))\n",
    "    \n",
    "    # # Join the predicted probabilities\n",
    "    # joined_prob = np.concatenate([y1_prob, y2_prob])\n",
    "    \n",
    "    return proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "def significant_features(df, variables, target, model1=LogisticRegression(), scaler=StandardScaler()):\n",
    "    if lognormal_variables is not None:\n",
    "        df = df.copy()\n",
    "        \n",
    "        for var in lognormal_variables:\n",
    "            if var == 'CONTRACT_CREDIT_LOSS':\n",
    "                df[var] = np.log1p(np.abs(df[var]))*np.sign(df[var])\n",
    "            else:\n",
    "                df[var] = np.log1p(df[var])\n",
    "\n",
    "    X_scaled = scaler.fit_transform(df[variables])\n",
    "\n",
    "    X = sm.add_constant(pd.DataFrame(X_scaled, columns=variables))\n",
    "    y = df[target]\n",
    "\n",
    "    # Fit the logistic regression model\n",
    "    model = sm.Logit(y, X).fit()\n",
    "\n",
    "    # Display the summary\n",
    "    print(model.summary())\n",
    "    # Get the p-values\n",
    "    p_values = model.pvalues\n",
    "\n",
    "    # Identify the non-significant variables\n",
    "    non_significant_vars = p_values[p_values > 0.05].index.tolist()\n",
    "\n",
    "    return model, non_significant_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def cut_exponential_tails(df, target):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Step 1: Estimate lambda for each row based on its 'target' value and 'ADJUSTED_TIME_TO_MATURITY'\n",
    "    df['LAMBDA_ESTIMATE'] = -np.log(1 - df[target])/730\n",
    "\n",
    "    # Step 2: Calculate new probabilities p_exp for each row based on its own lambda_estimate\n",
    "    df[target] = 1 - np.exp(-df['LAMBDA_ESTIMATE'] * df['ADJUSTED_TIME_TO_MATURITY'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def combined_probability(s):\n",
    "    return 1 - np.prod(1 - s.values)\n",
    "\n",
    "def create_submission_file(df_preds, target, example, filename='submission.csv', testing=False):\n",
    "    # Filter the data to only include BORROWER_IDs that are in the submission example\n",
    "    filtered_training_data = df_preds[df_preds['BORROWER_ID'].isin(example['BORROWER_ID'])]\n",
    "\n",
    "    # Print warning if the row count is off\n",
    "    if not testing and len(filtered_training_data) != 1564601:\n",
    "        print('WARNING: The filtered data does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "        # raise ValueError('WARNING: The submission file does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "    # Group by BORROWER_ID and calculate the combined probability\n",
    "\n",
    "    #######################x########################\n",
    "    #CUTTING TAILS DID NOT SEEM TO WORK\n",
    "    #######################x########################\n",
    "    # filtered_training_data = cut_exponential_tails(filtered_training_data, target)\n",
    "    grouped_data = filtered_training_data.groupby('BORROWER_ID')[target].apply(combined_probability).reset_index()\n",
    "\n",
    "    # Create the submission DataFrame\n",
    "    df_submission = pd.DataFrame()\n",
    "    df_submission['BORROWER_ID'] = grouped_data['BORROWER_ID']\n",
    "    df_submission['PRED'] = grouped_data[target]\n",
    "    print('Centering probabilities...')\n",
    "    # Center the probabilities around 1.48%\n",
    "    desired_mean = 0.0148  # 1.48% as a decimal\n",
    "    while (df_submission['PRED'].max() > 1 or df_submission['PRED'].min() < 0 or abs(df_submission['PRED'].mean() -0.0148) > 0.0005):\n",
    "        # print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "        df_submission['PRED'] = df_submission['PRED'].clip(lower=0, upper=1)\n",
    "        # print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "\n",
    "        current_mean = df_submission['PRED'].mean()\n",
    "        adjustment_factor = desired_mean  - current_mean\n",
    "        df_submission['PRED'] += adjustment_factor\n",
    "    print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "    # Save the submission file\n",
    "    if  not testing and filename is not None:\n",
    "        df_submission.to_csv(filename, index=False)\n",
    "    print(f'Saved file: {filename}')\n",
    "    # if abs(df_submission['PRED'].mean() -0.0148) > 0.0005:\n",
    "    #    raise ValueError('WARNING: mean is bad')\n",
    "        \n",
    "    # Print warning if the row count is off\n",
    "    if not testing and len(df_submission) != 1117674:\n",
    "        print('WARNING: The submission file does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "        # raise ValueError('WARNING: The submission file does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "        \n",
    "    return df_submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_2y_1y(df, variables, target, model1=LogisticRegression(), model2=LogisticRegression()):\n",
    "    df = df.copy()\n",
    "    start_date = pd.Timestamp('2015-01-01')\n",
    "    end_date = pd.Timestamp('2017-01-01')\n",
    "\n",
    "    # Mask for rows with CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN between start_date and end_date\n",
    "    mask_date_range = (df['CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN'] >= start_date) & (df['CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN'] <= end_date)\n",
    "\n",
    "    df = df[mask_date_range]\n",
    "\n",
    "    probs = train_and_predict_two_halves(\n",
    "        df, \n",
    "        variables, \n",
    "        target, \n",
    "        model1=model1,\n",
    "        model2=model2,\n",
    "    )\n",
    "    df['2Y-1Y-PROBS'] = probs\n",
    "    \n",
    "    test_data = pd.read_csv('./data/1y-test.csv')\n",
    "\n",
    "\n",
    "    df_submission = create_submission_file(df, '2Y-1Y-PROBS', test_data, filename=None, testing=True)\n",
    "\n",
    "    merged_df = pd.merge(test_data, df_submission, on='BORROWER_ID')\n",
    "    true_labels = merged_df['TARGET_EVENT_BINARY_1Y']\n",
    "    predicted_probs = merged_df['PRED']\n",
    "    logloss = log_loss(true_labels, predicted_probs)\n",
    "\n",
    "    print(f'Log loss: {logloss}')\n",
    "        \n",
    "    return logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, non_significant_vars = significant_features(training_data, X_columns, y_column,LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_significant_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_columns_significant = [col for col in X_columns if col not in non_significant_vars]\n",
    "# X_columns_significant.append('CONTRACT_INSTALMENT_AMOUNT_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\takat\\Documents\\Egyetem\\Gépi tanulási esettanulmány labor\\Defaults\\medium-advanced.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/medium-advanced.ipynb#Y121sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X_columns\u001b[39m.\u001b[39;49mremove(\u001b[39m'\u001b[39;49m\u001b[39mCONTRACT_CREDIT_INTERMEDIARY\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/medium-advanced.ipynb#Y121sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_columns\u001b[39m.\u001b[39mremove(\u001b[39m'\u001b[39m\u001b[39mBORROWER_COUNTY\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "X_columns.remove('CONTRACT_CREDIT_INTERMEDIARY')\n",
    "X_columns.remove('BORROWER_COUNTY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centering probabilities...\n",
      "1.0 0.008741806475673486 0.014800000000000002\n",
      "Saved file: ./predictions/logistic-regression-one-model.csv\n"
     ]
    }
   ],
   "source": [
    "predicted_probs = 'LOGISTIC_REG'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    y_column, \n",
    "    model1=LogisticRegression(max_iter=400, C=0.5, random_state=42),\n",
    ")\n",
    "# 0.015985249571782673\n",
    "training_data[predicted_probs] = probs\n",
    "submission = create_submission_file(training_data, predicted_probs, data_submission_example, filename='./predictions/logistic-regression-one-model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probs = 'RANDOM_FOREST'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    y_column, \n",
    "    model1=RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42),\n",
    "    model2=RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42),\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "submission = create_submission_file(training_data, predicted_probs,data_submission_example, filename='./predictions/random-forrest-2y-exp-no-multicolinearity-lognormal-time-no-dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probs = 'GRADIENT_BOOSTING_CLASSIFIER'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    y_column, \n",
    "    model1=GradientBoostingClassifier(random_state=42),\n",
    "    model2=GradientBoostingClassifier(random_state=42),\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "submission = create_submission_file(training_data, predicted_probs,data_submission_example, filename='./predictions/gbc-2y-exp-no-multicolinearity-lognormal-time-no-dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Model 1 on df2:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    795431\n",
      "           1       0.77      0.67      0.72      5401\n",
      "\n",
      "    accuracy                           1.00    800832\n",
      "   macro avg       0.88      0.83      0.86    800832\n",
      "weighted avg       1.00      1.00      1.00    800832\n",
      "\n",
      "Log Loss:\n",
      "0.008920031130332217\n",
      "Confusion Matrix:\n",
      "[[794339   1092]\n",
      " [  1781   3620]]\n",
      "Evaluation of Model 2 on df1:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    795363\n",
      "           1       0.76      0.65      0.70      5468\n",
      "\n",
      "    accuracy                           1.00    800831\n",
      "   macro avg       0.88      0.83      0.85    800831\n",
      "weighted avg       1.00      1.00      1.00    800831\n",
      "\n",
      "Log Loss:\n",
      "0.00899427534801325\n",
      "Confusion Matrix:\n",
      "[[794243   1120]\n",
      " [  1894   3574]]\n",
      "Centering probabilities...\n",
      "0.9933470937188168 0.011253558355142607 0.01479999999999997\n",
      "Saved file: ./predictions/xgboost-max_depth=3-2y-exp-no-multicolinearity-lognormal-time-no-dates.csv\n"
     ]
    }
   ],
   "source": [
    "predicted_probs = 'XGBOOST'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    y_column, \n",
    "    model1= xgb.XGBClassifier(max_depth=3, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    model2= xgb.XGBClassifier(max_depth=3, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "submission = create_submission_file(training_data, predicted_probs, data_submission_example, filename='./predictions/xgboost-max_depth=3-2y-exp-no-multicolinearity-lognormal-time-no-dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Model 1 on df2:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    509478\n",
      "           1       0.60      0.39      0.47      5071\n",
      "\n",
      "    accuracy                           0.99    514549\n",
      "   macro avg       0.79      0.69      0.73    514549\n",
      "weighted avg       0.99      0.99      0.99    514549\n",
      "\n",
      "Log Loss:\n",
      "0.020154686217490652\n",
      "Confusion Matrix:\n",
      "[[508136   1342]\n",
      " [  3093   1978]]\n",
      "Evaluation of Model 2 on df1:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    509480\n",
      "           1       0.60      0.40      0.48      5069\n",
      "\n",
      "    accuracy                           0.99    514549\n",
      "   macro avg       0.80      0.70      0.74    514549\n",
      "weighted avg       0.99      0.99      0.99    514549\n",
      "\n",
      "Log Loss:\n",
      "0.02041090972095113\n",
      "Confusion Matrix:\n",
      "[[508121   1359]\n",
      " [  3032   2037]]\n",
      "Centering probabilities...\n",
      "1.0 0.0027108450789681583 0.014800000000000004\n",
      "Saved file: None\n",
      "Log loss: 0.02449711803718221\n",
      "LogisticRegression(max_iter=400, penalty=None, random_state=42)\n",
      "Logloss is  0.02449711803718221\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Model 1 on df2:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    509478\n",
      "           1       0.60      0.39      0.47      5071\n",
      "\n",
      "    accuracy                           0.99    514549\n",
      "   macro avg       0.79      0.69      0.73    514549\n",
      "weighted avg       0.99      0.99      0.99    514549\n",
      "\n",
      "Log Loss:\n",
      "0.020154686217490652\n",
      "Confusion Matrix:\n",
      "[[508136   1342]\n",
      " [  3093   1978]]\n",
      "Evaluation of Model 2 on df1:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    509480\n",
      "           1       0.60      0.40      0.48      5069\n",
      "\n",
      "    accuracy                           0.99    514549\n",
      "   macro avg       0.80      0.70      0.74    514549\n",
      "weighted avg       0.99      0.99      0.99    514549\n",
      "\n",
      "Log Loss:\n",
      "0.02041090972095113\n",
      "Confusion Matrix:\n",
      "[[508121   1359]\n",
      " [  3032   2037]]\n",
      "Centering probabilities...\n",
      "1.0 0.0027108450789681583 0.014800000000000004\n",
      "Saved file: None\n",
      "Log loss: 0.02449711803718221\n",
      "LogisticRegression(C=0.5, max_iter=400, penalty=None, random_state=42)\n",
      "Logloss is  0.02449711803718221\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "models =  [\n",
    "    (LogisticRegression(max_iter=400, penalty=None, C=1.0, random_state=42),\n",
    "     LogisticRegression(max_iter=400, penalty=None, C=1.0, random_state=42)),\n",
    "    \n",
    "   (LogisticRegression(max_iter=400, penalty=None, C=0.5, random_state=42),\n",
    "     LogisticRegression(max_iter=400, penalty=None, C=0.5, random_state=42)),\n",
    "]\n",
    "\n",
    "best_logloss = 100\n",
    "best_models = None\n",
    "for model in models:\n",
    "\n",
    "    logloss = test_model_2y_1y(\n",
    "        training_data, \n",
    "        X_columns, \n",
    "        'TARGET_EVENT_BINARY_1Y',\n",
    "        model1=model[0],\n",
    "        model2=model[1],\n",
    "    )\n",
    "    if logloss < best_logloss:\n",
    "        best_logloss = logloss\n",
    "        best_models = model\n",
    "    print(model[0])\n",
    "    print('Logloss is ', logloss)\n",
    "    print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_by_size = {}\n",
    "\n",
    "# Initialize a dictionary to hold covariance matrices by group size\n",
    "cov_matrices_by_size = {}\n",
    "\n",
    "# Group by 'BORROWER_ID' and iterate through the first 100 groups\n",
    "for idx, (name, group) in enumerate(training_data.groupby('BORROWER_ID')):\n",
    "    if idx % 100 == 0: \n",
    "        print('At index', idx)\n",
    "        \n",
    "    if idx == 50000:\n",
    "        break\n",
    "    \n",
    "    # Sort the group by CONTRACT_DATE_OF_LOAN_AGREEMENT\n",
    "    group = group.sort_values(by='CONTRACT_DATE_OF_LOAN_AGREEMENT')\n",
    "    \n",
    "    group_size = len(group)\n",
    "    if group_size not in groups_by_size:\n",
    "        groups_by_size[group_size] = []\n",
    "        \n",
    "    groups_by_size[group_size].append(group)\n",
    "\n",
    "# Initialize a new dictionary to hold the merged DataFrames by size\n",
    "merged_groups_by_size = {}\n",
    "\n",
    "for size, dfs in groups_by_size.items():\n",
    "    # Merge all DataFrames of the same size into a single DataFrame\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    # Store the merged DataFrame in the new dictionary\n",
    "    merged_groups_by_size[size] = merged_df\n",
    "\n",
    "# Now, merged_groups_by_size contains the merged DataFrames categorized by group size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\takat\\AppData\\Local\\Temp\\ipykernel_13332\\2391036120.py:12: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  cov_matrix = np.cov(merged_df['TARGET_EVENT_BINARY'].values.reshape(size,-1))\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2748: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2748: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Your code to generate groups_by_size and merged_groups_by_size\n",
    "\n",
    "cov_matrices_by_size = {}\n",
    "\n",
    "for size, merged_df in merged_groups_by_size.items():\n",
    "    if size > 1:  # Covariance matrix for single-element arrays doesn't make sense\n",
    "        try:\n",
    "            cov_matrix = np.cov(merged_df['TARGET_EVENT_BINARY'].values.reshape(size,-1))\n",
    "            if not np.isnan(cov_matrix).any():  # Check for NaN values\n",
    "                cov_matrices_by_size[size] = cov_matrix.tolist()  # Convert numpy array to list for JSON serialization\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while calculating the covariance matrix for size {size}: {e}\")\n",
    "\n",
    "# Ensure that the dictionary contains only JSON-serializable items\n",
    "serializable_cov_matrices_by_size = {k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in cov_matrices_by_size.items()}\n",
    "\n",
    "# Save to JSON\n",
    "with open('./data/cov_matrices_by_size.json', 'w') as f:\n",
    "    json.dump(serializable_cov_matrices_by_size, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: [[0.005868589752778426, -3.2357827075016176e-05],\n",
       "  [-3.2357827075016176e-05, 0.005451716043799604]],\n",
       " 3: [[0.007337144521720125, -2.6009019928111114e-05, -2.6009019928111046e-05],\n",
       "  [-2.6009019928111114e-05, 0.003507502116019551, -1.2385247584814735e-05],\n",
       "  [-2.6009019928111046e-05, -1.2385247584814735e-05, 0.003507502116019557]],\n",
       " 4: [[0.0023364421983921592,\n",
       "   -1.3695440787761734e-05,\n",
       "   -5.478176315104693e-06,\n",
       "   -8.217264472657053e-06],\n",
       "  [-1.3695440787761734e-05,\n",
       "   0.005820562334798763,\n",
       "   -1.3695440787761748e-05,\n",
       "   -2.054316118164268e-05],\n",
       "  [-5.478176315104693e-06,\n",
       "   -1.3695440787761748e-05,\n",
       "   0.0023364421983921558,\n",
       "   -8.217264472657026e-06],\n",
       "  [-8.217264472657053e-06,\n",
       "   -2.054316118164268e-05,\n",
       "   -8.217264472657026e-06,\n",
       "   0.003500554665351909]],\n",
       " 8: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       " 5: [[0.013199554069119256,\n",
       "   -0.00022296544035674517,\n",
       "   -0.00013377926421404707,\n",
       "   -4.4593088071349e-05,\n",
       "   -8.918617614269807e-05],\n",
       "  [-0.00022296544035674517,\n",
       "   0.016443701226309938,\n",
       "   -0.0001672240802675587,\n",
       "   -5.574136008918631e-05,\n",
       "   -0.00011148272017837272],\n",
       "  [-0.00013377926421404707,\n",
       "   -0.0001672240802675587,\n",
       "   0.009933110367893017,\n",
       "   -3.344481605351181e-05,\n",
       "   -6.688963210702361e-05],\n",
       "  [-4.4593088071349e-05,\n",
       "   -5.574136008918631e-05,\n",
       "   -3.344481605351181e-05,\n",
       "   0.003333333333333323,\n",
       "   -2.2296544035674506e-05],\n",
       "  [-8.918617614269807e-05,\n",
       "   -0.00011148272017837272,\n",
       "   -6.688963210702361e-05,\n",
       "   -2.2296544035674506e-05,\n",
       "   0.006644370122630978]],\n",
       " 6: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.020197769829581318, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       " 7: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.028571428571428577, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       " 9: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.11111111111111109, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_matrices_by_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "import json\n",
    "\n",
    "with open('cov_matrices_by_size.json', 'w') as f:\n",
    "    json.dump(cov_matrices_by_size, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
