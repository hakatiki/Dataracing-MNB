{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni_D83AE5_uni (Baseline)\n",
    "#   {\"Logloss\": 0.024950720047991065, \"ROCAUC\": 0.9808700266292157}\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy.stats import gamma, kstest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    log_loss, \n",
    "    roc_auc_score, \n",
    "    roc_curve, \n",
    "    auc\n",
    ")\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')  # Use this line to suppress warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "training_data = pd.read_csv('training_data.csv')\n",
    "training_data = training_data[training_data['BORROWER_ID'] != 'xNullx']\n",
    "training_data = training_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data_submission_example = pd.read_csv('data_submission_example.csv')\n",
    "\n",
    "lognormal_variables = [\n",
    "    'CONTRACT_CREDIT_LOSS', 'CONTRACT_DEPT_SERVICE_TO_INCOME',\n",
    "    'CONTRACT_INCOME', 'CONTRACT_INSTALMENT_AMOUNT', 'CONTRACT_INSTALMENT_AMOUNT_2',\n",
    "    'CONTRACT_LOAN_AMOUNT', 'CONTRACT_MARKET_VALUE', 'CONTRACT_MORTGAGE_LENDING_VALUE', \n",
    "    'CONTRACT_LGD', 'CONTRACT_INCOME'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ADDITION 1\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "\n",
    "# numeric_columns = training_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# excluded_keywords = ['TARGET', 'event', 'binary', 'DATE', 'DAYS', 'YEARS', 'MATURITY', 'DAY', 'BORROWER']\n",
    "\n",
    "# X_columns = [col for col in numeric_columns if all(keyword.lower() not in col.lower() for keyword in excluded_keywords)]\n",
    "# X_columns.remove('CONTRACT_TYPE_OF_INTEREST_REPAYMENT')\n",
    "# X_columns.remove('CONTRACT_MORTGAGE_TYPE')\n",
    "# df_copy = training_data.copy()\n",
    "# print(df_copy.isna().sum().sum())\n",
    "# min_values = []\n",
    "# for i in X_columns:\n",
    "#     if i in lognormal_variables:\n",
    "#         min_values.append(1)\n",
    "#     else:\n",
    "#         min_values.append(-np.inf)\n",
    "# imputer = IterativeImputer(max_iter=30, random_state=42, min_value=min_values,)\n",
    "# imputed_values = imputer.fit_transform(df_copy[X_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ADDITION 2\n",
    "# imputed_df = pd.DataFrame(imputed_values, columns=X_columns)\n",
    "\n",
    "# for col in X_columns:\n",
    "#     if imputed_df[col].dtype == 'float64':\n",
    "#         imputed_df[col] = imputed_df[col].astype('float32')\n",
    "#     elif imputed_df[col].dtype == 'int64':\n",
    "#         imputed_df[col] = imputed_df[col].astype('int32')\n",
    "\n",
    "# training_data.update(imputed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.fillna(0, inplace=True)\n",
    "for col in training_data.columns:\n",
    "    try:\n",
    "        training_data[col] = pd.to_numeric(training_data[col], errors='ignore')\n",
    "    except:\n",
    "        continue\n",
    "training_data['TARGET_EVENT_BINARY'] = np.where(training_data['TARGET_EVENT'] == 'K', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CONTRACT_ID                            0\n",
       "BORROWER_ID                            0\n",
       "CONTRACT_BANK_ID                       0\n",
       "CONTRACT_CREDIT_INTERMEDIARY           0\n",
       "CONTRACT_CREDIT_LOSS                   0\n",
       "CONTRACT_CURRENCY                      0\n",
       "CONTRACT_DATE_OF_LOAN_AGREEMENT        0\n",
       "CONTRACT_DEPT_SERVICE_TO_INCOME        0\n",
       "CONTRACT_FREQUENCY_TYPE                0\n",
       "CONTRACT_INCOME                        0\n",
       "CONTRACT_INSTALMENT_AMOUNT             0\n",
       "CONTRACT_INSTALMENT_AMOUNT_2           0\n",
       "CONTRACT_INTEREST_PERIOD               0\n",
       "CONTRACT_INTEREST_RATE                 0\n",
       "CONTRACT_LGD                           0\n",
       "CONTRACT_LOAN_AMOUNT                   0\n",
       "CONTRACT_LOAN_CONTRACT_TYPE            0\n",
       "CONTRACT_LOAN_TO_VALUE_RATIO           0\n",
       "CONTRACT_LOAN_TYPE                     0\n",
       "CONTRACT_MARKET_VALUE                  0\n",
       "CONTRACT_MATURITY_DATE                 0\n",
       "CONTRACT_MORTGAGE_LENDING_VALUE        0\n",
       "CONTRACT_MORTGAGE_TYPE                 0\n",
       "CONTRACT_REFINANCED                    0\n",
       "CONTRACT_RISK_WEIGHTED_ASSETS          0\n",
       "CONTRACT_TYPE_OF_INTEREST_REPAYMENT    0\n",
       "BORROWER_BIRTH_YEAR                    0\n",
       "BORROWER_CITIZENSHIP                   0\n",
       "BORROWER_COUNTRY                       0\n",
       "BORROWER_COUNTY                        0\n",
       "BORROWER_TYPE_OF_CUSTOMER              0\n",
       "BORROWER_TYPE_OF_SETTLEMENT            0\n",
       "TARGET_EVENT                           0\n",
       "TARGET_EVENT_DAY                       0\n",
       "TARGET_EVENT_BINARY                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['TARGET_EVENT_E'] = np.where(training_data['TARGET_EVENT'] == 'E', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TARGET_EVENT_BINARY\n",
       "0    1590792\n",
       "1      10871\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['TARGET_EVENT_BINARY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date_variables = ['CONTRACT_DATE_OF_LOAN_AGREEMENT', 'CONTRACT_MATURITY_DATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['TARGET_EVENT_DAY'].replace(0.0, np.nan, inplace=True)\n",
    "training_data['TARGET_EVENT_DAY_JULIAN'] = pd.to_datetime(training_data['TARGET_EVENT_DAY'], origin='julian', unit='D', errors='coerce')\n",
    "training_data['TARGET_EVENT_DAY_DATETIME'] = pd.to_datetime(training_data['TARGET_EVENT_DAY_JULIAN'],  errors='coerce')\n",
    "\n",
    "training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN'] = pd.to_datetime(training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT'], origin='julian', unit='D')\n",
    "training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] = pd.to_datetime(training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN'],)\n",
    "\n",
    "training_data['CONTRACT_MATURITY_DATE_JULIAN'] = pd.to_datetime(training_data['CONTRACT_MATURITY_DATE'], origin='julian', unit='D')\n",
    "training_data['CONTRACT_MATURITY_DATE_DATETIME'] = pd.to_datetime(training_data['CONTRACT_MATURITY_DATE_JULIAN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['DAY_DIFF'] = (training_data['TARGET_EVENT_DAY_DATETIME'] - training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME']).dt.days\n",
    "training_data['DAYS_TO_END'] = (pd.Timestamp(\"2020-01-01\")- training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME']).dt.days\n",
    "training_data['YEARS_TO_END'] = training_data['DAYS_TO_END'] / 365\n",
    "training_data['DAYS_TO_2018'] = (pd.Timestamp(\"2018-01-01\")- training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME']).dt.days\n",
    "training_data['YEARS_TO_2018'] = training_data['DAYS_TO_2018'] / 365\n",
    "training_data['TIME_TO_MATURITY_DAYS'] = (training_data['CONTRACT_MATURITY_DATE']-training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT'])\n",
    "training_data['2020_OR_MATURITY'] = np.minimum(training_data['TIME_TO_MATURITY_DAYS'], training_data['DAYS_TO_END'])\n",
    "training_data['2020_OR_MATURITY_YEARS'] = training_data['2020_OR_MATURITY'] / 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_target_column(dataframe, column_name, event, day_diff_upper_limit):\n",
    "    dataframe[column_name] = np.where(\n",
    "        (dataframe['TARGET_EVENT'] == event) & \n",
    "        (dataframe['DAY_DIFF'] <= day_diff_upper_limit) & \n",
    "        (dataframe['DAY_DIFF'] >= 0), \n",
    "        1, \n",
    "        0\n",
    "    )\n",
    "\n",
    "timeframes = {\n",
    "    'TARGET_EVENT_BINARY_2Y': 730,\n",
    "    'TARGET_EVENT_BINARY_1Y': 365,\n",
    "    'TARGET_EVENT_BINARY_6M': 365//2,\n",
    "}\n",
    "\n",
    "for column_name, days in timeframes.items():\n",
    "    create_binary_target_column(training_data, column_name, 'K', days)\n",
    "\n",
    "\n",
    "# target_event_binary_columns = []\n",
    "# for i in range(6):\n",
    "#     start_day = 273 + i * 30\n",
    "#     column_header = 'TARGET_EVENT_BINARY_' + str(start_day) + 'D'\n",
    "#     training_data[column_header] = np.where(\n",
    "#         (training_data['TARGET_EVENT'] == 'K') & \n",
    "#         (training_data['DAY_DIFF'] <= start_day) & \n",
    "#         (training_data['DAY_DIFF'] >= 0), \n",
    "#         1, \n",
    "#         0\n",
    "#     )\n",
    "#     target_event_binary_columns.append(column_header)\n",
    "\n",
    "training_data.drop('DAY_DIFF', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET_EVENT_BINARY\n",
      "0    1590792\n",
      "1      10871\n",
      "Name: count, dtype: int64\n",
      "TARGET_EVENT_BINARY_2Y\n",
      "0    1591751\n",
      "1       9912\n",
      "Name: count, dtype: int64\n",
      "TARGET_EVENT_BINARY_1Y\n",
      "0    1596927\n",
      "1       4736\n",
      "Name: count, dtype: int64\n",
      "TARGET_EVENT_BINARY_6M\n",
      "0    1601267\n",
      "1        396\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(training_data['TARGET_EVENT_BINARY'].value_counts())\n",
    "print(training_data['TARGET_EVENT_BINARY_2Y'].value_counts())\n",
    "print(training_data['TARGET_EVENT_BINARY_1Y'].value_counts())\n",
    "print(training_data['TARGET_EVENT_BINARY_6M'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CONTRACT_CREDIT_INTERMEDIARY',\n",
       " 'CONTRACT_CREDIT_LOSS',\n",
       " 'CONTRACT_CURRENCY',\n",
       " 'CONTRACT_DEPT_SERVICE_TO_INCOME',\n",
       " 'CONTRACT_INCOME',\n",
       " 'CONTRACT_INSTALMENT_AMOUNT',\n",
       " 'CONTRACT_INSTALMENT_AMOUNT_2',\n",
       " 'CONTRACT_INTEREST_PERIOD',\n",
       " 'CONTRACT_INTEREST_RATE',\n",
       " 'CONTRACT_LGD',\n",
       " 'CONTRACT_LOAN_AMOUNT',\n",
       " 'CONTRACT_LOAN_CONTRACT_TYPE',\n",
       " 'CONTRACT_LOAN_TO_VALUE_RATIO',\n",
       " 'CONTRACT_MARKET_VALUE',\n",
       " 'CONTRACT_MORTGAGE_LENDING_VALUE',\n",
       " 'CONTRACT_MORTGAGE_TYPE',\n",
       " 'CONTRACT_REFINANCED',\n",
       " 'CONTRACT_RISK_WEIGHTED_ASSETS',\n",
       " 'CONTRACT_TYPE_OF_INTEREST_REPAYMENT',\n",
       " 'BORROWER_BIRTH_YEAR',\n",
       " 'BORROWER_CITIZENSHIP',\n",
       " 'BORROWER_COUNTRY',\n",
       " 'BORROWER_COUNTY',\n",
       " 'BORROWER_TYPE_OF_SETTLEMENT']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_columns = training_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "excluded_keywords = ['TARGET', 'event', 'binary', 'DATE', 'DAYS', 'YEARS', 'MATURITY', 'DAY']\n",
    "\n",
    "X_columns = [col for col in numeric_columns if all(keyword.lower() not in col.lower() for keyword in excluded_keywords)]\n",
    "y_column = 'TARGET_EVENT_BINARY_2Y' \n",
    "X_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy_financial as npf\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_monthly_payment(row):\n",
    "    annual_interest_rate = row['CONTRACT_INTEREST_RATE'] / 100  # Convert percentage to decimal\n",
    "    monthly_interest_rate = annual_interest_rate / 12\n",
    "    term_in_months = ((pd.to_datetime(row['CONTRACT_MATURITY_DATE_DATETIME'], format='%d/%m/%Y') - \n",
    "                       pd.to_datetime(row['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'], format='%d/%m/%Y'))).days/30\n",
    "    principal = row['CONTRACT_LOAN_AMOUNT']\n",
    "    monthly_payment = npf.pmt(monthly_interest_rate, term_in_months, -principal)\n",
    "    return monthly_payment\n",
    "# ADDITION 3:\n",
    "# training_data['MONTHLY_PAYMENT'] = training_data.apply(calculate_monthly_payment, axis=1)\n",
    "# training_data['MONTHLY_PAYMENT'] = training_data['MONTHLY_PAYMENT'].apply(lambda x: 0 if not np.isfinite(x) else x)\n",
    "# X_columns.extend(['MONTHLY_PAYMENT'])\n",
    "# lognormal_variables.extend(['MONTHLY_PAYMENT'])\n",
    "# X_columns.append('TIME_TO_MATURITY_DAYS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_type_dummies = pd.get_dummies(training_data['CONTRACT_LOAN_TYPE'], prefix='LOAN_TYPE', drop_first=True)\n",
    "frequency_type_dummies = pd.get_dummies(training_data['CONTRACT_FREQUENCY_TYPE'], prefix='FREQ_TYPE', drop_first=True)\n",
    "interest_type_dummies = pd.get_dummies(training_data['CONTRACT_TYPE_OF_INTEREST_REPAYMENT'], prefix='INTEREST_TYPE', drop_first=True)\n",
    "mortgage_type_dummies =  pd.get_dummies(training_data['CONTRACT_MORTGAGE_TYPE'], prefix='MORTGAGE_TYPE', drop_first=True)\n",
    "\n",
    "training_data = pd.concat([training_data, loan_type_dummies, frequency_type_dummies,interest_type_dummies,mortgage_type_dummies ], axis=1)\n",
    "\n",
    "X_columns.extend(loan_type_dummies.columns)\n",
    "X_columns.extend(frequency_type_dummies.columns)\n",
    "X_columns.extend(interest_type_dummies.columns)\n",
    "X_columns.extend(mortgage_type_dummies.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['BORROWER_LOAN_COUNT'] = training_data.groupby('BORROWER_ID')['BORROWER_ID'].transform('count')\n",
    "training_data['LOAN_BORROWER_COUNT'] = training_data.groupby('CONTRACT_ID')['CONTRACT_ID'].transform('count')\n",
    "training_data['TOTAL_LOAN_AMOUNT'] = training_data.groupby('BORROWER_ID')['CONTRACT_LOAN_AMOUNT'].transform('sum')\n",
    "training_data['TOTAL_INSTALLMENT_AMOUNT_1'] = training_data.groupby('BORROWER_ID')['CONTRACT_INSTALMENT_AMOUNT'].transform('sum')\n",
    "training_data['TOTAL_INSTALLMENT_AMOUNT_2'] = training_data.groupby('BORROWER_ID')['CONTRACT_INSTALMENT_AMOUNT_2'].transform('sum')\n",
    "training_data['TOTAL_INSTALLMENT_AMOUNT'] = training_data['TOTAL_INSTALLMENT_AMOUNT_1'] + training_data['TOTAL_INSTALLMENT_AMOUNT_2']\n",
    "\n",
    "X_columns.extend(['BORROWER_LOAN_COUNT', 'TOTAL_LOAN_AMOUNT','TOTAL_INSTALLMENT_AMOUNT','LOAN_BORROWER_COUNT'])\n",
    "lognormal_variables.extend([ 'TOTAL_LOAN_AMOUNT','TOTAL_INSTALLMENT_AMOUNT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables removed: {'BORROWER_COUNTRY', 'FREQ_TYPE_2f88e16c', 'CONTRACT_REFINANCED'}\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.95\n",
    "correlation_matrix = training_data[X_columns].corr()\n",
    "highly_correlated_set = set()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            highly_correlated_set.add(colname)\n",
    "\n",
    "X_columns = [col for col in X_columns if col not in highly_correlated_set]\n",
    "print('Variables removed:', highly_correlated_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_percentage_generator_2016(df, percentage, target):\n",
    "    df_copy = df.copy()\n",
    "    df_filtered = df_copy[df_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] < '2016-01-01']\n",
    "    df_mean = df_filtered[target].mean()\n",
    "    print(f\"Mean in year {df_mean}\")\n",
    "    df_defautled = df_filtered[target].sum()\n",
    "    df_not_defaulted = len(df_filtered) - df_defautled\n",
    "\n",
    "    required_val = (df_defautled - percentage * len(df_filtered))/(percentage - 1)\n",
    "    \n",
    "    df_filtered_after = df_copy[df_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] > '2016-01-01']\n",
    "    df_filtered_after = df_filtered_after[df_filtered_after[target]==1] \n",
    "    print(len(df_filtered_after))\n",
    "    print(required_val)\n",
    "    required_val = min(int(required_val) ,len(df_filtered_after))\n",
    "    df_filtered_after = df_filtered_after.sample(n=int(required_val),random_state=42).reset_index(drop=True)\n",
    "\n",
    "    df_filtered = pd.concat([df_filtered, df_filtered_after])\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def default_percentage_generator_2016_maximal(df, percentage, target):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    defaulted = df_copy[(df_copy[target]==1) & (df_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] < '2018-01-01')]\n",
    "    total_needed = len(defaulted) / percentage * 100 - len(defaulted)\n",
    "\n",
    "\n",
    "    # print(len(defaulted))\n",
    "    # print(total_needed)\n",
    "\n",
    "    df_filtered_2016 = df_copy[(df_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] < '2016-01-01')& (df_copy[target]==0)]\n",
    "    # print(len(df_filtered_2016))\n",
    "    # print(total_needed)\n",
    "    max_needed = min(total_needed,len(df_filtered_2016))\n",
    "    df_filtered_2016 = df_filtered_2016.sample(n=int(max_needed),random_state=42).reset_index(drop=True)\n",
    "\n",
    "    extra_needed = total_needed - len(df_filtered_2016)\n",
    "\n",
    "    df_filtered_after = df_copy[df_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] > '2016-01-01']\n",
    "    df_filtered_after = df_filtered_after[df_filtered_after[target]==0]\n",
    "    df_filtered_after = df_filtered_after.sample(n=int(extra_needed),random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    df_filtered = pd.concat([df_filtered_2016, df_filtered_after,defaulted])\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# default_percentage_generator_2016_maximal(training_data, 1.48, 'TARGET_EVENT_BINARY')['TARGET_EVENT_BINARY'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities(data, column, time_factor):\n",
    "    lambdas = -np.log(1 - data[column]) / time_factor\n",
    "    probs_2y = 1 - np.exp(-2 * lambdas)\n",
    "    return probs_2y\n",
    "def calculate_probabilities_vec(data, time_factor):\n",
    "    lambdas = -np.log(1 - data) / time_factor\n",
    "    probs_2y = 1 - np.exp(-2 * lambdas)\n",
    "    return probs_2y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "def train_and_predict_two_halves(df, variables, target, model=LogisticRegression(), scaler=StandardScaler(), augment_distribution=False,calibrate=False,augment_distribution_percentage = 1.48, unique_loans=False,should_smote =False,maximal_sample=False):\n",
    "    if lognormal_variables is not None:\n",
    "        df = df.copy()\n",
    "        \n",
    "        for var in lognormal_variables:\n",
    "            if var == 'CONTRACT_CREDIT_LOSS':\n",
    "                df[var] = np.log1p(np.abs(df[var]))*np.sign(df[var])\n",
    "            else:\n",
    "                df[var] = np.log1p(df[var])\n",
    "    if augment_distribution and not maximal_sample:\n",
    "        df_filtered = default_percentage_generator_2016(df, augment_distribution_percentage/100, target)\n",
    "    else:\n",
    "        df_filtered = df[df['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] < '2016-01-01']\n",
    "    \n",
    "    if maximal_sample:\n",
    "        df_filtered = default_percentage_generator_2016_maximal(df, augment_distribution_percentage, target)\n",
    "        print(df_filtered[target].mean())\n",
    "\n",
    "    if unique_loans:\n",
    "        df_filtered = df_filtered.drop_duplicates(subset=['CONTRACT_ID'])\n",
    "\n",
    "    \n",
    "    X_scaled = scaler.fit_transform(df[variables])\n",
    "    X_filtered = scaler.transform(df_filtered[variables])\n",
    "\n",
    "    y = df[target] \n",
    "    y_filtered = df_filtered[target] \n",
    "    if should_smote:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_filtered, y_filtered = smote.fit_resample(X_filtered, y_filtered)\n",
    "\n",
    "    if calibrate:\n",
    "        model = CalibratedClassifierCV(base_estimator=model, method='isotonic', )\n",
    "\n",
    "    model.fit(X_filtered, y_filtered)\n",
    "    \n",
    "\n",
    "    print(\"Logloss:\")\n",
    "    test_proba = model.predict_proba(X_filtered)\n",
    "    print(log_loss(y_filtered,test_proba))\n",
    "    proba = model.predict_proba(X_scaled)[:, 1]\n",
    "    # Additional code for ROC AUC score\n",
    "    roc_auc = roc_auc_score(y_filtered,test_proba[:,1])\n",
    "    print(\"ROC AUC Score:\", roc_auc)\n",
    "\n",
    "    return proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significant_features(df, variables, target, model1=LogisticRegression(), scaler=StandardScaler()):\n",
    "    if lognormal_variables is not None:\n",
    "        df = df.copy()\n",
    "        print(df[variables].isna().sum().sum())\n",
    "        \n",
    "        for var in lognormal_variables:\n",
    "            if var == 'CONTRACT_CREDIT_LOSS':\n",
    "                df[var] = np.log1p(np.abs(df[var]))*np.sign(df[var])\n",
    "            else:\n",
    "                df[var] = np.log1p(df[var])\n",
    "    print(np.isinf(df[variables]).sum().sum())\n",
    "    X_scaled = scaler.fit_transform(df[variables])\n",
    "    print(np.isinf(X_scaled).sum().sum())\n",
    "    print(pd.DataFrame(X_scaled, columns=variables).isna().sum().sum())\n",
    "    X = sm.add_constant(pd.DataFrame(X_scaled, columns=variables))\n",
    "    y = df[target]\n",
    "    model = sm.Logit(y, X).fit()\n",
    "    print(model.summary())\n",
    "    p_values = model.pvalues\n",
    "    non_significant_vars = p_values[p_values > 0.05].index.tolist()\n",
    "\n",
    "    return model, non_significant_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_probability(s):\n",
    "    if len(s) == 2:\n",
    "        p_a, p_b = s.values\n",
    "        return p_a + p_b - p_a * p_b #- (-3.2357827075016176e-05)\n",
    "    else:\n",
    "        return 1 - np.prod(1 - s.values)\n",
    "\n",
    "def create_submission_file(df_preds, target, example, filename='submission.csv', testing=False):\n",
    "    # Filter the data to only include BORROWER_IDs that are in the submission example\n",
    "    df_preds.loc[df_preds['TARGET_EVENT'] == 'E', target] = 0\n",
    "\n",
    "    print(\"Logloss:\")\n",
    "    print(log_loss(df_preds['TARGET_EVENT_BINARY'], df_preds[target]))\n",
    "\n",
    "    filtered_training_data = df_preds[df_preds['BORROWER_ID'].isin(example['BORROWER_ID'])]\n",
    "\n",
    "    # Print warning if the row count is off\n",
    "    if not testing and len(filtered_training_data) != 1564601:\n",
    "        print('WARNING: The filtered data does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "        # raise ValueError('WARNING: The submission file does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "    # Group by BORROWER_ID and calculate the combined probability\n",
    "\n",
    "    #######################x########################\n",
    "    #CUTTING TAILS DID NOT SEEM TO WORK\n",
    "    #######################x########################\n",
    "    # filtered_training_data = cut_exponential_tails(filtered_training_data, target)\n",
    "    grouped_data = filtered_training_data.groupby('BORROWER_ID')[target].apply(combined_probability).reset_index()\n",
    "\n",
    "    # Create the submission DataFrame\n",
    "    df_submission = pd.DataFrame()\n",
    "    df_submission['BORROWER_ID'] = grouped_data['BORROWER_ID']\n",
    "    df_submission['PRED'] = grouped_data[target]\n",
    "    print('Before centering:')\n",
    "    print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "\n",
    "    print('Centering probabilities...')\n",
    "    # Center the probabilities around 1.48%\n",
    "    desired_mean = 0.0148  # 1.48% as a decimal\n",
    "    # while (df_submission['PRED'].max() > 1 or df_submission['PRED'].min() < 0 or abs(df_submission['PRED'].mean() -0.0148) > 0.0005):\n",
    "    #     # print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "    #     df_submission['PRED'] = df_submission['PRED'].clip(lower=0, upper=1)\n",
    "    #     # print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "\n",
    "    #     current_mean = df_submission['PRED'].mean()\n",
    "    #     adjustment_factor = desired_mean  - current_mean\n",
    "    #     df_submission['PRED'] += adjustment_factor\n",
    "    initial_guess = 2\n",
    "    probas_unscaled = df_submission['PRED'].values\n",
    "    new_proba = probas_unscaled.copy()\n",
    "    while abs(new_proba.mean() - desired_mean) > 0.00001:\n",
    "        \n",
    "        new_proba = calculate_probabilities_vec(probas_unscaled, initial_guess)\n",
    "        error = new_proba.mean() - desired_mean\n",
    "        if error > 0:\n",
    "            initial_guess += 0.001\n",
    "        else:\n",
    "            initial_guess -= 0.001\n",
    "        print(error, initial_guess)\n",
    "    df_submission['PRED'] = new_proba\n",
    "        \n",
    "        \n",
    "    \n",
    "    print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "    # Save the submission file\n",
    "    if  not testing and filename is not None:\n",
    "        df_submission.to_csv(filename, index=False)\n",
    "    print(f'Saved file: {filename}')\n",
    "    # if abs(df_submission['PRED'].mean() -0.0148) > 0.0005:\n",
    "    #    raise ValueError('WARNING: mean is bad')\n",
    "        \n",
    "    # Print warning if the row count is off\n",
    "    if not testing and len(df_submission) != 1117674:\n",
    "        print('WARNING: The submission file does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "        # raise ValueError('WARNING: The submission file does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "        \n",
    "    return df_submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_2y_1y(df, variables, target, model1=LogisticRegression(), model2=LogisticRegression()):\n",
    "    df = df.copy()\n",
    "    start_date = pd.Timestamp('2015-01-01')\n",
    "    end_date = pd.Timestamp('2017-01-01')\n",
    "\n",
    "    # Mask for rows with CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN between start_date and end_date\n",
    "    mask_date_range = (df['CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN'] >= start_date) & (df['CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN'] <= end_date)\n",
    "\n",
    "    df = df[mask_date_range]\n",
    "\n",
    "    probs = train_and_predict_two_halves(\n",
    "        df, \n",
    "        variables, \n",
    "        target, \n",
    "        model1=model1,\n",
    "        model2=model2,\n",
    "    )\n",
    "    df['2Y-1Y-PROBS'] = probs\n",
    "    \n",
    "    test_data = pd.read_csv('./data/1y-test.csv')\n",
    "\n",
    "\n",
    "    df_submission = create_submission_file(df, '2Y-1Y-PROBS', test_data, filename=None, testing=True)\n",
    "\n",
    "    merged_df = pd.merge(test_data, df_submission, on='BORROWER_ID')\n",
    "    true_labels = merged_df['TARGET_EVENT_BINARY_1Y']\n",
    "    predicted_probs = merged_df['PRED']\n",
    "    logloss = log_loss(true_labels, predicted_probs)\n",
    "\n",
    "    print(f'Log loss: {logloss}')\n",
    "        \n",
    "    return logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_yearly_proba(data, proba, targets =[ 0.0051, 0.0098, 0.0185], logging=False):\n",
    "    data = data.copy()\n",
    "    probs = data[proba]\n",
    "    starter_scales=[2.8, 1.2, 0.75]\n",
    "    new_proba = np.zeros(len(data))\n",
    "    mask_2016 = (data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] < '2016-01-01') & (data['TARGET_EVENT_BINARY'] != 1)\n",
    "    mask_2017 = (data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] > '2016-01-01') & (data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] < '2017-01-01')& (data['TARGET_EVENT_BINARY'] != 1)\n",
    "    mask_2018 = (data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] > '2017-01-01') & (data['TARGET_EVENT_BINARY'] != 1)\n",
    "\n",
    "    proba_2016 = probs\n",
    "    proba_2017 = probs\n",
    "    proba_2018 = probs\n",
    "    if logging:\n",
    "        print(\"Before scaling:\")\n",
    "        print(proba_2016[mask_2016].mean(), proba_2017[mask_2017].mean(), proba_2018[mask_2018].mean())\n",
    "        print(probs.mean())\n",
    "    calib_2016 = True\n",
    "    calib_2017 = True\n",
    "    calib_2018 = True\n",
    "\n",
    "    while calib_2016 or calib_2017 or calib_2018:    \n",
    "        proba_2016 = probs\n",
    "        proba_2017 = probs\n",
    "        proba_2018 = probs\n",
    "        proba_2016 = calculate_probabilities_vec(proba_2016, starter_scales[0])\n",
    "        proba_2017 = calculate_probabilities_vec(proba_2017, starter_scales[1])\n",
    "        proba_2018 = calculate_probabilities_vec(proba_2018, starter_scales[2])\n",
    "        \n",
    "        new_proba[mask_2016] = proba_2016[mask_2016]\n",
    "        new_proba[mask_2017] = proba_2017[mask_2017]\n",
    "        new_proba[mask_2018] = proba_2018[mask_2018]\n",
    "        if logging:\n",
    "            print('Adter scaling:')\n",
    "            print(proba_2016[mask_2016].mean(), proba_2017[mask_2017].mean(), proba_2018[mask_2018].mean())\n",
    "            print(new_proba.mean())\n",
    "\n",
    "        mean_2016 = proba_2016[mask_2016].mean()\n",
    "        mean_2017 = proba_2017[mask_2017].mean()\n",
    "        mean_2018 = proba_2018[mask_2018].mean()\n",
    "\n",
    "        diff_2016 = mean_2016 - targets[0]\n",
    "        diff_2017 = mean_2017 - targets[1]\n",
    "        diff_2018 = mean_2018 - targets[2]\n",
    "        if diff_2016 > 0.0001:\n",
    "            starter_scales[0] += 0.01\n",
    "        elif diff_2016 < -0.0001:\n",
    "            starter_scales[0] -= 0.01\n",
    "        else:\n",
    "            calib_2016 = False\n",
    "        if diff_2017 > 0.0001:\n",
    "            starter_scales[1] += 0.01\n",
    "        elif diff_2017 < -0.0001:\n",
    "            starter_scales[1] -= 0.01\n",
    "        else:\n",
    "            calib_2017 = False\n",
    "        if diff_2018 > 0.0001:\n",
    "            starter_scales[2] += 0.01\n",
    "        elif diff_2018 < -0.0001:\n",
    "            starter_scales[2] -= 0.01\n",
    "        else:\n",
    "            calib_2018 = False\n",
    "    return new_proba\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: Maximum number of iterations has been exceeded.\n",
    "#          Current function value: 0.014855\n",
    "#          Iterations: 35\n",
    "#                              Logit Regression Results                             \n",
    "# ==================================================================================\n",
    "# Dep. Variable:     TARGET_EVENT_BINARY_2Y   No. Observations:              1601663\n",
    "# Model:                              Logit   Df Residuals:                  1601611\n",
    "# Method:                               MLE   Df Model:                           51\n",
    "# Date:                    Wed, 08 Nov 2023   Pseudo R-squ.:                  0.6053\n",
    "# Time:                            17:03:09   Log-Likelihood:                -23792.\n",
    "# converged:                          False   LL-Null:                       -60284.\n",
    "# Covariance Type:                nonrobust   LLR p-value:                     0.000\n",
    "\n",
    "#  No dummies and feature engineering:\n",
    "# Optimization terminated successfully.\n",
    "#          Current function value: 0.017321\n",
    "#          Iterations 13\n",
    "#                              Logit Regression Results                             \n",
    "# ==================================================================================\n",
    "# Dep. Variable:     TARGET_EVENT_BINARY_2Y   No. Observations:              1601663\n",
    "# Model:                              Logit   Df Residuals:                  1601638\n",
    "# Method:                               MLE   Df Model:                           24\n",
    "# Date:                    Wed, 08 Nov 2023   Pseudo R-squ.:                  0.5398\n",
    "# Time:                            20:38:24   Log-Likelihood:                -27743.\n",
    "# converged:                           True   LL-Null:                       -60284.\n",
    "# Covariance Type:                nonrobust   LLR p-value:                     0.000\n",
    "# Two more variables added: LOAN_TYPE, FREQ_TYPE, MORGTAGE_TYPE, INTEREST_TYPE\n",
    "# Warning: Maximum number of iterations has been exceeded.\n",
    "#          Current function value: 0.014613\n",
    "#          Iterations: 35\n",
    "#                              Logit Regression Results                             \n",
    "# ==================================================================================\n",
    "# Dep. Variable:     TARGET_EVENT_BINARY_2Y   No. Observations:              1601663\n",
    "# Model:                              Logit   Df Residuals:                  1601589\n",
    "# Method:                               MLE   Df Model:                           73\n",
    "# Date:                    Thu, 09 Nov 2023   Pseudo R-squ.:                  0.6118\n",
    "# Time:                            18:28:24   Log-Likelihood:                -23405.\n",
    "# converged:                          False   LL-Null:                       -60284.\n",
    "# Covariance Type:                nonrobust   LLR p-value:                     0.000\n",
    "# Adding monthly to LOAN_TYPE, FREQ_TYPE, MORGTAGE_TYPE, INTEREST_TYPE\n",
    "# Warning: Maximum number of iterations has been exceeded.\n",
    "#          Current function value: 0.014232\n",
    "#          Iterations: 35\n",
    "#                              Logit Regression Results                             \n",
    "# ==================================================================================\n",
    "# Dep. Variable:     TARGET_EVENT_BINARY_2Y   No. Observations:              1601663\n",
    "# Model:                              Logit   Df Residuals:                  1601584\n",
    "# Method:                               MLE   Df Model:                           78\n",
    "# Date:                    Thu, 09 Nov 2023   Pseudo R-squ.:                  0.6219\n",
    "# Time:                            18:35:37   Log-Likelihood:                -22795.\n",
    "# converged:                          False   LL-Null:                       -60284.\n",
    "# Covariance Type:                nonrobust   LLR p-value:                     0.000\n",
    "#  ADDING TIME TO MATURITY in days\n",
    "# Warning: Maximum number of iterations has been exceeded.\n",
    "#          Current function value: 0.014143\n",
    "#          Iterations: 35\n",
    "#                              Logit Regression Results                             \n",
    "# ==================================================================================\n",
    "# Dep. Variable:     TARGET_EVENT_BINARY_2Y   No. Observations:              1601663\n",
    "# Model:                              Logit   Df Residuals:                  1601586\n",
    "# Method:                               MLE   Df Model:                           76\n",
    "# Date:                    Mon, 13 Nov 2023   Pseudo R-squ.:                  0.6242\n",
    "# Time:                            17:00:42   Log-Likelihood:                -22653.\n",
    "# converged:                          False   LL-Null:                       -60284.\n",
    "# Covariance Type:                nonrobust   LLR p-value:                     0.000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy = X_columns.copy()\n",
    "X_copy.remove('CONTRACT_CREDIT_INTERMEDIARY')\n",
    "X_copy.remove('BORROWER_COUNTY')\n",
    "X_copy.remove('CONTRACT_TYPE_OF_INTEREST_REPAYMENT')\n",
    "X_copy.remove('CONTRACT_MORTGAGE_TYPE')\n",
    "X_copy.remove('BORROWER_TYPE_OF_SETTLEMENT')\n",
    "X_copy.remove('CONTRACT_LOAN_AMOUNT')\n",
    "X_copy.remove('CONTRACT_INSTALMENT_AMOUNT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[X_columns].isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.014609\n",
      "         Iterations: 35\n",
      "                             Logit Regression Results                             \n",
      "==================================================================================\n",
      "Dep. Variable:     TARGET_EVENT_BINARY_2Y   No. Observations:              1601663\n",
      "Model:                              Logit   Df Residuals:                  1601588\n",
      "Method:                               MLE   Df Model:                           74\n",
      "Date:                    Wed, 22 Nov 2023   Pseudo R-squ.:                  0.6119\n",
      "Time:                            11:04:31   Log-Likelihood:                -23398.\n",
      "converged:                          False   LL-Null:                       -60284.\n",
      "Covariance Type:                nonrobust   LLR p-value:                     0.000\n",
      "=======================================================================================================\n",
      "                                          coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "const                                 -10.3999   2.84e+05  -3.67e-05      1.000   -5.56e+05    5.56e+05\n",
      "CONTRACT_CREDIT_INTERMEDIARY            0.0744      0.005     16.116      0.000       0.065       0.083\n",
      "CONTRACT_CREDIT_LOSS                    4.4748      0.055     81.192      0.000       4.367       4.583\n",
      "CONTRACT_CURRENCY                      -0.0110      0.003     -3.994      0.000      -0.016      -0.006\n",
      "CONTRACT_DEPT_SERVICE_TO_INCOME        -0.0802      0.028     -2.839      0.005      -0.135      -0.025\n",
      "CONTRACT_INCOME                         0.2310      0.028      8.224      0.000       0.176       0.286\n",
      "CONTRACT_INSTALMENT_AMOUNT             -0.0002      0.013     -0.014      0.989      -0.026       0.026\n",
      "CONTRACT_INSTALMENT_AMOUNT_2           -0.1207      0.023     -5.238      0.000      -0.166      -0.076\n",
      "CONTRACT_INTEREST_PERIOD               -0.1379      0.031     -4.482      0.000      -0.198      -0.078\n",
      "CONTRACT_INTEREST_RATE                  0.2394      0.018     13.049      0.000       0.203       0.275\n",
      "CONTRACT_LGD                           -0.4554      0.035    -12.964      0.000      -0.524      -0.387\n",
      "CONTRACT_LOAN_AMOUNT                   -0.0043      0.044     -0.099      0.921      -0.090       0.081\n",
      "CONTRACT_LOAN_CONTRACT_TYPE             0.8318      0.062     13.346      0.000       0.710       0.954\n",
      "CONTRACT_LOAN_TO_VALUE_RATIO           -0.6246      0.041    -15.100      0.000      -0.706      -0.543\n",
      "CONTRACT_MARKET_VALUE                   0.2964      0.108      2.747      0.006       0.085       0.508\n",
      "CONTRACT_MORTGAGE_LENDING_VALUE        -1.0646      0.092    -11.605      0.000      -1.244      -0.885\n",
      "CONTRACT_MORTGAGE_TYPE                  0.1519   1.37e+06   1.11e-07      1.000   -2.68e+06    2.68e+06\n",
      "CONTRACT_RISK_WEIGHTED_ASSETS           0.0335      0.005      6.672      0.000       0.024       0.043\n",
      "CONTRACT_TYPE_OF_INTEREST_REPAYMENT    -0.6791   1.01e+05  -6.71e-06      1.000   -1.98e+05    1.98e+05\n",
      "BORROWER_BIRTH_YEAR                    -0.1645      0.039     -4.226      0.000      -0.241      -0.088\n",
      "BORROWER_CITIZENSHIP                   -0.0674      0.009     -7.419      0.000      -0.085      -0.050\n",
      "BORROWER_COUNTY                         0.0094      0.013      0.705      0.481      -0.017       0.036\n",
      "BORROWER_TYPE_OF_SETTLEMENT            -0.0167      0.016     -1.026      0.305      -0.048       0.015\n",
      "LOAN_TYPE_1f951336                      5.0375   1071.598      0.005      0.996   -2095.256    2105.331\n",
      "LOAN_TYPE_2f88e16c                      2.5477    669.246      0.004      0.997   -1309.151    1314.246\n",
      "LOAN_TYPE_47693941                     -0.0366    920.617  -3.97e-05      1.000   -1804.413    1804.340\n",
      "LOAN_TYPE_5a06241e                      2.2027    409.085      0.005      0.996    -799.588     803.994\n",
      "LOAN_TYPE_694cbaee                      0.9544    167.749      0.006      0.995    -327.827     329.736\n",
      "LOAN_TYPE_69f70539                      3.4644    725.165      0.005      0.996   -1417.833    1424.762\n",
      "LOAN_TYPE_7e2065f4                      4.0372    819.502      0.005      0.996   -1602.158    1610.232\n",
      "LOAN_TYPE_83910425                      1.2953    235.721      0.005      0.996    -460.709     463.300\n",
      "LOAN_TYPE_8fe006f1                     -0.0002   4447.424  -5.24e-08      1.000   -8716.790    8716.790\n",
      "LOAN_TYPE_955ae3ef                      3.2464    665.848      0.005      0.996   -1301.792    1308.285\n",
      "LOAN_TYPE_95c4f8fb                      0.3246     67.846      0.005      0.996    -132.652     133.301\n",
      "LOAN_TYPE_b503a0de                      8.2169   1735.759      0.005      0.996   -3393.809    3410.243\n",
      "LOAN_TYPE_cde77491                    -23.8009   3.48e+07  -6.83e-07      1.000   -6.83e+07    6.83e+07\n",
      "LOAN_TYPE_cf07c2dd                      2.6273    535.133      0.005      0.996   -1046.215    1051.469\n",
      "LOAN_TYPE_d3aaffde                      4.4177    831.656      0.005      0.996   -1625.598    1634.433\n",
      "LOAN_TYPE_eab72d7a                      0.6851    136.388      0.005      0.996    -266.631     268.002\n",
      "LOAN_TYPE_f792971b                      0.0680     45.700      0.001      0.999     -89.502      89.638\n",
      "FREQ_TYPE_3265c5b7                     -0.3720      0.153     -2.425      0.015      -0.673      -0.071\n",
      "FREQ_TYPE_479a2e13                     -0.9796      0.395     -2.478      0.013      -1.755      -0.205\n",
      "FREQ_TYPE_87db11f5                     -0.8482      0.251     -3.386      0.001      -1.339      -0.357\n",
      "FREQ_TYPE_89efd382                     -0.0236      0.023     -1.015      0.310      -0.069       0.022\n",
      "FREQ_TYPE_ad534644                     -0.0994      0.047     -2.101      0.036      -0.192      -0.007\n",
      "FREQ_TYPE_bd092d5a                      0.0003      0.004      0.079      0.937      -0.008       0.009\n",
      "INTEREST_TYPE_100001.0                  0.0613   3.36e+04   1.82e-06      1.000   -6.58e+04    6.58e+04\n",
      "INTEREST_TYPE_100002.0                 -0.1932   6.11e+04  -3.16e-06      1.000    -1.2e+05     1.2e+05\n",
      "INTEREST_TYPE_100003.0                 -0.3411   1.16e+05  -2.95e-06      1.000   -2.27e+05    2.27e+05\n",
      "INTEREST_TYPE_100004.0                 -0.2657   9.54e+04  -2.79e-06      1.000   -1.87e+05    1.87e+05\n",
      "INTEREST_TYPE_110001.0                  0.0677   3.74e+04   1.81e-06      1.000   -7.32e+04    7.32e+04\n",
      "INTEREST_TYPE_140001.0                  0.0233   2.95e+04    7.9e-07      1.000   -5.79e+04    5.79e+04\n",
      "INTEREST_TYPE_140002.0                  0.1129    2.7e+04   4.19e-06      1.000   -5.29e+04    5.29e+04\n",
      "INTEREST_TYPE_140003.0                 -0.0102   3.62e+04  -2.83e-07      1.000    -7.1e+04     7.1e+04\n",
      "MORTGAGE_TYPE_1.0                      -0.0704   3890.471  -1.81e-05      1.000   -7625.254    7625.113\n",
      "MORTGAGE_TYPE_3.0                      -0.0212    361.627  -5.86e-05      1.000    -708.798     708.756\n",
      "MORTGAGE_TYPE_4.0                       0.0703   1.54e+05   4.56e-07      1.000   -3.02e+05    3.02e+05\n",
      "MORTGAGE_TYPE_5.0                       0.0549   6.47e+04   8.49e-07      1.000   -1.27e+05    1.27e+05\n",
      "MORTGAGE_TYPE_6.0                       0.0814   6.49e+04   1.26e-06      1.000   -1.27e+05    1.27e+05\n",
      "MORTGAGE_TYPE_7.0                       0.1032   8.09e+04   1.28e-06      1.000   -1.59e+05    1.59e+05\n",
      "MORTGAGE_TYPE_8.0                       0.0052   1.27e+05   4.09e-08      1.000    -2.5e+05     2.5e+05\n",
      "MORTGAGE_TYPE_10.0                     -0.0135   1409.639  -9.59e-06      1.000   -2762.856    2762.829\n",
      "MORTGAGE_TYPE_13.0                     -0.2850   2.24e+04  -1.27e-05      1.000   -4.38e+04    4.38e+04\n",
      "MORTGAGE_TYPE_41.0                     -0.1328   3.95e+04  -3.36e-06      1.000   -7.74e+04    7.74e+04\n",
      "MORTGAGE_TYPE_42.0                      0.0174   3.57e+04   4.87e-07      1.000      -7e+04       7e+04\n",
      "MORTGAGE_TYPE_43.0                     -0.0201   1.21e+04  -1.67e-06      1.000   -2.37e+04    2.37e+04\n",
      "MORTGAGE_TYPE_44.0                     -0.0742   2.42e+05  -3.07e-07      1.000   -4.74e+05    4.74e+05\n",
      "MORTGAGE_TYPE_45.0                     -0.0197   5.28e+05  -3.73e-08      1.000   -1.03e+06    1.03e+06\n",
      "MORTGAGE_TYPE_46.0                      0.2027   1.16e+06   1.74e-07      1.000   -2.28e+06    2.28e+06\n",
      "MORTGAGE_TYPE_47.0                     -0.1399   4.17e+04  -3.35e-06      1.000   -8.18e+04    8.18e+04\n",
      "MORTGAGE_TYPE_48.0                     -0.0188   5.78e+05  -3.26e-08      1.000   -1.13e+06    1.13e+06\n",
      "BORROWER_LOAN_COUNT                    -0.1235      0.021     -5.956      0.000      -0.164      -0.083\n",
      "TOTAL_LOAN_AMOUNT                      -0.4556      0.043    -10.694      0.000      -0.539      -0.372\n",
      "TOTAL_INSTALLMENT_AMOUNT                0.1122      0.018      6.275      0.000       0.077       0.147\n",
      "LOAN_BORROWER_COUNT                     0.0483      0.024      2.027      0.043       0.002       0.095\n",
      "=======================================================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.50 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "model, non_significant_vars = significant_features(training_data, X_columns, y_column,LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['const',\n",
       " 'CONTRACT_INSTALMENT_AMOUNT',\n",
       " 'CONTRACT_LOAN_AMOUNT',\n",
       " 'CONTRACT_MORTGAGE_TYPE',\n",
       " 'CONTRACT_TYPE_OF_INTEREST_REPAYMENT',\n",
       " 'BORROWER_COUNTY',\n",
       " 'BORROWER_TYPE_OF_SETTLEMENT',\n",
       " 'LOAN_TYPE_1f951336',\n",
       " 'LOAN_TYPE_2f88e16c',\n",
       " 'LOAN_TYPE_47693941',\n",
       " 'LOAN_TYPE_5a06241e',\n",
       " 'LOAN_TYPE_694cbaee',\n",
       " 'LOAN_TYPE_69f70539',\n",
       " 'LOAN_TYPE_7e2065f4',\n",
       " 'LOAN_TYPE_83910425',\n",
       " 'LOAN_TYPE_8fe006f1',\n",
       " 'LOAN_TYPE_955ae3ef',\n",
       " 'LOAN_TYPE_95c4f8fb',\n",
       " 'LOAN_TYPE_b503a0de',\n",
       " 'LOAN_TYPE_cde77491',\n",
       " 'LOAN_TYPE_cf07c2dd',\n",
       " 'LOAN_TYPE_d3aaffde',\n",
       " 'LOAN_TYPE_eab72d7a',\n",
       " 'LOAN_TYPE_f792971b',\n",
       " 'FREQ_TYPE_89efd382',\n",
       " 'FREQ_TYPE_bd092d5a',\n",
       " 'INTEREST_TYPE_100001.0',\n",
       " 'INTEREST_TYPE_100002.0',\n",
       " 'INTEREST_TYPE_100003.0',\n",
       " 'INTEREST_TYPE_100004.0',\n",
       " 'INTEREST_TYPE_110001.0',\n",
       " 'INTEREST_TYPE_140001.0',\n",
       " 'INTEREST_TYPE_140002.0',\n",
       " 'INTEREST_TYPE_140003.0',\n",
       " 'MORTGAGE_TYPE_1.0',\n",
       " 'MORTGAGE_TYPE_3.0',\n",
       " 'MORTGAGE_TYPE_4.0',\n",
       " 'MORTGAGE_TYPE_5.0',\n",
       " 'MORTGAGE_TYPE_6.0',\n",
       " 'MORTGAGE_TYPE_7.0',\n",
       " 'MORTGAGE_TYPE_8.0',\n",
       " 'MORTGAGE_TYPE_10.0',\n",
       " 'MORTGAGE_TYPE_13.0',\n",
       " 'MORTGAGE_TYPE_41.0',\n",
       " 'MORTGAGE_TYPE_42.0',\n",
       " 'MORTGAGE_TYPE_43.0',\n",
       " 'MORTGAGE_TYPE_44.0',\n",
       " 'MORTGAGE_TYPE_45.0',\n",
       " 'MORTGAGE_TYPE_46.0',\n",
       " 'MORTGAGE_TYPE_47.0',\n",
       " 'MORTGAGE_TYPE_48.0']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_significant_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns.remove('CONTRACT_CURRENCY')\n",
    "X_columns.remove('BORROWER_COUNTY')\n",
    "X_columns.remove('CONTRACT_MORTGAGE_TYPE')\n",
    "X_columns.remove('BORROWER_TYPE_OF_SETTLEMENT')\n",
    "# Assuming you have a pandas DataFrame named training_data\n",
    "# unique_training_data = training_data.drop_duplicates(subset='CONTRACT_ID', keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean in year 0.011345816067598232\n",
      "5206\n",
      "1750.5931790499392\n",
      "Logloss:\n",
      "0.023868488801544085\n",
      "ROC AUC Score: 0.9884455664174394\n",
      "0.01436353973362327\n"
     ]
    }
   ],
   "source": [
    "# 0.006785876080349979\n",
    "# Before centering:\n",
    "# 0.9978858840423693 8.548717289613705e-15 0.006063585227926563\n",
    "# Centering probabilities...\n",
    "# 1.0 0.008736424229876956 0.01479999999999999\n",
    "# One extra dummy\n",
    "# Logloss: 0.02502985804399272\n",
    "# Zero extra dummy\n",
    "# Logloss: 0.025941091599465484\n",
    "# All dummies\n",
    "# Logloss: 0.024974156733831323\n",
    "# Adding calib (isotonic)\n",
    "# Logloss: 0.02385418586559387\n",
    "# Logloss: imputed\n",
    "# 0.02372888852503455\n",
    "# Logloss: imputed and removed a few unnecessary variables\n",
    "# 0.023720691340422\n",
    "# Logloss: imouted and added monthly payment\n",
    "# 0.02294559159914238\n",
    "predicted_probs = 'LOGISTIC_REG'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    'TARGET_EVENT_BINARY',\n",
    "    model=LogisticRegression(max_iter=400, random_state=42,solver='lbfgs'),\n",
    "    augment_distribution=True,\n",
    "    augment_distribution_percentage=1.48,\n",
    "    calibrate=True,\n",
    "    unique_loans=False,\n",
    "    should_smote=False,\n",
    "    maximal_sample=False,\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "print(probs.mean())\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'LOGISTIC_REG'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011471762617978111\n"
     ]
    }
   ],
   "source": [
    "new_proba = scale_yearly_proba(training_data, 'LOGISTIC_REG')\n",
    "print(new_proba.mean())\n",
    "training_data['YEAR_SCALED_PROBA'] = new_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss:\n",
      "0.26207997638270275\n",
      "Before centering:\n",
      "0.9999999623206586 6.3741058324851e-06 0.015031788522875091\n",
      "Centering probabilities...\n",
      "0.00023178852287509058 2.001\n",
      "0.00022692203316808726 2.002\n",
      "0.00022205890538514947 2.0029999999999997\n",
      "0.00021719913576989137 2.0039999999999996\n",
      "0.00021234272057200215 2.0049999999999994\n",
      "0.0002074896560471315 2.0059999999999993\n",
      "0.0002026399384570752 2.0069999999999992\n",
      "0.0001977935640694456 2.007999999999999\n",
      "0.00019295052915800116 2.008999999999999\n",
      "0.00018811083000247995 2.009999999999999\n",
      "0.00018327446288839318 2.010999999999999\n",
      "0.0001784414241074294 2.0119999999999987\n",
      "0.0001736117099571128 2.0129999999999986\n",
      "0.00016878531674086386 2.0139999999999985\n",
      "0.00016396224076810004 2.0149999999999983\n",
      "0.0001591424783540761 2.0159999999999982\n",
      "0.00015432602581992756 2.016999999999998\n",
      "0.00014951287949272266 2.017999999999998\n",
      "0.0001447030357053427 2.018999999999998\n",
      "0.00013989649079654103 2.019999999999998\n",
      "0.00013509324111088926 2.0209999999999977\n",
      "0.00013029328299882757 2.0219999999999976\n",
      "0.00012549661281653808 2.0229999999999975\n",
      "0.00012070322692609575 2.0239999999999974\n",
      "0.00011591312169527586 2.0249999999999972\n",
      "0.00011112629349766152 2.025999999999997\n",
      "0.00010634273871263679 2.026999999999997\n",
      "0.00010156245372526865 2.027999999999997\n",
      "9.678543492642327e-05 2.028999999999997\n",
      "9.201167871265674e-05 2.0299999999999967\n",
      "8.724118148626357e-05 2.0309999999999966\n",
      "8.247393965520738e-05 2.0319999999999965\n",
      "7.770994963321627e-05 2.0329999999999964\n",
      "7.294920783959026e-05 2.0339999999999963\n",
      "6.81917106993956e-05 2.034999999999996\n",
      "6.343745464326353e-05 2.035999999999996\n",
      "5.868643610756029e-05 2.036999999999996\n",
      "5.3938651534194554e-05 2.037999999999996\n",
      "4.919409737079092e-05 2.0389999999999957\n",
      "4.4452770070479986e-05 2.0399999999999956\n",
      "3.971466609203368e-05 2.0409999999999955\n",
      "3.497978189983923e-05 2.0419999999999954\n",
      "3.0248113963796813e-05 2.0429999999999953\n",
      "2.55196587594219e-05 2.043999999999995\n",
      "2.079441276770823e-05 2.044999999999995\n",
      "1.607237247529085e-05 2.045999999999995\n",
      "1.135353437421889e-05 2.046999999999995\n",
      "6.637894962113408e-06 2.0479999999999947\n",
      "0.9999999442096401 6.227754078103054e-06 0.014806637894962114\n",
      "Saved file: ./predictions/best-model-so-far.csv\n"
     ]
    }
   ],
   "source": [
    "submission = create_submission_file(training_data, 'YEAR_SCALED_PROBA', data_submission_example, filename='./predictions/best-model-so-far.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean in year 0.011345816067598232\n",
      "5206\n",
      "1750.5931790499392\n",
      "Logloss:\n",
      "0.01727993881580623\n",
      "ROC AUC Score: 0.9963451071460232\n",
      "0.012088098393529596\n"
     ]
    }
   ],
   "source": [
    "# Logloss:\n",
    "# 0.010851681705608482\n",
    "# ROC AUC Score: 0.9988352689711549\n",
    "predicted_probs = 'XGB'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    'TARGET_EVENT_BINARY',\n",
    "    model=xgb.XGBClassifier(max_depth=2, n_estimators=70, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    augment_distribution=True,\n",
    "    augment_distribution_percentage=1.48,\n",
    "    calibrate=True,\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "print(probs.mean())\n",
    "# this helps\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'XGB'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011420687278482304\n"
     ]
    }
   ],
   "source": [
    "training_data['ENSEMBLE'] = 0.4*training_data['XGB'] + 0.6*training_data['LOGISTIC_REG']\n",
    "new_proba = scale_yearly_proba(training_data, 'ENSEMBLE')\n",
    "training_data['XGB_YEAR_SCALED_PROBA'] = new_proba\n",
    "print(new_proba.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss:\n",
      "0.263031275248133\n",
      "Before centering:\n",
      "0.9999999995735819 4.142014932706495e-06 0.014833470872319666\n",
      "Centering probabilities...\n",
      "3.347087231966514e-05 2.001\n",
      "2.886188259912728e-05 2.002\n",
      "2.4256032155318616e-05 2.0029999999999997\n",
      "1.9653317456656133e-05 2.0039999999999996\n",
      "1.5053734977378552e-05 2.0049999999999994\n",
      "1.045728119745265e-05 2.0059999999999993\n",
      "5.8639526025437705e-06 2.0069999999999992\n",
      "0.9999999995451564 4.129626080096038e-06 0.014805863952602544\n",
      "Saved file: ./predictions/best-model-so-far.csv\n"
     ]
    }
   ],
   "source": [
    "submission = create_submission_file(training_data, 'XGB_YEAR_SCALED_PROBA', data_submission_example, filename='./predictions/best-model-so-far.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean in year 0.011345816067598232\n",
      "5206\n",
      "1750.5931790499392\n",
      "Logloss:\n",
      "0.018344382184530682\n",
      "ROC AUC Score: 0.9961497377868637\n",
      "0.01241943327260914\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Logloss:\n",
    "# 0.02484109588442129 RF 200-3\n",
    "# Logloss:\n",
    "# 0.023534501248296504 RF 200-4-10\n",
    "# Logloss:\n",
    "# 0.02304888836026815 400-4- log_loss 25 \n",
    "# Logloss:\n",
    "# 0.0216212401349101 400-5- log_loss 25\n",
    "# Logloss:\n",
    "# 0.020320409332841818 400-6- log_loss 25\n",
    "# Logloss:\n",
    "# 0.018208761775049223 400-8- log_loss 30\n",
    "# Logloss:\n",
    "# 0.019426191288290054 200-8- log_loss 100\n",
    "# Logloss:\n",
    "# 0.01860565128621486 400 10 Logloss 100\n",
    "# Logloss:\n",
    "# 0.018445624629156568 400 11 Logloss 100\n",
    "\n",
    "predicted_probs = 'NN'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    'TARGET_EVENT_BINARY',\n",
    "    model= RandomForestClassifier(n_estimators=600, max_depth=11, random_state=42, min_samples_leaf=100, criterion=\"log_loss\", n_jobs=-1   ),\n",
    "    augment_distribution=True,\n",
    "    augment_distribution_percentage=1.48,\n",
    "    calibrate=True,\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "print(probs.mean())\n",
    "# this helps\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'NN'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['ENSEMBLE_NN'] = 0.5*training_data['ENSEMBLE']+ 0.5*training_data['NN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011410706533720307\n"
     ]
    }
   ],
   "source": [
    "new_proba = scale_yearly_proba(training_data, 'ENSEMBLE_NN')\n",
    "print(new_proba.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['NN_LOG_XGB'] = new_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss:\n",
      "0.26391921060636875\n",
      "Before centering:\n",
      "0.9999999999998295 2.2064184639347673e-06 0.01463195102982293\n",
      "Centering probabilities...\n",
      "-0.00016804897017707113 1.999\n",
      "-0.00016364759303028747 1.9980000000000002\n",
      "-0.0001592432506811025 1.9970000000000003\n",
      "-0.0001548359397882306 1.9960000000000004\n",
      "-0.00015042565700490625 1.9950000000000006\n",
      "-0.00014601239897886475 1.9940000000000007\n",
      "-0.00014159616235236497 1.9930000000000008\n",
      "-0.00013717694376209552 1.9920000000000009\n",
      "-0.0001327547398392824 1.991000000000001\n",
      "-0.0001283295472095953 1.990000000000001\n",
      "-0.000123901362493092 1.9890000000000012\n",
      "-0.00011947018230437124 1.9880000000000013\n",
      "-0.00011503600325238339 1.9870000000000014\n",
      "-0.00011059882194050345 1.9860000000000015\n",
      "-0.00010615863496651716 1.9850000000000017\n",
      "-0.00010171543892258973 1.9840000000000018\n",
      "-9.726923039528844e-05 1.9830000000000019\n",
      "-9.282000596550107e-05 1.982000000000002\n",
      "-8.836776220849316e-05 1.981000000000002\n",
      "-8.391249569386638e-05 1.9800000000000022\n",
      "-7.945420298554988e-05 1.9790000000000023\n",
      "-7.499288064178634e-05 1.9780000000000024\n",
      "-7.052852521510083e-05 1.9770000000000025\n",
      "-6.606113325233545e-05 1.9760000000000026\n",
      "-6.159070129459383e-05 1.9750000000000028\n",
      "-5.711722587725329e-05 1.9740000000000029\n",
      "-5.264070352990584e-05 1.973000000000003\n",
      "-4.816113077644839e-05 1.972000000000003\n",
      "-4.3678504134942245e-05 1.9710000000000032\n",
      "-3.9192820117687696e-05 1.9700000000000033\n",
      "-3.470407523120146e-05 1.9690000000000034\n",
      "-3.0212265976133426e-05 1.9680000000000035\n",
      "-2.57173888473846e-05 1.9670000000000036\n",
      "-2.121944033394753e-05 1.9660000000000037\n",
      "-1.671841691899996e-05 1.9650000000000039\n",
      "-1.2214315079852808e-05 1.964000000000004\n",
      "-7.707131287934543e-06 1.963000000000004\n",
      "0.9999999999999005 2.246861934174227e-06 0.014792292868712066\n",
      "Saved file: ./predictions/nn-xgb-log.csv\n"
     ]
    }
   ],
   "source": [
    "# Logloss:\n",
    "# 0.2627792536515289: \n",
    "submission = create_submission_file(training_data, 'NN_LOG_XGB', data_submission_example, filename='./predictions/nn-xgb-log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean in year 0.011345816067598232\n",
      "5206\n",
      "1750.5931790499392\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394910\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4502\n",
      "[LightGBM] [Info] Number of data points in the train set: 400842, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198296\n",
      "[LightGBM] [Info] Start training from score -4.198296\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394910\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4506\n",
      "[LightGBM] [Info] Number of data points in the train set: 400842, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198296\n",
      "[LightGBM] [Info] Start training from score -4.198296\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394910\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4504\n",
      "[LightGBM] [Info] Number of data points in the train set: 400842, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198296\n",
      "[LightGBM] [Info] Start training from score -4.198296\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394911\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4505\n",
      "[LightGBM] [Info] Number of data points in the train set: 400843, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198299\n",
      "[LightGBM] [Info] Start training from score -4.198299\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394911\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051248 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4504\n",
      "[LightGBM] [Info] Number of data points in the train set: 400843, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198299\n",
      "[LightGBM] [Info] Start training from score -4.198299\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Logloss:\n",
      "0.01688919180840362\n",
      "ROC AUC Score: 0.9962910399639101\n",
      "0.011813348099003687\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Logloss:\n",
    "# 0.02484109588442129 RF 200-3\n",
    "predicted_probs = 'LGBM'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    'TARGET_EVENT_BINARY',\n",
    "    model= LGBMClassifier(n_estimators=60, max_depth=4,boosting_type=\"dart\", random_state=42),\n",
    "    augment_distribution=True,\n",
    "    augment_distribution_percentage=1.48,\n",
    "    calibrate=True,\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "print(probs.mean())\n",
    "# this helps\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'LGBM'] = 0\n",
    "\n",
    "training_data['ENSEMBLE_LGBM'] = 0.9*training_data['ENSEMBLE_NN']+ 0.1*training_data['LGBM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011416816987101135\n"
     ]
    }
   ],
   "source": [
    "new_proba = scale_yearly_proba(training_data, 'ENSEMBLE_LGBM')\n",
    "print(new_proba.mean())\n",
    "training_data['NN_LOG_XGB_LGBM'] = new_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss:\n",
      "0.2658451083533875\n",
      "Before centering:\n",
      "0.9999999999999993 7.133126343594753e-06 0.01454769895618425\n",
      "Centering probabilities...\n",
      "-0.0002523010438157513 1.999\n",
      "-0.00024821121286087294 1.9980000000000002\n",
      "-0.0002441187098878063 1.9970000000000003\n",
      "-0.00024002353190681787 1.9960000000000004\n",
      "-0.00023592567592323373 1.9950000000000006\n",
      "-0.00023182513893748452 1.9940000000000007\n",
      "-0.00022772191794510552 1.9930000000000008\n",
      "-0.00022361600993668108 1.9920000000000009\n",
      "-0.0002195074118978603 1.991000000000001\n",
      "-0.00021539612080934308 1.990000000000001\n",
      "-0.00021128213364687495 1.9890000000000012\n",
      "-0.0002071654473812315 1.9880000000000013\n",
      "-0.00020304605897822348 1.9870000000000014\n",
      "-0.00019892396539863445 1.9860000000000015\n",
      "-0.0001947991635982433 1.9850000000000017\n",
      "-0.00019067165052788666 1.9840000000000018\n",
      "-0.00018654142313326467 1.9830000000000019\n",
      "-0.00018240847835514044 1.982000000000002\n",
      "-0.00017827281312917005 1.981000000000002\n",
      "-0.0001741344243859893 1.9800000000000022\n",
      "-0.00016999330905114428 1.9790000000000023\n",
      "-0.0001658494640451088 1.9780000000000024\n",
      "-0.00016170288628326 1.9770000000000025\n",
      "-0.00015755357267587672 1.9760000000000026\n",
      "-0.00015340152012812726 1.9750000000000028\n",
      "-0.0001492467255400521 1.9740000000000029\n",
      "-0.00014508918580655693 1.973000000000003\n",
      "-0.00014092889781741266 1.972000000000003\n",
      "-0.00013676585845719472 1.9710000000000032\n",
      "-0.00013260006460537842 1.9700000000000033\n",
      "-0.00012843151313617074 1.9690000000000034\n",
      "-0.00012426020091864907 1.9680000000000035\n",
      "-0.00012008612481666757 1.9670000000000036\n",
      "-0.0001159092816888814 1.9660000000000037\n",
      "-0.00011172966838871728 1.9650000000000039\n",
      "-0.00010754728176430757 1.964000000000004\n",
      "-0.00010336211865862206 1.963000000000004\n",
      "-9.917417590928936e-05 1.9620000000000042\n",
      "-9.498345034874775e-05 1.9610000000000043\n",
      "-9.078993880409433e-05 1.9600000000000044\n",
      "-8.659363809713355e-05 1.9590000000000045\n",
      "-8.2394545044372e-05 1.9580000000000046\n",
      "-7.819265645701845e-05 1.9570000000000047\n",
      "-7.398796914091788e-05 1.9560000000000048\n",
      "-6.978047989658273e-05 1.955000000000005\n",
      "-6.557018551919466e-05 1.954000000000005\n",
      "-6.135708279852992e-05 1.9530000000000052\n",
      "-5.7141168518997504e-05 1.9520000000000053\n",
      "-5.2922439459658285e-05 1.9510000000000054\n",
      "-4.870089239410874e-05 1.9500000000000055\n",
      "-4.447652409056077e-05 1.9490000000000056\n",
      "-4.024933131181914e-05 1.9480000000000057\n",
      "-3.60193108152225e-05 1.9470000000000058\n",
      "-3.178645935266594e-05 1.946000000000006\n",
      "-2.7550773670571502e-05 1.945000000000006\n",
      "-2.331225050991592e-05 1.9440000000000062\n",
      "-1.907088660616993e-05 1.9430000000000063\n",
      "-1.4826678689313852e-05 1.9420000000000064\n",
      "-1.0579623483822007e-05 1.9410000000000065\n",
      "-6.329717708598512e-06 1.9400000000000066\n",
      "0.9999999999999998 7.3499490678141655e-06 0.014793670282291402\n",
      "Saved file: ./predictions/nn-xgb-log-lgbm.csv\n"
     ]
    }
   ],
   "source": [
    "# Logloss:\n",
    "# 0.2627792536515289: \n",
    "submission = create_submission_file(training_data, 'NN_LOG_XGB_LGBM', data_submission_example, filename='./predictions/nn-xgb-log-lgbm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean in year 0.011345816067598232\n",
      "5206\n",
      "1750.5931790499392\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394910\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4502\n",
      "[LightGBM] [Info] Number of data points in the train set: 400842, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198296\n",
      "[LightGBM] [Info] Start training from score -4.198296\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032953 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4505\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032255 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4497\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031433 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4501\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4502\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4497\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394910\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039011 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4506\n",
      "[LightGBM] [Info] Number of data points in the train set: 400842, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198296\n",
      "[LightGBM] [Info] Start training from score -4.198296\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4503\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4495\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4499\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4500\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4499\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394910\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4504\n",
      "[LightGBM] [Info] Number of data points in the train set: 400842, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198296\n",
      "[LightGBM] [Info] Start training from score -4.198296\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4497\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4500\n",
      "[LightGBM] [Info] Number of data points in the train set: 320673, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198423\n",
      "[LightGBM] [Info] Start training from score -4.198423\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4501\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4504\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4499\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394911\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4505\n",
      "[LightGBM] [Info] Number of data points in the train set: 400843, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198299\n",
      "[LightGBM] [Info] Start training from score -4.198299\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4503\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4503\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198426\n",
      "[LightGBM] [Info] Start training from score -4.198426\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037213 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4499\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198426\n",
      "[LightGBM] [Info] Start training from score -4.198426\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030494 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4502\n",
      "[LightGBM] [Info] Number of data points in the train set: 320675, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198215\n",
      "[LightGBM] [Info] Start training from score -4.198215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4505\n",
      "[LightGBM] [Info] Number of data points in the train set: 320675, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198215\n",
      "[LightGBM] [Info] Start training from score -4.198215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 5932, number of negative: 394911\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4504\n",
      "[LightGBM] [Info] Number of data points in the train set: 400843, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014799 -> initscore=-4.198299\n",
      "[LightGBM] [Info] Start training from score -4.198299\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315928\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034786 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4500\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198212\n",
      "[LightGBM] [Info] Start training from score -4.198212\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4503\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198426\n",
      "[LightGBM] [Info] Start training from score -4.198426\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4745, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029607 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4500\n",
      "[LightGBM] [Info] Number of data points in the train set: 320674, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014797 -> initscore=-4.198426\n",
      "[LightGBM] [Info] Start training from score -4.198426\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4503\n",
      "[LightGBM] [Info] Number of data points in the train set: 320675, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198215\n",
      "[LightGBM] [Info] Start training from score -4.198215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 4746, number of negative: 315929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4502\n",
      "[LightGBM] [Info] Number of data points in the train set: 320675, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014800 -> initscore=-4.198215\n",
      "[LightGBM] [Info] Start training from score -4.198215\n",
      "Logloss:\n",
      "0.016402771113048446\n",
      "ROC AUC Score: 0.9969368434110715\n",
      "0.011993954257523266\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "# XGB is the lowest score \n",
    "# Logloss:\n",
    "# 0.01727067871139171\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=400, max_depth=11, random_state=42, min_samples_leaf=100, criterion=\"log_loss\", n_jobs=-1   )),\n",
    "    ('lgb', LGBMClassifier(n_estimators=60, max_depth=4,boosting_type=\"dart\", random_state=42)),\n",
    "    ('xgb', xgb.XGBClassifier(max_depth=2, n_estimators=70, random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
    "]\n",
    "meta_model = LogisticRegression()\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "\n",
    "predicted_probs = 'STACKED'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    'TARGET_EVENT_BINARY',\n",
    "    model= stacked_model,\n",
    "    augment_distribution=True,\n",
    "    augment_distribution_percentage=1.48,\n",
    "    calibrate=True,\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "print(probs.mean())\n",
    "# this helps\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'STACKED'] = 0\n",
    "\n",
    "training_data['ENSEMBLE_STACKED'] = 0.9*training_data['ENSEMBLE_NN']+ 0.1*training_data['STACKED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_model.final_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean in year 0.011345816067598232\n",
      "5206\n",
      "1750.5931790499392\n",
      "Logloss:\n",
      "0.03345960035278355\n",
      "ROC AUC Score: 0.9690328434892286\n",
      "0.019327983772770608\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "\n",
    "predicted_probs = 'QDA'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    'TARGET_EVENT_BINARY',\n",
    "    model= QuadraticDiscriminantAnalysis(),\n",
    "    augment_distribution=True,\n",
    "    augment_distribution_percentage=1.48,\n",
    "    calibrate=True,\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "print(probs.mean())\n",
    "# this helps\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'QDA'] = 0\n",
    "\n",
    "training_data['ENSEMBLE_QDA'] = 0.9*training_data['ENSEMBLE_NN']+ 0.1*training_data['QDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean in year 0.011345816067598232\n",
      "5206\n",
      "1750.5931790499392\n",
      "Logloss:\n",
      "0.029537779758320133\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 880. MiB for an array with shape (1601663, 72) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 589, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py\", line 206, in _parallel_decision_function\n    return sum(\n           ^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py\", line 207, in <genexpr>\n    estimator.decision_function(X[:, features])\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 783, in decision_function\n    dec = self._decision_function(X)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 532, in _decision_function\n    X = self._validate_for_predict(X)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 611, in _validate_for_predict\n    X = self._validate_data(\n        ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 604, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 880. MiB for an array with shape (1601663, 72) and data type float64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\takat\\Documents\\Egyetem\\Gépi tanulási esettanulmány labor\\Defaults\\two-more-dummies.ipynb Cell 49\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Logloss:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# 0.025080414519332874 1% & 10 estimators\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m predicted_probs \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSVC\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m probs \u001b[39m=\u001b[39m train_and_predict_two_halves(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     training_data, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     X_columns, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mTARGET_EVENT_BINARY\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     model\u001b[39m=\u001b[39;49m BaggingClassifier(SVC(kernel\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrbf\u001b[39;49m\u001b[39m'\u001b[39;49m, probability\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), max_samples\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m, n_estimators\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     augment_distribution\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     augment_distribution_percentage\u001b[39m=\u001b[39;49m\u001b[39m1.48\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     calibrate\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m training_data[predicted_probs] \u001b[39m=\u001b[39m probs\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(probs\u001b[39m.\u001b[39mmean())\n",
      "\u001b[1;32mc:\\Users\\takat\\Documents\\Egyetem\\Gépi tanulási esettanulmány labor\\Defaults\\two-more-dummies.ipynb Cell 49\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m test_proba \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_proba(X_filtered)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(log_loss(y_filtered,test_proba))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m proba \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_proba(X_scaled)[:, \u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Additional code for ROC AUC score\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/two-more-dummies.ipynb#Y135sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m roc_auc \u001b[39m=\u001b[39m roc_auc_score(y_filtered,test_proba[:,\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\calibration.py:500\u001b[0m, in \u001b[0;36mCalibratedClassifierCV.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    498\u001b[0m mean_proba \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((_num_samples(X), \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_)))\n\u001b[0;32m    499\u001b[0m \u001b[39mfor\u001b[39;00m calibrated_classifier \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalibrated_classifiers_:\n\u001b[1;32m--> 500\u001b[0m     proba \u001b[39m=\u001b[39m calibrated_classifier\u001b[39m.\u001b[39;49mpredict_proba(X)\n\u001b[0;32m    501\u001b[0m     mean_proba \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m proba\n\u001b[0;32m    503\u001b[0m mean_proba \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalibrated_classifiers_)\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\calibration.py:792\u001b[0m, in \u001b[0;36m_CalibratedClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    790\u001b[0m n_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses)\n\u001b[0;32m    791\u001b[0m pred_method, method_name \u001b[39m=\u001b[39m _get_prediction_method(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator)\n\u001b[1;32m--> 792\u001b[0m predictions \u001b[39m=\u001b[39m _compute_predictions(pred_method, method_name, X, n_classes)\n\u001b[0;32m    794\u001b[0m label_encoder \u001b[39m=\u001b[39m LabelEncoder()\u001b[39m.\u001b[39mfit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses)\n\u001b[0;32m    795\u001b[0m pos_class_indices \u001b[39m=\u001b[39m label_encoder\u001b[39m.\u001b[39mtransform(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator\u001b[39m.\u001b[39mclasses_)\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\calibration.py:683\u001b[0m, in \u001b[0;36m_compute_predictions\u001b[1;34m(pred_method, method_name, X, n_classes)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_compute_predictions\u001b[39m(pred_method, method_name, X, n_classes):\n\u001b[0;32m    660\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return predictions for `X` and reshape binary outputs to shape\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[39m    (n_samples, 1).\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[39m        (X.shape[0], 1).\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 683\u001b[0m     predictions \u001b[39m=\u001b[39m pred_method(X\u001b[39m=\u001b[39;49mX)\n\u001b[0;32m    685\u001b[0m     \u001b[39mif\u001b[39;00m method_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdecision_function\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    686\u001b[0m         \u001b[39mif\u001b[39;00m predictions\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:970\u001b[0m, in \u001b[0;36mBaggingClassifier.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[39m# Parallel loop\u001b[39;00m\n\u001b[0;32m    968\u001b[0m n_jobs, _, starts \u001b[39m=\u001b[39m _partition_estimators(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n\u001b[1;32m--> 970\u001b[0m all_decisions \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose)(\n\u001b[0;32m    971\u001b[0m     delayed(_parallel_decision_function)(\n\u001b[0;32m    972\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimators_[starts[i] : starts[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]],\n\u001b[0;32m    973\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimators_features_[starts[i] : starts[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]],\n\u001b[0;32m    974\u001b[0m         X,\n\u001b[0;32m    975\u001b[0m     )\n\u001b[0;32m    976\u001b[0m     \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(n_jobs)\n\u001b[0;32m    977\u001b[0m )\n\u001b[0;32m    979\u001b[0m \u001b[39m# Reduce\u001b[39;00m\n\u001b[0;32m    980\u001b[0m decisions \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(all_decisions) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1693\u001b[0m \n\u001b[0;32m   1694\u001b[0m     \u001b[39m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m     \u001b[39m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[39m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     \u001b[39m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aborting:\n\u001b[1;32m-> 1699\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_error_fast()\n\u001b[0;32m   1700\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m     \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m     \u001b[39m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[39m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[39m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m \u001b[39m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[39mif\u001b[39;00m error_job \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1734\u001b[0m     error_job\u001b[39m.\u001b[39;49mget_result(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    730\u001b[0m backend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel\u001b[39m.\u001b[39m_backend\n\u001b[0;32m    732\u001b[0m \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    733\u001b[0m     \u001b[39m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[39m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[39m# be returned.\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_return_or_raise()\n\u001b[0;32m    738\u001b[0m \u001b[39m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 754\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[0;32m    756\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 880. MiB for an array with shape (1601663, 72) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Logloss:\n",
    "# 0.025080414519332874 1% & 10 estimators\n",
    "predicted_probs = 'SVC'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    'TARGET_EVENT_BINARY',\n",
    "    model= BaggingClassifier(SVC(kernel='rbf', probability=True), max_samples=2000, n_estimators=100, n_jobs=-1, random_state=42),\n",
    "    augment_distribution=True,\n",
    "    augment_distribution_percentage=1.48,\n",
    "    calibrate=True,\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "print(probs.mean())\n",
    "training_data.loc[training_data['TARGET_EVENT'] == 'E', 'SVC'] = 0\n",
    "training_data['ENSEMBLE_SVC'] = 0.9*training_data['ENSEMBLE_LGBM']+ 0.1*training_data['SVC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_proba = scale_yearly_proba(training_data, 'ENSEMBLE_SVC')\n",
    "print(new_proba.mean())\n",
    "training_data['NN_LOG_XGB_LGBM_SVC'] = new_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logloss:\n",
    "# 0.2627792536515289: \n",
    "submission = create_submission_file(training_data, 'NN_LOG_XGB_LGBM_SVC', data_submission_example, filename='./predictions/nn-xgb-log-lgbm-svc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "# Other necessary imports, like pandas, if you're using a DataFrame\n",
    "\n",
    "def train_and_evaluate(df, variables, target, model=LogisticRegression(), scaler=StandardScaler(), cv_folds=5, augment_distribution=False):\n",
    "    # Your lognormal_variables transformation code remains the same\n",
    "    if lognormal_variables is not None:\n",
    "        df = df.copy()\n",
    "        for var in lognormal_variables:\n",
    "            if var == 'CONTRACT_CREDIT_LOSS':\n",
    "                df[var] = np.log1p(np.abs(df[var]))*np.sign(df[var])\n",
    "            else:\n",
    "                df[var] = np.log1p(df[var])\n",
    "\n",
    "    # Split the data before the given date for training \n",
    "    if augment_distribution:\n",
    "        df_filtered = default_percentage_generator_2016(df, 1.48/100, target)\n",
    "    else:\n",
    "        df_filtered = df[df['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] < '2016-01-01']\n",
    "    X_filtered = df_filtered[variables]\n",
    "    y_filtered = df_filtered[target]\n",
    "    \n",
    "    # Scale the filtered features\n",
    "    X_filtered_scaled = scaler.fit_transform(X_filtered)\n",
    "    \n",
    "    # Perform cross-validation and fit the model\n",
    "    cv_scores = cross_val_score(model, X_filtered_scaled, y_filtered, cv=cv_folds, scoring='neg_log_loss')\n",
    "    \n",
    "    print(f\"CV Log Loss Scores: {-cv_scores}\")\n",
    "    print(f\"Mean CV Log Loss: {-np.mean(cv_scores)}\")\n",
    "    \n",
    "    # Now fit the model to the entire dataset\n",
    "    model.fit(X_filtered_scaled, y_filtered)\n",
    "    \n",
    "    # Scale the entire dataset to predict probabilities for all records\n",
    "    X_scaled = scaler.transform(df[variables])\n",
    "    \n",
    "    # Predict probabilities using the trained model\n",
    "    proba = model.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "    print(\"Logloss:\")\n",
    "    print(log_loss(y_filtered, model.predict_proba(X_filtered_scaled)))\n",
    "    # Calculate log loss on the full dataset\n",
    "    full_log_loss = log_loss(df[target], proba)\n",
    "\n",
    "    print(f\"Full Dataset Log Loss: {full_log_loss}\")\n",
    "\n",
    "    return proba, cv_scores, full_log_loss\n",
    "\n",
    "# You might call the function like this\n",
    "# probabilities, cv_scores, full_log_loss = train_and_evaluate(df, variables, target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logloss xgb :\n",
    "# 0.016317211616370328\n",
    "# Logloss logistic regression:\n",
    "# 0.02385418586559387\n",
    "predicted_probs = 'XGBOOST'\n",
    "probs, _, _ = train_and_evaluate(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    'TARGET_EVENT_BINARY',\n",
    "    model=xgb.XGBClassifier(max_depth=3, reg_lambda=1, n_estimators=60, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    augment_distribution=True\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "print(probs.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_number = 1.6\n",
    "experiment_probs = calculate_probabilities(training_data, 'XGBOOST', magic_number)\n",
    "print(experiment_probs.mean(), experiment_probs.max(), experiment_probs.min(), experiment_probs.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['EXPERIMENT_PROBS_XGB'] = experiment_probs\n",
    "submission_exp = create_submission_file(training_data, 'EXPERIMENT_PROBS_XGB', data_submission_example, filename='./predictions/xgb-prediction-with-time-param-0-585.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(experiment_probs.mean(), experiment_probs.max(), experiment_probs.min(), experiment_probs.std())\n",
    "sns.kdeplot(submission, label='Submission OG', lw=3, alpha=1)\n",
    "sns.kdeplot(training_data['LOGISTIC_REG'], label='PredictionOG', lw=3, alpha=1)\n",
    "sns.kdeplot(experiment_probs, label='Fine tuned solution', lw=3, alpha=1)\n",
    "sns.kdeplot(submission_exp, label='Fine tuned solution per borrower', lw=3, alpha=1)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('Overlayed Histogram of Submission and Predicted Probabilities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (submission_exp['PRED'] > 0.9999)# | (submission_exp['PRED'] < 0.01)\n",
    "ensemble_submission = submission.copy()\n",
    "ensemble_submission.loc[condition,'PRED'] = submission_exp.loc[condition,'PRED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_submission['PRED'].mean()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_probabilities_vec(ensemble_submission['PRED'].values, 2.029).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_submission['PRED'] = calculate_probabilities_vec(ensemble_submission['PRED'].values, 2.029)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_submission.to_csv('./predictions/ensemble-submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(experiment_probs.mean(), experiment_probs.max(), experiment_probs.min(), experiment_probs.std())\n",
    "sns.kdeplot(submission, label='Submission OG', lw=3, alpha=1, color='r')\n",
    "# sns.kdeplot(training_data['LOGISTIC_REG'], label='PredictionOG', lw=3, alpha=1)\n",
    "sns.kdeplot(ensemble_submission, label='Fine tuned solution', lw=3, alpha=1, color='g')\n",
    "# sns.kdeplot(submission_exp, label='Fine tuned solution per borrower', lw=3, alpha=1)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('Overlayed Histogram of Submission and Predicted Probabilities')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
