{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni_D83AE5_uni (Baseline)\n",
    "import pandas as pd\n",
    "# Importing necessary libraries for logistic regression and scaling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, log_loss, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "# Load the data\n",
    "training_data = pd.read_csv('training_data.csv')\n",
    "\n",
    "# Drop rows where BORROWER_ID is 'xNullx'\n",
    "training_data = training_data[training_data['BORROWER_ID'] != 'xNullx']\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "training_data = training_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "data_submission_example = pd.read_csv('data_submission_example.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Assuming training_data =  training_data.sample(n=10000, random_state=1)\n",
    "\n",
    "# Filling NaN values with 0\n",
    "training_data.fillna(0, inplace=True)\n",
    "\n",
    "# Converting columns to numeric where possible\n",
    "for col in training_data.columns:\n",
    "    try:\n",
    "        training_data[col] = pd.to_numeric(training_data[col], errors='ignore')\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Creating a sample target variable\n",
    "training_data['TARGET_EVENT_BINARY'] = np.where(training_data['TARGET_EVENT'] == 'K', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lognormal_variables = [\n",
    "    'CONTRACT_CREDIT_LOSS', 'CONTRACT_DEPT_SERVICE_TO_INCOME',\n",
    "    'CONTRACT_INCOME', 'CONTRACT_INSTALMENT_AMOUNT', 'CONTRACT_INSTALMENT_AMOUNT_2',\n",
    "    'CONTRACT_LOAN_AMOUNT', 'CONTRACT_MARKET_VALUE', 'CONTRACT_MORTGAGE_LENDING_VALUE', \n",
    "    'CONTRACT_LGD', 'CONTRACT_INCOME'\n",
    "]\n",
    "date_variables = ['CONTRACT_DATE_OF_LOAN_AGREEMENT', 'CONTRACT_MATURITY_DATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['TARGET_EVENT_DAY_DATETIME'] = pd.to_datetime(training_data['TARGET_EVENT_DAY'])\n",
    "training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] = pd.to_datetime(training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT'])\n",
    "\n",
    "# Calculate the day difference\n",
    "training_data['DAY_DIFF'] = (training_data['TARGET_EVENT_DAY_DATETIME'] - training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME']).dt.days\n",
    "\n",
    "# Create TARGET_EVENT_BINARY_2Y based on conditions\n",
    "training_data['TARGET_EVENT_BINARY_2Y'] = np.where(\n",
    "    (training_data['TARGET_EVENT'] == 'K') & \n",
    "    (training_data['DAY_DIFF'] <= 730) & \n",
    "    (training_data['DAY_DIFF'] >= 0), \n",
    "    1, \n",
    "    0\n",
    ")\n",
    "training_data['TARGET_EVENT_BINARY_1Y'] = np.where(\n",
    "        (training_data['TARGET_EVENT'] == 'K') & \n",
    "        (training_data['DAY_DIFF'] <= 365) & \n",
    "        (training_data['DAY_DIFF'] >= 0), \n",
    "        1, \n",
    "        0\n",
    "    )\n",
    "# Drop the temporary 'DAY_DIFF' column if needed\n",
    "training_data.drop('DAY_DIFF', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CONTRACT_CREDIT_INTERMEDIARY',\n",
       " 'CONTRACT_CREDIT_LOSS',\n",
       " 'CONTRACT_CURRENCY',\n",
       " 'CONTRACT_DEPT_SERVICE_TO_INCOME',\n",
       " 'CONTRACT_INCOME',\n",
       " 'CONTRACT_INSTALMENT_AMOUNT',\n",
       " 'CONTRACT_INSTALMENT_AMOUNT_2',\n",
       " 'CONTRACT_INTEREST_PERIOD',\n",
       " 'CONTRACT_INTEREST_RATE',\n",
       " 'CONTRACT_LGD',\n",
       " 'CONTRACT_LOAN_AMOUNT',\n",
       " 'CONTRACT_LOAN_CONTRACT_TYPE',\n",
       " 'CONTRACT_LOAN_TO_VALUE_RATIO',\n",
       " 'CONTRACT_MARKET_VALUE',\n",
       " 'CONTRACT_MORTGAGE_LENDING_VALUE',\n",
       " 'CONTRACT_MORTGAGE_TYPE',\n",
       " 'CONTRACT_REFINANCED',\n",
       " 'CONTRACT_RISK_WEIGHTED_ASSETS',\n",
       " 'CONTRACT_TYPE_OF_INTEREST_REPAYMENT',\n",
       " 'BORROWER_BIRTH_YEAR',\n",
       " 'BORROWER_CITIZENSHIP',\n",
       " 'BORROWER_COUNTRY',\n",
       " 'BORROWER_COUNTY',\n",
       " 'BORROWER_TYPE_OF_SETTLEMENT']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify numeric columns\n",
    "numeric_columns = training_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Identify target columns that shouldn't be in the X variables\n",
    "excluded_keywords = ['TARGET', 'event', 'binary', 'DATE']\n",
    "\n",
    "# Create lists for X variable columns and target column\n",
    "X_columns = [col for col in numeric_columns if all(keyword.lower() not in col.lower() for keyword in excluded_keywords)]\n",
    "y_column = 'TARGET_EVENT_BINARY_2Y' \n",
    "X_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the 'CONTRACT_LOAN_TYPE' and 'CONTRACT_FREQUENCY_TYPE' columns\n",
    "loan_type_dummies = pd.get_dummies(training_data['CONTRACT_LOAN_TYPE'], prefix='LOAN_TYPE', drop_first=True)\n",
    "frequency_type_dummies = pd.get_dummies(training_data['CONTRACT_FREQUENCY_TYPE'], prefix='FREQ_TYPE', drop_first=True)\n",
    "\n",
    "# Concatenate the original DataFrame with the one-hot encoded columns\n",
    "training_data = pd.concat([training_data, loan_type_dummies, frequency_type_dummies], axis=1)\n",
    "\n",
    "# Add the names of the one-hot encoded columns to X_columns\n",
    "X_columns.extend(loan_type_dummies.columns)\n",
    "X_columns.extend(frequency_type_dummies.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['BORROWER_LOAN_COUNT'] = training_data.groupby('BORROWER_ID')['BORROWER_ID'].transform('count')\n",
    "training_data['TOTAL_LOAN_AMOUNT'] = training_data.groupby('BORROWER_ID')['CONTRACT_LOAN_AMOUNT'].transform('sum')\n",
    "training_data['TOTAL_INSTALLMENT_AMOUNT_1'] = training_data.groupby('BORROWER_ID')['CONTRACT_INSTALMENT_AMOUNT'].transform('sum')\n",
    "training_data['TOTAL_INSTALLMENT_AMOUNT_2'] = training_data.groupby('BORROWER_ID')['CONTRACT_INSTALMENT_AMOUNT_2'].transform('sum')\n",
    "training_data['TOTAL_INSTALLMENT_AMOUNT'] = training_data['TOTAL_INSTALLMENT_AMOUNT_1'] + training_data['TOTAL_INSTALLMENT_AMOUNT_2']\n",
    "# training_data['AVERAGE_LOAN_AMOUNT'] = training_data['TOTAL_LOAN_AMOUNT'] / training_data['BORROWER_LOAN_COUNT'] noo good\n",
    "# training_data['AVERAGE_LOAN_AMOUNT'] = training_data['TOTAL_LOAN_AMOUNT'] / training_data['BORROWER_LOAN_COUNT']\n",
    "# training_data['AVERAGE_LOAN_TERM'] = training_data.groupby('BORROWER_ID')['CONTRACT_MATURITY_DATE'].transform('mean')\n",
    "# training_data['AVERAGE_INTEREST_RATE'] = training_data.groupby('BORROWER_ID')['CONTRACT_INTEREST_RATE'].transform('mean')\n",
    "# training_data['MAX_DEBT_TO_INCOME'] = training_data.groupby('BORROWER_ID')['CONTRACT_DEPT_SERVICE_TO_INCOME'].transform('max')\n",
    "# training_data['HAS_MULTIPLE_LOAN_TYPES'] = training_data.groupby('BORROWER_ID')['CONTRACT_LOAN_TYPE'].transform('nunique').apply(lambda x: 1 if x > 1 else 0)\n",
    "# training_data['HAS_REFINANCED'] = training_data.groupby('BORROWER_ID')['CONTRACT_REFINANCED'].transform('max')\n",
    "# Step 1: Identify duplicated rows based on CONTRACT_ID\n",
    "\n",
    "# Count the occurrences of each CONTRACT_ID\n",
    "contract_id_counts = training_data.groupby('CONTRACT_ID')['CONTRACT_ID'].transform('size')\n",
    "\n",
    "# Create a new column 'JOINT_ACCOUNT' with the number of occurrences of each CONTRACT_ID\n",
    "training_data['JOINT_ACCOUNT'] = contract_id_counts\n",
    "\n",
    "# Now 'JOINT_ACCOUNT' contains the count of each 'CONTRACT_ID' occurrences\n",
    "\n",
    "# # # training_data['RELATIVE_LOAN_AMOUNT'] = training_data['CONTRACT_LOAN_AMOUNT'] / training_data['TOTAL_LOAN_AMOUNT']\n",
    "# training_data['IS_FIXED_RATE'] = training_data['CONTRACT_LOAN_CONTRACT_TYPE'].apply(lambda x: 1 if x == 'fixed' else 0)\n",
    "# # Ez még nem tudom hogyan de hasznos lehet \n",
    "# # Assuming CONTRACT_MATURITY_DATE is a datetime object and 'current_date' is today's date\n",
    "# # training_data['DAYS_TO_MATURITY'] = (training_data['CONTRACT_MATURITY_DATE'] - current_date).dt.days\n",
    "X_columns.extend(['BORROWER_LOAN_COUNT', 'TOTAL_LOAN_AMOUNT','TOTAL_INSTALLMENT_AMOUNT','JOINT_ACCOUNT'])\n",
    "lognormal_variables.extend([ 'TOTAL_LOAN_AMOUNT','TOTAL_INSTALLMENT_AMOUNT',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables removed: {'BORROWER_COUNTRY', 'FREQ_TYPE_2f88e16c', 'CONTRACT_REFINANCED'}\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.85  # Set your own threshold\n",
    "correlation_matrix = training_data[X_columns].corr()\n",
    "# Get pairs of highly correlated features\n",
    "highly_correlated_set = set()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            highly_correlated_set.add(colname)\n",
    "\n",
    "# Remove highly correlated features from X_columns\n",
    "X_columns = [col for col in X_columns if col not in highly_correlated_set]\n",
    "print('Variables removed:', highly_correlated_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, log_loss, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def filter_2016(df1, df2, variables):\n",
    "    # Create deep copies to avoid modifying original data\n",
    "    df1_copy = df1.copy()\n",
    "    df2_copy = df2.copy()\n",
    "\n",
    "    # Convert to datetime format if not already\n",
    "    df1_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'] = pd.to_datetime(df1_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'])\n",
    "    df2_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'] = pd.to_datetime(df2_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'])\n",
    "    \n",
    "    # Filter data where CONTRACT_DATE_OF_LOAN_AGREEMENT is before 2016\n",
    "    df1_filtered = df1_copy[df1_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'].dt.year < 2016]\n",
    "    df2_filtered = df2_copy[df2_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'].dt.year < 2016]\n",
    "    \n",
    "    df1_filtered['CONTRACT_DATE_OF_LOAN_AGREEMENT'] = df1_filtered['CONTRACT_DATE_OF_LOAN_AGREEMENT'].apply(lambda x: x.toordinal())\n",
    "    df2_filtered['CONTRACT_DATE_OF_LOAN_AGREEMENT'] = df2_filtered['CONTRACT_DATE_OF_LOAN_AGREEMENT'].apply(lambda x: x.toordinal())\n",
    "\n",
    "    return df1_filtered, df2_filtered\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "def train_and_predict_two_halves(df, variables, target, model1=LogisticRegression(), scaler=StandardScaler()):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Log-transform specified variables, if needed\n",
    "    # Assuming lognormal_variables is a list of variables that need log transformation\n",
    "    lognormal_variables = []  # replace with your actual variables that need transformation\n",
    "    for var in lognormal_variables:\n",
    "        if var == 'CONTRACT_CREDIT_LOSS':\n",
    "            df[var] = np.log1p(np.abs(df[var]))*np.sign(df[var])\n",
    "        else:\n",
    "            df[var] = np.log1p(df[var])\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[variables], df[target], test_size=0.5, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model1.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict probabilities on the test data\n",
    "    y_test_proba = model1.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate log loss on the test set\n",
    "    test_log_loss = log_loss(y_test, y_test_proba)\n",
    "    print(\"Log Loss on test set:\", test_log_loss)\n",
    "    \n",
    "    # Return probabilities for the entire dataset for consistency with your original code\n",
    "    # You should only do this if you need these probabilities for further analysis\n",
    "    # Be cautious about using these probabilities for model evaluation, as they come from the model fitted on the entire dataset\n",
    "    X_scaled = scaler.fit_transform(df[variables])\n",
    "    model1.fit(X_scaled, df[target])\n",
    "    all_proba = model1.predict_proba(X_scaled)[:, 1]\n",
    "    \n",
    "    return all_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "def significant_features(df, variables, target, model1=LogisticRegression(), scaler=StandardScaler()):\n",
    "    if lognormal_variables is not None:\n",
    "        df = df.copy()\n",
    "        \n",
    "        for var in lognormal_variables:\n",
    "            if var == 'CONTRACT_CREDIT_LOSS':\n",
    "                df[var] = np.log1p(np.abs(df[var]))*np.sign(df[var])\n",
    "            else:\n",
    "                df[var] = np.log1p(df[var])\n",
    "\n",
    "    X_scaled = scaler.fit_transform(df[variables])\n",
    "\n",
    "    X = sm.add_constant(pd.DataFrame(X_scaled, columns=variables))\n",
    "    y = df[target]\n",
    "\n",
    "    # Fit the logistic regression model\n",
    "    model = sm.Logit(y, X).fit()\n",
    "\n",
    "    # Display the summary\n",
    "    print(model.summary())\n",
    "    # Get the p-values\n",
    "    p_values = model.pvalues\n",
    "\n",
    "    # Identify the non-significant variables\n",
    "    non_significant_vars = p_values[p_values > 0.05].index.tolist()\n",
    "\n",
    "    return model, non_significant_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def cut_exponential_tails(df, target):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Step 1: Estimate lambda for each row based on its 'target' value and 'ADJUSTED_TIME_TO_MATURITY'\n",
    "    df['LAMBDA_ESTIMATE'] = -np.log(1 - df[target])/730\n",
    "\n",
    "    # Step 2: Calculate new probabilities p_exp for each row based on its own lambda_estimate\n",
    "    df[target] = 1 - np.exp(-df['LAMBDA_ESTIMATE'] * df['ADJUSTED_TIME_TO_MATURITY'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def combined_probability(s):\n",
    "    return 1 - np.prod(1 - s.values)\n",
    "\n",
    "def create_submission_file(df_preds, target, example, filename='submission.csv', testing=False):\n",
    "    # Filter the data to only include BORROWER_IDs that are in the submission example\n",
    "    filtered_training_data = df_preds[df_preds['BORROWER_ID'].isin(example['BORROWER_ID'])]\n",
    "\n",
    "    # Print warning if the row count is off\n",
    "    if not testing and len(filtered_training_data) != 1564601:\n",
    "        print('WARNING: The filtered data does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "        # raise ValueError('WARNING: The submission file does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "    # Group by BORROWER_ID and calculate the combined probability\n",
    "\n",
    "    #######################x########################\n",
    "    #CUTTING TAILS DID NOT SEEM TO WORK\n",
    "    #######################x########################\n",
    "    # filtered_training_data = cut_exponential_tails(filtered_training_data, target)\n",
    "    grouped_data = filtered_training_data.groupby('BORROWER_ID')[target].apply(combined_probability).reset_index()\n",
    "\n",
    "    # Create the submission DataFrame\n",
    "    df_submission = pd.DataFrame()\n",
    "    df_submission['BORROWER_ID'] = grouped_data['BORROWER_ID']\n",
    "    df_submission['PRED'] = grouped_data[target]\n",
    "    print('Centering probabilities...')\n",
    "    # Center the probabilities around 1.48%\n",
    "    # desired_mean = 0.0148  # 1.48% as a decimal\n",
    "    desired_mean = 0.014477  # 1.48% as a decimal\n",
    "    while (df_submission['PRED'].max() > 1 or df_submission['PRED'].min() < 0 or abs(df_submission['PRED'].mean() -0.0148) > 0.0005):\n",
    "        # print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "        df_submission['PRED'] = df_submission['PRED'].clip(lower=0, upper=1)\n",
    "        # print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "\n",
    "        current_mean = df_submission['PRED'].mean()\n",
    "        adjustment_factor = desired_mean  - current_mean\n",
    "        df_submission['PRED'] += adjustment_factor\n",
    "    print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "    # Save the submission file\n",
    "    if  not testing and filename is not None:\n",
    "        df_submission.to_csv(filename, index=False)\n",
    "    print(f'Saved file: {filename}')\n",
    "    # if abs(df_submission['PRED'].mean() -0.0148) > 0.0005:\n",
    "    #    raise ValueError('WARNING: mean is bad')\n",
    "        \n",
    "    # Print warning if the row count is off\n",
    "    if not testing and len(df_submission) != 1117674:\n",
    "        print('WARNING: The submission file does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "        # raise ValueError('WARNING: The submission file does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "        \n",
    "    return df_submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_2y_1y(df, variables, target, model1=LogisticRegression(), model2=LogisticRegression()):\n",
    "    df = df.copy()\n",
    "    start_date = pd.Timestamp('2015-01-01')\n",
    "    end_date = pd.Timestamp('2017-01-01')\n",
    "\n",
    "    # Mask for rows with CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN between start_date and end_date\n",
    "    mask_date_range = (df['CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN'] >= start_date) & (df['CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN'] <= end_date)\n",
    "\n",
    "    df = df[mask_date_range]\n",
    "\n",
    "    probs = train_and_predict_two_halves(\n",
    "        df, \n",
    "        variables, \n",
    "        target, \n",
    "        model1=model1,\n",
    "        model2=model2,\n",
    "    )\n",
    "    df['2Y-1Y-PROBS'] = probs\n",
    "    \n",
    "    test_data = pd.read_csv('./data/1y-test.csv')\n",
    "\n",
    "\n",
    "    df_submission = create_submission_file(df, '2Y-1Y-PROBS', test_data, filename=None, testing=True)\n",
    "\n",
    "    merged_df = pd.merge(test_data, df_submission, on='BORROWER_ID')\n",
    "    true_labels = merged_df['TARGET_EVENT_BINARY_1Y']\n",
    "    predicted_probs = merged_df['PRED']\n",
    "    logloss = log_loss(true_labels, predicted_probs)\n",
    "\n",
    "    print(f'Log loss: {logloss}')\n",
    "        \n",
    "    return logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.016240\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Logit Regression Results                             \n",
      "==================================================================================\n",
      "Dep. Variable:     TARGET_EVENT_BINARY_2Y   No. Observations:              1601663\n",
      "Model:                              Logit   Df Residuals:                  1601613\n",
      "Method:                               MLE   Df Model:                           49\n",
      "Date:                    Thu, 02 Nov 2023   Pseudo R-squ.:                  0.6004\n",
      "Time:                            14:24:20   Log-Likelihood:                -26011.\n",
      "converged:                          False   LL-Null:                       -65100.\n",
      "Covariance Type:                nonrobust   LLR p-value:                     0.000\n",
      "=======================================================================================================\n",
      "                                          coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "const                                 -10.2434      8.694     -1.178      0.239     -27.283       6.797\n",
      "CONTRACT_CREDIT_INTERMEDIARY            0.0017      0.004      0.408      0.684      -0.006       0.010\n",
      "CONTRACT_CREDIT_LOSS                    4.7373      0.051     93.569      0.000       4.638       4.837\n",
      "CONTRACT_CURRENCY                      -0.0086      0.003     -3.171      0.002      -0.014      -0.003\n",
      "CONTRACT_DEPT_SERVICE_TO_INCOME        -0.0863      0.026     -3.287      0.001      -0.138      -0.035\n",
      "CONTRACT_INCOME                         0.2542      0.026      9.718      0.000       0.203       0.305\n",
      "CONTRACT_INSTALMENT_AMOUNT              0.2449      0.012     20.568      0.000       0.222       0.268\n",
      "CONTRACT_INSTALMENT_AMOUNT_2           -0.1001      0.021     -4.714      0.000      -0.142      -0.058\n",
      "CONTRACT_INTEREST_PERIOD               -0.1076      0.024     -4.515      0.000      -0.154      -0.061\n",
      "CONTRACT_INTEREST_RATE                  0.2717      0.018     15.456      0.000       0.237       0.306\n",
      "CONTRACT_LGD                           -0.5262      0.031    -17.078      0.000      -0.587      -0.466\n",
      "CONTRACT_LOAN_AMOUNT                   -0.0850      0.039     -2.201      0.028      -0.161      -0.009\n",
      "CONTRACT_LOAN_CONTRACT_TYPE             0.7916      0.056     14.220      0.000       0.682       0.901\n",
      "CONTRACT_LOAN_TO_VALUE_RATIO           -0.6006      0.038    -15.852      0.000      -0.675      -0.526\n",
      "CONTRACT_MARKET_VALUE                   0.4064      0.084      4.821      0.000       0.241       0.572\n",
      "CONTRACT_MORTGAGE_LENDING_VALUE        -1.1081      0.079    -14.009      0.000      -1.263      -0.953\n",
      "CONTRACT_MORTGAGE_TYPE                  0.1045      0.028      3.797      0.000       0.051       0.158\n",
      "CONTRACT_RISK_WEIGHTED_ASSETS           0.0309      0.005      5.980      0.000       0.021       0.041\n",
      "CONTRACT_TYPE_OF_INTEREST_REPAYMENT    -0.5837      0.013    -45.459      0.000      -0.609      -0.559\n",
      "BORROWER_BIRTH_YEAR                    -0.2268      0.037     -6.058      0.000      -0.300      -0.153\n",
      "BORROWER_CITIZENSHIP                    0.0299      0.008      3.556      0.000       0.013       0.046\n",
      "BORROWER_COUNTY                        -0.0110      0.013     -0.861      0.389      -0.036       0.014\n",
      "BORROWER_TYPE_OF_SETTLEMENT             0.0432      0.015      2.893      0.004       0.014       0.072\n",
      "LOAN_TYPE_1f951336                      4.4677    337.178      0.013      0.989    -656.390     665.325\n",
      "LOAN_TYPE_2f88e16c                      2.1384    210.581      0.010      0.992    -410.593     414.869\n",
      "LOAN_TYPE_47693941                     -0.0316    149.616     -0.000      1.000    -293.274     293.210\n",
      "LOAN_TYPE_5a06241e                      2.0334    128.720      0.016      0.987    -250.253     254.320\n",
      "LOAN_TYPE_694cbaee                      0.8363     52.783      0.016      0.987    -102.616     104.288\n",
      "LOAN_TYPE_69f70539                      2.9656    228.175      0.013      0.990    -444.249     450.180\n",
      "LOAN_TYPE_7e2065f4                      3.4994    257.858      0.014      0.989    -501.892     508.891\n",
      "LOAN_TYPE_83910425                      1.1635     74.170      0.016      0.987    -144.208     146.535\n",
      "LOAN_TYPE_8fe006f1                     -0.0020   4779.627   -4.2e-07      1.000   -9367.899    9367.895\n",
      "LOAN_TYPE_955ae3ef                      2.8198    209.511      0.013      0.989    -407.815     413.455\n",
      "LOAN_TYPE_95c4f8fb                      0.3008     21.348      0.014      0.989     -41.541      42.142\n",
      "LOAN_TYPE_b503a0de                      7.0892    546.163      0.013      0.990   -1063.370    1077.548\n",
      "LOAN_TYPE_cde77491                     -0.0244    634.590  -3.84e-05      1.000   -1243.799    1243.750\n",
      "LOAN_TYPE_cf07c2dd                      2.3344    168.382      0.014      0.989    -327.689     332.358\n",
      "LOAN_TYPE_d3aaffde                      3.9981    261.685      0.015      0.988    -508.895     516.891\n",
      "LOAN_TYPE_eab72d7a                      0.6068     42.915      0.014      0.989     -83.505      84.719\n",
      "LOAN_TYPE_f792971b                     -0.0103     94.358     -0.000      1.000    -184.949     184.929\n",
      "FREQ_TYPE_3265c5b7                     -0.3260      0.136     -2.400      0.016      -0.592      -0.060\n",
      "FREQ_TYPE_479a2e13                     -0.9354      0.350     -2.671      0.008      -1.622      -0.249\n",
      "FREQ_TYPE_87db11f5                     -0.8272      0.222     -3.727      0.000      -1.262      -0.392\n",
      "FREQ_TYPE_89efd382                     -0.0322      0.021     -1.530      0.126      -0.073       0.009\n",
      "FREQ_TYPE_ad534644                     -0.1019      0.042     -2.438      0.015      -0.184      -0.020\n",
      "FREQ_TYPE_bd092d5a                  -3.846e-06      0.004     -0.001      0.999      -0.008       0.008\n",
      "BORROWER_LOAN_COUNT                    -0.1241      0.020     -6.320      0.000      -0.163      -0.086\n",
      "TOTAL_LOAN_AMOUNT                      -0.4433      0.039    -11.345      0.000      -0.520      -0.367\n",
      "TOTAL_INSTALLMENT_AMOUNT                0.1069      0.017      6.259      0.000       0.073       0.140\n",
      "JOINT_ACCOUNT                           0.0716      0.022      3.317      0.001       0.029       0.114\n",
      "=======================================================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.48 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "model, non_significant_vars = significant_features(training_data, X_columns, y_column,LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['const',\n",
       " 'CONTRACT_CREDIT_INTERMEDIARY',\n",
       " 'BORROWER_COUNTY',\n",
       " 'LOAN_TYPE_1f951336',\n",
       " 'LOAN_TYPE_2f88e16c',\n",
       " 'LOAN_TYPE_47693941',\n",
       " 'LOAN_TYPE_5a06241e',\n",
       " 'LOAN_TYPE_694cbaee',\n",
       " 'LOAN_TYPE_69f70539',\n",
       " 'LOAN_TYPE_7e2065f4',\n",
       " 'LOAN_TYPE_83910425',\n",
       " 'LOAN_TYPE_8fe006f1',\n",
       " 'LOAN_TYPE_955ae3ef',\n",
       " 'LOAN_TYPE_95c4f8fb',\n",
       " 'LOAN_TYPE_b503a0de',\n",
       " 'LOAN_TYPE_cde77491',\n",
       " 'LOAN_TYPE_cf07c2dd',\n",
       " 'LOAN_TYPE_d3aaffde',\n",
       " 'LOAN_TYPE_eab72d7a',\n",
       " 'LOAN_TYPE_f792971b',\n",
       " 'FREQ_TYPE_89efd382',\n",
       " 'FREQ_TYPE_bd092d5a']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_significant_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_columns_significant = [col for col in X_columns if col not in non_significant_vars]\n",
    "# X_columns_significant.append('CONTRACT_INSTALMENT_AMOUNT_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns.remove('CONTRACT_CREDIT_INTERMEDIARY')\n",
    "X_columns.remove('BORROWER_COUNTY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss on test set: 0.021630749460802286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Unique: Log Loss on test set: 0.02457979254826265\n",
    "# Non-unique: Log Loss on test set: 0.021630749460802286\n",
    "# 0.015985249571782673\n",
    "\n",
    "training_data_unique = training_data.drop_duplicates(subset='CONTRACT_ID')\n",
    "predicted_probs = 'LOGISTIC_REG'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    y_column, \n",
    "    model1=LogisticRegression(max_iter=400, C=0.5, random_state=42),\n",
    ")\n",
    "\n",
    "# training_data_unique[predicted_probs] = probs\n",
    "# probs_df = pd.DataFrame({\n",
    "#     'CONTRACT_ID': training_data_unique['CONTRACT_ID'],\n",
    "#      predicted_probs: probs\n",
    "# })\n",
    "# training_data = training_data.merge(probs_df, on='CONTRACT_ID', how='left')\n",
    "training_data[predicted_probs] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(x):\n",
    "    if x<1.953286e-06:\n",
    "        return 10**(-10)\n",
    "    else:\n",
    "        return x\n",
    "training_data['CLIPPED'] = training_data[predicted_probs].apply(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTRACT_ID</th>\n",
       "      <th>BORROWER_ID</th>\n",
       "      <th>CONTRACT_BANK_ID</th>\n",
       "      <th>CONTRACT_CREDIT_INTERMEDIARY</th>\n",
       "      <th>CONTRACT_CREDIT_LOSS</th>\n",
       "      <th>CONTRACT_CURRENCY</th>\n",
       "      <th>CONTRACT_DATE_OF_LOAN_AGREEMENT</th>\n",
       "      <th>CONTRACT_DEPT_SERVICE_TO_INCOME</th>\n",
       "      <th>CONTRACT_FREQUENCY_TYPE</th>\n",
       "      <th>CONTRACT_INCOME</th>\n",
       "      <th>...</th>\n",
       "      <th>FREQ_TYPE_ad534644</th>\n",
       "      <th>FREQ_TYPE_bd092d5a</th>\n",
       "      <th>BORROWER_LOAN_COUNT</th>\n",
       "      <th>TOTAL_LOAN_AMOUNT</th>\n",
       "      <th>TOTAL_INSTALLMENT_AMOUNT_1</th>\n",
       "      <th>TOTAL_INSTALLMENT_AMOUNT_2</th>\n",
       "      <th>TOTAL_INSTALLMENT_AMOUNT</th>\n",
       "      <th>JOINT_ACCOUNT</th>\n",
       "      <th>LOGISTIC_REG</th>\n",
       "      <th>CLIPPED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MDE1zNJ</td>\n",
       "      <td>PRF52C7u</td>\n",
       "      <td>caa130b5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>359398.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457709</td>\n",
       "      <td>54.85</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>372461.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>3319933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80717.0</td>\n",
       "      <td>80717.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1rruIo4</td>\n",
       "      <td>MIYaoPq8</td>\n",
       "      <td>f789f8b0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3937.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457904</td>\n",
       "      <td>29.47</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>223276.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2533432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20311.0</td>\n",
       "      <td>20311.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xhdrsu5x</td>\n",
       "      <td>0xFcqzYv</td>\n",
       "      <td>c2a40996</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18785.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457282</td>\n",
       "      <td>81.65</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>231007.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>7024876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98662.0</td>\n",
       "      <td>98662.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LrUfGq+</td>\n",
       "      <td>Y551bMvC</td>\n",
       "      <td>bd940aa2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3848.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457553</td>\n",
       "      <td>8.33</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>661272.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>7998750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35616.0</td>\n",
       "      <td>35616.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fR2gPfST</td>\n",
       "      <td>A5NUFs2z</td>\n",
       "      <td>1d42bbf5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457406</td>\n",
       "      <td>0.00</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>120723</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12080.0</td>\n",
       "      <td>12080.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.001811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601658</th>\n",
       "      <td>XladPB3d</td>\n",
       "      <td>0sMkSY/g</td>\n",
       "      <td>1d42bbf5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100075.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2458012</td>\n",
       "      <td>30.46</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>507273.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5990964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32582.0</td>\n",
       "      <td>32582.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601659</th>\n",
       "      <td>q6UqBBI</td>\n",
       "      <td>ww9/NOGp</td>\n",
       "      <td>f789f8b0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14888.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457515</td>\n",
       "      <td>27.75</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>216392.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1420072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20080.0</td>\n",
       "      <td>20080.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601660</th>\n",
       "      <td>hIBR3bvU</td>\n",
       "      <td>MnSjeYaF</td>\n",
       "      <td>533b7c4a</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457680</td>\n",
       "      <td>0.00</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2453666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14947.0</td>\n",
       "      <td>14947.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.000596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601661</th>\n",
       "      <td>nUmU4XLo</td>\n",
       "      <td>9cpBKGZ0</td>\n",
       "      <td>caa130b5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457345</td>\n",
       "      <td>10.66</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>173130.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>272804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17001.0</td>\n",
       "      <td>17001.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.001163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601662</th>\n",
       "      <td>muTG8saz</td>\n",
       "      <td>TPKz6u6z</td>\n",
       "      <td>1d42bbf5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12851.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457948</td>\n",
       "      <td>50.13</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>115434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1107035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48694.0</td>\n",
       "      <td>48694.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.000684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1601663 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CONTRACT_ID BORROWER_ID CONTRACT_BANK_ID  \\\n",
       "0           MDE1zNJ    PRF52C7u         caa130b5   \n",
       "1           1rruIo4    MIYaoPq8         f789f8b0   \n",
       "2          xhdrsu5x    0xFcqzYv         c2a40996   \n",
       "3           LrUfGq+    Y551bMvC         bd940aa2   \n",
       "4          fR2gPfST    A5NUFs2z         1d42bbf5   \n",
       "...             ...         ...              ...   \n",
       "1601658    XladPB3d    0sMkSY/g         1d42bbf5   \n",
       "1601659     q6UqBBI    ww9/NOGp         f789f8b0   \n",
       "1601660    hIBR3bvU    MnSjeYaF         533b7c4a   \n",
       "1601661    nUmU4XLo    9cpBKGZ0         caa130b5   \n",
       "1601662    muTG8saz    TPKz6u6z         1d42bbf5   \n",
       "\n",
       "         CONTRACT_CREDIT_INTERMEDIARY  CONTRACT_CREDIT_LOSS  \\\n",
       "0                                 2.0              359398.0   \n",
       "1                                 2.0                3937.0   \n",
       "2                                 2.0               18785.0   \n",
       "3                                 1.0                3848.0   \n",
       "4                                 2.0                   0.0   \n",
       "...                               ...                   ...   \n",
       "1601658                           1.0              100075.0   \n",
       "1601659                           2.0               14888.0   \n",
       "1601660                           2.0                   0.0   \n",
       "1601661                           1.0                   0.0   \n",
       "1601662                           2.0               12851.0   \n",
       "\n",
       "         CONTRACT_CURRENCY  CONTRACT_DATE_OF_LOAN_AGREEMENT  \\\n",
       "0                       31                          2457709   \n",
       "1                       31                          2457904   \n",
       "2                       31                          2457282   \n",
       "3                       31                          2457553   \n",
       "4                       31                          2457406   \n",
       "...                    ...                              ...   \n",
       "1601658                 31                          2458012   \n",
       "1601659                 31                          2457515   \n",
       "1601660                 31                          2457680   \n",
       "1601661                 31                          2457345   \n",
       "1601662                 31                          2457948   \n",
       "\n",
       "         CONTRACT_DEPT_SERVICE_TO_INCOME CONTRACT_FREQUENCY_TYPE  \\\n",
       "0                                  54.85                479a2e13   \n",
       "1                                  29.47                479a2e13   \n",
       "2                                  81.65                479a2e13   \n",
       "3                                   8.33                479a2e13   \n",
       "4                                   0.00                479a2e13   \n",
       "...                                  ...                     ...   \n",
       "1601658                            30.46                479a2e13   \n",
       "1601659                            27.75                479a2e13   \n",
       "1601660                             0.00                479a2e13   \n",
       "1601661                            10.66                479a2e13   \n",
       "1601662                            50.13                479a2e13   \n",
       "\n",
       "         CONTRACT_INCOME  ...  FREQ_TYPE_ad534644  FREQ_TYPE_bd092d5a  \\\n",
       "0               372461.0  ...               False               False   \n",
       "1               223276.0  ...               False               False   \n",
       "2               231007.0  ...               False               False   \n",
       "3               661272.0  ...               False               False   \n",
       "4                    0.0  ...               False               False   \n",
       "...                  ...  ...                 ...                 ...   \n",
       "1601658         507273.0  ...               False               False   \n",
       "1601659         216392.0  ...               False               False   \n",
       "1601660              0.0  ...               False               False   \n",
       "1601661         173130.0  ...               False               False   \n",
       "1601662         115434.0  ...               False               False   \n",
       "\n",
       "         BORROWER_LOAN_COUNT  TOTAL_LOAN_AMOUNT  TOTAL_INSTALLMENT_AMOUNT_1  \\\n",
       "0                          2            3319933                         0.0   \n",
       "1                          1            2533432                         0.0   \n",
       "2                          2            7024876                         0.0   \n",
       "3                          1            7998750                         0.0   \n",
       "4                          1             120723                         0.0   \n",
       "...                      ...                ...                         ...   \n",
       "1601658                    1            5990964                         0.0   \n",
       "1601659                    2            1420072                         0.0   \n",
       "1601660                    2            2453666                         0.0   \n",
       "1601661                    1             272804                         0.0   \n",
       "1601662                    2            1107035                         0.0   \n",
       "\n",
       "         TOTAL_INSTALLMENT_AMOUNT_2  TOTAL_INSTALLMENT_AMOUNT  JOINT_ACCOUNT  \\\n",
       "0                           80717.0                   80717.0              3   \n",
       "1                           20311.0                   20311.0              2   \n",
       "2                           98662.0                   98662.0              4   \n",
       "3                           35616.0                   35616.0              3   \n",
       "4                           12080.0                   12080.0              1   \n",
       "...                             ...                       ...            ...   \n",
       "1601658                     32582.0                   32582.0              2   \n",
       "1601659                     20080.0                   20080.0              1   \n",
       "1601660                     14947.0                   14947.0              1   \n",
       "1601661                     17001.0                   17001.0              1   \n",
       "1601662                     48694.0                   48694.0              1   \n",
       "\n",
       "        LOGISTIC_REG   CLIPPED  \n",
       "0           0.000507  0.000507  \n",
       "1           0.000204  0.000204  \n",
       "2           0.000218  0.000218  \n",
       "3           0.000229  0.000229  \n",
       "4           0.001811  0.001811  \n",
       "...              ...       ...  \n",
       "1601658     0.000267  0.000267  \n",
       "1601659     0.000685  0.000685  \n",
       "1601660     0.000596  0.000596  \n",
       "1601661     0.001163  0.001163  \n",
       "1601662     0.000684  0.000684  \n",
       "\n",
       "[1601663 rows x 71 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centering probabilities...\n",
      "1.0 0.0073470474013857655 0.014476999999999993\n",
      "Saved file: ./predictions/logistic-regression-one-model-clipped-0.0148-predicting-for-one-acc.csv\n"
     ]
    }
   ],
   "source": [
    "submission = create_submission_file(training_data, predicted_probs, data_submission_example, filename='./predictions/logistic-regression-one-model-clipped-0.0148-predicting-for-one-acc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.601663e+06\n",
       "mean     6.786570e-03\n",
       "std      3.931776e-02\n",
       "min      0.000000e+00\n",
       "25%      3.544936e-04\n",
       "50%      8.418085e-04\n",
       "75%      1.663326e-03\n",
       "max      1.000000e+00\n",
       "Name: LOGISTIC_REG, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[predicted_probs].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.117674e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.447700e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.788298e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.347047e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.954259e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.531692e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.686392e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               PRED\n",
       "count  1.117674e+06\n",
       "mean   1.447700e-02\n",
       "std    3.788298e-02\n",
       "min    7.347047e-03\n",
       "25%    7.954259e-03\n",
       "50%    8.531692e-03\n",
       "75%    9.686392e-03\n",
       "max    1.000000e+00"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "predicted_probs = 'RANDOM_FOREST'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    y_column, \n",
    "    model1=RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42),\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "submission = create_submission_file(training_data, predicted_probs,data_submission_example, filename='./predictions/random-forrest-2y-exp-no-multicolinearity-lognormal-time-no-dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.601663e+06\n",
       "mean     6.786637e-03\n",
       "std      2.867128e-02\n",
       "min      7.236204e-04\n",
       "25%      1.237809e-03\n",
       "50%      1.550852e-03\n",
       "75%      2.272555e-03\n",
       "max      4.370051e-01\n",
       "Name: RANDOM_FOREST, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[predicted_probs].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probs = 'GRADIENT_BOOSTING_CLASSIFIER'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    y_column, \n",
    "    model1=GradientBoostingClassifier(random_state=42),\n",
    "    model2=GradientBoostingClassifier(random_state=42),\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "submission = create_submission_file(training_data, predicted_probs,data_submission_example, filename='./predictions/gbc-2y-exp-no-multicolinearity-lognormal-time-no-dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Model 1 on df2:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    795431\n",
      "           1       0.77      0.67      0.72      5401\n",
      "\n",
      "    accuracy                           1.00    800832\n",
      "   macro avg       0.88      0.83      0.86    800832\n",
      "weighted avg       1.00      1.00      1.00    800832\n",
      "\n",
      "Log Loss:\n",
      "0.008920031130332217\n",
      "Confusion Matrix:\n",
      "[[794339   1092]\n",
      " [  1781   3620]]\n",
      "Evaluation of Model 2 on df1:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    795363\n",
      "           1       0.76      0.65      0.70      5468\n",
      "\n",
      "    accuracy                           1.00    800831\n",
      "   macro avg       0.88      0.83      0.85    800831\n",
      "weighted avg       1.00      1.00      1.00    800831\n",
      "\n",
      "Log Loss:\n",
      "0.00899427534801325\n",
      "Confusion Matrix:\n",
      "[[794243   1120]\n",
      " [  1894   3574]]\n",
      "Centering probabilities...\n",
      "0.9933470937188168 0.011253558355142607 0.01479999999999997\n",
      "Saved file: ./predictions/xgboost-max_depth=3-2y-exp-no-multicolinearity-lognormal-time-no-dates.csv\n"
     ]
    }
   ],
   "source": [
    "predicted_probs = 'XGBOOST'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    y_column, \n",
    "    model1= xgb.XGBClassifier(max_depth=3, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    model2= xgb.XGBClassifier(max_depth=3, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "submission = create_submission_file(training_data, predicted_probs, data_submission_example, filename='./predictions/xgboost-max_depth=3-2y-exp-no-multicolinearity-lognormal-time-no-dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Model 1 on df2:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    509478\n",
      "           1       0.60      0.39      0.47      5071\n",
      "\n",
      "    accuracy                           0.99    514549\n",
      "   macro avg       0.79      0.69      0.73    514549\n",
      "weighted avg       0.99      0.99      0.99    514549\n",
      "\n",
      "Log Loss:\n",
      "0.020154686217490652\n",
      "Confusion Matrix:\n",
      "[[508136   1342]\n",
      " [  3093   1978]]\n",
      "Evaluation of Model 2 on df1:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    509480\n",
      "           1       0.60      0.40      0.48      5069\n",
      "\n",
      "    accuracy                           0.99    514549\n",
      "   macro avg       0.80      0.70      0.74    514549\n",
      "weighted avg       0.99      0.99      0.99    514549\n",
      "\n",
      "Log Loss:\n",
      "0.02041090972095113\n",
      "Confusion Matrix:\n",
      "[[508121   1359]\n",
      " [  3032   2037]]\n",
      "Centering probabilities...\n",
      "1.0 0.0027108450789681583 0.014800000000000004\n",
      "Saved file: None\n",
      "Log loss: 0.02449711803718221\n",
      "LogisticRegression(max_iter=400, penalty=None, random_state=42)\n",
      "Logloss is  0.02449711803718221\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Model 1 on df2:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    509478\n",
      "           1       0.60      0.39      0.47      5071\n",
      "\n",
      "    accuracy                           0.99    514549\n",
      "   macro avg       0.79      0.69      0.73    514549\n",
      "weighted avg       0.99      0.99      0.99    514549\n",
      "\n",
      "Log Loss:\n",
      "0.020154686217490652\n",
      "Confusion Matrix:\n",
      "[[508136   1342]\n",
      " [  3093   1978]]\n",
      "Evaluation of Model 2 on df1:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    509480\n",
      "           1       0.60      0.40      0.48      5069\n",
      "\n",
      "    accuracy                           0.99    514549\n",
      "   macro avg       0.80      0.70      0.74    514549\n",
      "weighted avg       0.99      0.99      0.99    514549\n",
      "\n",
      "Log Loss:\n",
      "0.02041090972095113\n",
      "Confusion Matrix:\n",
      "[[508121   1359]\n",
      " [  3032   2037]]\n",
      "Centering probabilities...\n",
      "1.0 0.0027108450789681583 0.014800000000000004\n",
      "Saved file: None\n",
      "Log loss: 0.02449711803718221\n",
      "LogisticRegression(C=0.5, max_iter=400, penalty=None, random_state=42)\n",
      "Logloss is  0.02449711803718221\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "models =  [\n",
    "    (LogisticRegression(max_iter=400, penalty=None, C=1.0, random_state=42),\n",
    "     LogisticRegression(max_iter=400, penalty=None, C=1.0, random_state=42)),\n",
    "    \n",
    "   (LogisticRegression(max_iter=400, penalty=None, C=0.5, random_state=42),\n",
    "     LogisticRegression(max_iter=400, penalty=None, C=0.5, random_state=42)),\n",
    "]\n",
    "\n",
    "best_logloss = 100\n",
    "best_models = None\n",
    "for model in models:\n",
    "\n",
    "    logloss = test_model_2y_1y(\n",
    "        training_data, \n",
    "        X_columns, \n",
    "        'TARGET_EVENT_BINARY_1Y',\n",
    "        model1=model[0],\n",
    "        model2=model[1],\n",
    "    )\n",
    "    if logloss < best_logloss:\n",
    "        best_logloss = logloss\n",
    "        best_models = model\n",
    "    print(model[0])\n",
    "    print('Logloss is ', logloss)\n",
    "    print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_by_size = {}\n",
    "\n",
    "# Initialize a dictionary to hold covariance matrices by group size\n",
    "cov_matrices_by_size = {}\n",
    "\n",
    "# Group by 'BORROWER_ID' and iterate through the first 100 groups\n",
    "for idx, (name, group) in enumerate(training_data.groupby('BORROWER_ID')):\n",
    "    if idx % 100 == 0: \n",
    "        print('At index', idx)\n",
    "        \n",
    "    if idx == 50000:\n",
    "        break\n",
    "    \n",
    "    # Sort the group by CONTRACT_DATE_OF_LOAN_AGREEMENT\n",
    "    group = group.sort_values(by='CONTRACT_DATE_OF_LOAN_AGREEMENT')\n",
    "    \n",
    "    group_size = len(group)\n",
    "    if group_size not in groups_by_size:\n",
    "        groups_by_size[group_size] = []\n",
    "        \n",
    "    groups_by_size[group_size].append(group)\n",
    "\n",
    "# Initialize a new dictionary to hold the merged DataFrames by size\n",
    "merged_groups_by_size = {}\n",
    "\n",
    "for size, dfs in groups_by_size.items():\n",
    "    # Merge all DataFrames of the same size into a single DataFrame\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    # Store the merged DataFrame in the new dictionary\n",
    "    merged_groups_by_size[size] = merged_df\n",
    "\n",
    "# Now, merged_groups_by_size contains the merged DataFrames categorized by group size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\takat\\AppData\\Local\\Temp\\ipykernel_13332\\2391036120.py:12: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  cov_matrix = np.cov(merged_df['TARGET_EVENT_BINARY'].values.reshape(size,-1))\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2748: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2748: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Your code to generate groups_by_size and merged_groups_by_size\n",
    "\n",
    "cov_matrices_by_size = {}\n",
    "\n",
    "for size, merged_df in merged_groups_by_size.items():\n",
    "    if size > 1:  # Covariance matrix for single-element arrays doesn't make sense\n",
    "        try:\n",
    "            cov_matrix = np.cov(merged_df['TARGET_EVENT_BINARY'].values.reshape(size,-1))\n",
    "            if not np.isnan(cov_matrix).any():  # Check for NaN values\n",
    "                cov_matrices_by_size[size] = cov_matrix.tolist()  # Convert numpy array to list for JSON serialization\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while calculating the covariance matrix for size {size}: {e}\")\n",
    "\n",
    "# Ensure that the dictionary contains only JSON-serializable items\n",
    "serializable_cov_matrices_by_size = {k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in cov_matrices_by_size.items()}\n",
    "\n",
    "# Save to JSON\n",
    "with open('./data/cov_matrices_by_size.json', 'w') as f:\n",
    "    json.dump(serializable_cov_matrices_by_size, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: [[0.005868589752778426, -3.2357827075016176e-05],\n",
       "  [-3.2357827075016176e-05, 0.005451716043799604]],\n",
       " 3: [[0.007337144521720125, -2.6009019928111114e-05, -2.6009019928111046e-05],\n",
       "  [-2.6009019928111114e-05, 0.003507502116019551, -1.2385247584814735e-05],\n",
       "  [-2.6009019928111046e-05, -1.2385247584814735e-05, 0.003507502116019557]],\n",
       " 4: [[0.0023364421983921592,\n",
       "   -1.3695440787761734e-05,\n",
       "   -5.478176315104693e-06,\n",
       "   -8.217264472657053e-06],\n",
       "  [-1.3695440787761734e-05,\n",
       "   0.005820562334798763,\n",
       "   -1.3695440787761748e-05,\n",
       "   -2.054316118164268e-05],\n",
       "  [-5.478176315104693e-06,\n",
       "   -1.3695440787761748e-05,\n",
       "   0.0023364421983921558,\n",
       "   -8.217264472657026e-06],\n",
       "  [-8.217264472657053e-06,\n",
       "   -2.054316118164268e-05,\n",
       "   -8.217264472657026e-06,\n",
       "   0.003500554665351909]],\n",
       " 8: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       " 5: [[0.013199554069119256,\n",
       "   -0.00022296544035674517,\n",
       "   -0.00013377926421404707,\n",
       "   -4.4593088071349e-05,\n",
       "   -8.918617614269807e-05],\n",
       "  [-0.00022296544035674517,\n",
       "   0.016443701226309938,\n",
       "   -0.0001672240802675587,\n",
       "   -5.574136008918631e-05,\n",
       "   -0.00011148272017837272],\n",
       "  [-0.00013377926421404707,\n",
       "   -0.0001672240802675587,\n",
       "   0.009933110367893017,\n",
       "   -3.344481605351181e-05,\n",
       "   -6.688963210702361e-05],\n",
       "  [-4.4593088071349e-05,\n",
       "   -5.574136008918631e-05,\n",
       "   -3.344481605351181e-05,\n",
       "   0.003333333333333323,\n",
       "   -2.2296544035674506e-05],\n",
       "  [-8.918617614269807e-05,\n",
       "   -0.00011148272017837272,\n",
       "   -6.688963210702361e-05,\n",
       "   -2.2296544035674506e-05,\n",
       "   0.006644370122630978]],\n",
       " 6: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.020197769829581318, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       " 7: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.028571428571428577, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       " 9: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.11111111111111109, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_matrices_by_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "import json\n",
    "\n",
    "with open('cov_matrices_by_size.json', 'w') as f:\n",
    "    json.dump(cov_matrices_by_size, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
