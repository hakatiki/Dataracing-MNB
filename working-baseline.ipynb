{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni_D83AE5_uni (Baseline)\n",
    "#  {\"Logloss\": 0.04508655474248735, \"ROCAUC\": 0.9744860696967259}\n",
    "import pandas as pd\n",
    "# Importing necessary libraries for logistic regression and scaling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, log_loss, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "# Load the data\n",
    "training_data = pd.read_csv('training_data.csv')\n",
    "\n",
    "# Drop rows where BORROWER_ID is 'xNullx'\n",
    "training_data = training_data[training_data['BORROWER_ID'] != 'xNullx']\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "training_data = training_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "data_submission_example = pd.read_csv('data_submission_example.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Assuming training_data =  training_data.sample(n=10000, random_state=1)\n",
    "\n",
    "# Filling NaN values with 0\n",
    "training_data.fillna(0, inplace=True)\n",
    "\n",
    "# Converting columns to numeric where possible\n",
    "for col in training_data.columns:\n",
    "    try:\n",
    "        training_data[col] = pd.to_numeric(training_data[col], errors='ignore')\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Creating a sample target variable\n",
    "training_data['TARGET_EVENT_BINARY'] = np.where(training_data['TARGET_EVENT'] == 'K', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lognormal_variables = [\n",
    "    'CONTRACT_CREDIT_LOSS', 'CONTRACT_DEPT_SERVICE_TO_INCOME',\n",
    "    'CONTRACT_INCOME', 'CONTRACT_INSTALMENT_AMOUNT', 'CONTRACT_INSTALMENT_AMOUNT_2',\n",
    "    'CONTRACT_LOAN_AMOUNT', 'CONTRACT_MARKET_VALUE', 'CONTRACT_MORTGAGE_LENDING_VALUE', \n",
    "    'CONTRACT_LGD', 'CONTRACT_INCOME'\n",
    "]\n",
    "date_variables = ['CONTRACT_DATE_OF_LOAN_AGREEMENT', 'CONTRACT_MATURITY_DATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['TARGET_EVENT_DAY_DATETIME'] = pd.to_datetime(training_data['TARGET_EVENT_DAY'])\n",
    "training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME'] = pd.to_datetime(training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT'])\n",
    "\n",
    "# Calculate the day difference\n",
    "training_data['DAY_DIFF'] = (training_data['TARGET_EVENT_DAY_DATETIME'] - training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_DATETIME']).dt.days\n",
    "\n",
    "# Create TARGET_EVENT_BINARY_2Y based on conditions\n",
    "training_data['TARGET_EVENT_BINARY_2Y'] = np.where(\n",
    "    (training_data['TARGET_EVENT'] == 'K') & \n",
    "    (training_data['DAY_DIFF'] <= 730) & \n",
    "    (training_data['DAY_DIFF'] >= 0), \n",
    "    1, \n",
    "    0\n",
    ")\n",
    "training_data['TARGET_EVENT_BINARY_1Y'] = np.where(\n",
    "        (training_data['TARGET_EVENT'] == 'K') & \n",
    "        (training_data['DAY_DIFF'] <= 365) & \n",
    "        (training_data['DAY_DIFF'] >= 0), \n",
    "        1, \n",
    "        0\n",
    "    )\n",
    "# Drop the temporary 'DAY_DIFF' column if needed\n",
    "training_data.drop('DAY_DIFF', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CONTRACT_CREDIT_INTERMEDIARY',\n",
       " 'CONTRACT_CREDIT_LOSS',\n",
       " 'CONTRACT_CURRENCY',\n",
       " 'CONTRACT_DEPT_SERVICE_TO_INCOME',\n",
       " 'CONTRACT_INCOME',\n",
       " 'CONTRACT_INSTALMENT_AMOUNT',\n",
       " 'CONTRACT_INSTALMENT_AMOUNT_2',\n",
       " 'CONTRACT_INTEREST_PERIOD',\n",
       " 'CONTRACT_INTEREST_RATE',\n",
       " 'CONTRACT_LGD',\n",
       " 'CONTRACT_LOAN_AMOUNT',\n",
       " 'CONTRACT_LOAN_CONTRACT_TYPE',\n",
       " 'CONTRACT_LOAN_TO_VALUE_RATIO',\n",
       " 'CONTRACT_MARKET_VALUE',\n",
       " 'CONTRACT_MORTGAGE_LENDING_VALUE',\n",
       " 'CONTRACT_MORTGAGE_TYPE',\n",
       " 'CONTRACT_REFINANCED',\n",
       " 'CONTRACT_RISK_WEIGHTED_ASSETS',\n",
       " 'CONTRACT_TYPE_OF_INTEREST_REPAYMENT',\n",
       " 'BORROWER_BIRTH_YEAR',\n",
       " 'BORROWER_CITIZENSHIP',\n",
       " 'BORROWER_COUNTRY',\n",
       " 'BORROWER_COUNTY',\n",
       " 'BORROWER_TYPE_OF_SETTLEMENT']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify numeric columns\n",
    "numeric_columns = training_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Identify target columns that shouldn't be in the X variables\n",
    "excluded_keywords = ['TARGET', 'event', 'binary', 'DATE']\n",
    "\n",
    "# Create lists for X variable columns and target column\n",
    "X_columns = [col for col in numeric_columns if all(keyword.lower() not in col.lower() for keyword in excluded_keywords)]\n",
    "y_column = 'TARGET_EVENT_BINARY_2Y' \n",
    "X_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in date_variables:\n",
    "    training_data[var+ '_JULIAN'] = pd.to_datetime(training_data[var], origin='julian', unit='D')\n",
    "\n",
    "training_data['TIME_TO_MATURITY'] = (training_data['CONTRACT_MATURITY_DATE_JULIAN'] - training_data['CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN']).dt.days\n",
    "\n",
    "year_2018_date = pd.Timestamp('2018-01-01')\n",
    "training_data['TIME_TO_MATURITY'] = (training_data['CONTRACT_MATURITY_DATE_JULIAN'] - year_2018_date).dt.days\n",
    "training_data['ADJUSTED_TIME_TO_MATURITY'] = training_data['TIME_TO_MATURITY'].apply(lambda x: max(min(730, x),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the 'CONTRACT_LOAN_TYPE' and 'CONTRACT_FREQUENCY_TYPE' columns\n",
    "loan_type_dummies = pd.get_dummies(training_data['CONTRACT_LOAN_TYPE'], prefix='LOAN_TYPE', drop_first=True)\n",
    "frequency_type_dummies = pd.get_dummies(training_data['CONTRACT_FREQUENCY_TYPE'], prefix='FREQ_TYPE', drop_first=True)\n",
    "\n",
    "# Concatenate the original DataFrame with the one-hot encoded columns\n",
    "training_data = pd.concat([training_data, loan_type_dummies, frequency_type_dummies], axis=1)\n",
    "\n",
    "# Add the names of the one-hot encoded columns to X_columns\n",
    "X_columns.extend(loan_type_dummies.columns)\n",
    "X_columns.extend(frequency_type_dummies.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['BORROWER_LOAN_COUNT'] = training_data.groupby('BORROWER_ID')['BORROWER_ID'].transform('count')\n",
    "training_data['TOTAL_LOAN_AMOUNT'] = training_data.groupby('BORROWER_ID')['CONTRACT_LOAN_AMOUNT'].transform('sum')\n",
    "training_data['TOTAL_INSTALLMENT_AMOUNT_1'] = training_data.groupby('BORROWER_ID')['CONTRACT_INSTALMENT_AMOUNT'].transform('sum')\n",
    "training_data['TOTAL_INSTALLMENT_AMOUNT_2'] = training_data.groupby('BORROWER_ID')['CONTRACT_INSTALMENT_AMOUNT_2'].transform('sum')\n",
    "training_data['TOTAL_INSTALLMENT_AMOUNT'] = training_data['TOTAL_INSTALLMENT_AMOUNT_1'] + training_data['TOTAL_INSTALLMENT_AMOUNT_2']\n",
    "# training_data['AVERAGE_LOAN_AMOUNT'] = training_data['TOTAL_LOAN_AMOUNT'] / training_data['BORROWER_LOAN_COUNT']\n",
    "# training_data['AVERAGE_LOAN_TERM'] = training_data.groupby('BORROWER_ID')['CONTRACT_MATURITY_DATE'].transform('mean')\n",
    "# training_data['AVERAGE_INTEREST_RATE'] = training_data.groupby('BORROWER_ID')['CONTRACT_INTEREST_RATE'].transform('mean')\n",
    "# training_data['MAX_DEBT_TO_INCOME'] = training_data.groupby('BORROWER_ID')['CONTRACT_DEPT_SERVICE_TO_INCOME'].transform('max')\n",
    "# training_data['HAS_MULTIPLE_LOAN_TYPES'] = training_data.groupby('BORROWER_ID')['CONTRACT_LOAN_TYPE'].transform('nunique').apply(lambda x: 1 if x > 1 else 0)\n",
    "# training_data['HAS_REFINANCED'] = training_data.groupby('BORROWER_ID')['CONTRACT_REFINANCED'].transform('max')\n",
    "\n",
    "# # # training_data['RELATIVE_LOAN_AMOUNT'] = training_data['CONTRACT_LOAN_AMOUNT'] / training_data['TOTAL_LOAN_AMOUNT']\n",
    "# training_data['IS_FIXED_RATE'] = training_data['CONTRACT_LOAN_CONTRACT_TYPE'].apply(lambda x: 1 if x == 'fixed' else 0)\n",
    "# # Ez m√©g nem tudom hogyan de hasznos lehet \n",
    "# # Assuming CONTRACT_MATURITY_DATE is a datetime object and 'current_date' is today's date\n",
    "# # training_data['DAYS_TO_MATURITY'] = (training_data['CONTRACT_MATURITY_DATE'] - current_date).dt.days\n",
    "X_columns.extend(['BORROWER_LOAN_COUNT', 'TOTAL_LOAN_AMOUNT','TOTAL_INSTALLMENT_AMOUNT'])\n",
    "lognormal_variables.extend([ 'TOTAL_LOAN_AMOUNT','TOTAL_INSTALLMENT_AMOUNT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_default_probability(df, group_column, target_column, default_identifier, new_column_name):\n",
    "    # Calculate the number of defaults per group\n",
    "    defaults_per_group = df[df[target_column] == default_identifier].groupby(group_column).size()\n",
    "    \n",
    "    # Calculate the size of each group and avoid zero division by adding 1\n",
    "    group_size_plus_one = df.groupby(group_column).size() + 1\n",
    "    \n",
    "    # Calculate default probability by group (number of defaults divided by group size plus one)\n",
    "    default_prob_by_group = defaults_per_group / group_size_plus_one\n",
    "    \n",
    "    # Map the default probabilities to the original DataFrame\n",
    "    df[new_column_name] = df[group_column].map(default_prob_by_group).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to your dataframe for each column of interest\n",
    "training_data = calculate_default_probability(\n",
    "    training_data, 'CONTRACT_BANK_ID', 'TARGET_EVENT_BINARY_2Y', 1, 'DEFAULT_PROB_BY_BANK'\n",
    ")\n",
    "training_data = calculate_default_probability(\n",
    "    training_data, 'CONTRACT_LOAN_CONTRACT_TYPE', 'TARGET_EVENT_BINARY_2Y', 1, 'DEFAULT_PROB_BY_CONTRACT_TYPE'\n",
    ")\n",
    "training_data = calculate_default_probability(\n",
    "    training_data, 'CONTRACT_LOAN_TYPE', 'TARGET_EVENT_BINARY_2Y', 1, 'DEFAULT_PROB_BY_LOAN_TYPE'\n",
    ")\n",
    "training_data = calculate_default_probability(\n",
    "    training_data, 'CONTRACT_FREQUENCY_TYPE', 'TARGET_EVENT_BINARY_2Y', 1, 'DEFAULT_PROB_BY_FREQUENCY_TYPE'\n",
    ")\n",
    "X_columns.extend(['DEFAULT_PROB_BY_BANK', 'DEFAULT_PROB_BY_CONTRACT_TYPE', 'DEFAULT_PROB_BY_LOAN_TYPE', 'DEFAULT_PROB_BY_FREQUENCY_TYPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables removed: {'DEFAULT_PROB_BY_FREQUENCY_TYPE', 'BORROWER_COUNTRY', 'FREQ_TYPE_2f88e16c', 'DEFAULT_PROB_BY_LOAN_TYPE', 'CONTRACT_REFINANCED'}\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.85  # Set your own threshold\n",
    "correlation_matrix = training_data[X_columns].corr()\n",
    "# Get pairs of highly correlated features\n",
    "highly_correlated_set = set()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            highly_correlated_set.add(colname)\n",
    "\n",
    "# Remove highly correlated features from X_columns\n",
    "X_columns = [col for col in X_columns if col not in highly_correlated_set]\n",
    "print('Variables removed:', highly_correlated_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, log_loss, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def filter_2016(df1, df2, variables):\n",
    "    # Create deep copies to avoid modifying original data\n",
    "    df1_copy = df1.copy()\n",
    "    df2_copy = df2.copy()\n",
    "\n",
    "    # Convert to datetime format if not already\n",
    "    df1_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'] = pd.to_datetime(df1_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'])\n",
    "    df2_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'] = pd.to_datetime(df2_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'])\n",
    "    \n",
    "    # Filter data where CONTRACT_DATE_OF_LOAN_AGREEMENT is before 2016\n",
    "    df1_filtered = df1_copy[df1_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'].dt.year < 2016]\n",
    "    df2_filtered = df2_copy[df2_copy['CONTRACT_DATE_OF_LOAN_AGREEMENT'].dt.year < 2016]\n",
    "    \n",
    "    df1_filtered['CONTRACT_DATE_OF_LOAN_AGREEMENT'] = df1_filtered['CONTRACT_DATE_OF_LOAN_AGREEMENT'].apply(lambda x: x.toordinal())\n",
    "    df2_filtered['CONTRACT_DATE_OF_LOAN_AGREEMENT'] = df2_filtered['CONTRACT_DATE_OF_LOAN_AGREEMENT'].apply(lambda x: x.toordinal())\n",
    "\n",
    "    return df1_filtered, df2_filtered\n",
    "\n",
    "def train_and_predict_two_halves(df, variables, target, model1=LogisticRegression(), scaler=StandardScaler()):\n",
    "    # Split the dataframe into two halves\n",
    "    if lognormal_variables is not None:\n",
    "        df = df.copy()\n",
    "        \n",
    "        for var in lognormal_variables:\n",
    "            if var == 'CONTRACT_CREDIT_LOSS':\n",
    "                df[var] = np.log1p(np.abs(df[var]))*np.sign(df[var])\n",
    "            else:\n",
    "                df[var] = np.log1p(df[var])\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[variables], df[target], test_size=0.5, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model1.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict probabilities on the test data\n",
    "    y_test_proba = model1.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate log loss on the test set\n",
    "    test_log_loss = log_loss(y_test, y_test_proba)\n",
    "    print(\"Log Loss on test set:\", test_log_loss)\n",
    "    \n",
    "    X_scaled = scaler.fit_transform(df[variables])\n",
    "\n",
    "    y = df[target]\n",
    "    model1.fit(X_scaled, y)\n",
    "    proba = model1.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "    return proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def cluster(df, variables, scaler=StandardScaler()):\n",
    "    # Log transformation\n",
    "    if lognormal_variables is not None:\n",
    "        df = df.copy()\n",
    "        for var in lognormal_variables:\n",
    "            if var == 'CONTRACT_CREDIT_LOSS':\n",
    "                df[var] = np.log1p(np.abs(df[var])) * np.sign(df[var])\n",
    "            else:\n",
    "                df[var] = np.log1p(df[var])\n",
    "\n",
    "    # Scaling\n",
    "    X_scaled = scaler.fit_transform(df[variables])\n",
    "\n",
    "    # Determine the optimal number of clusters using the Elbow method\n",
    "    # inertia = []\n",
    "    # k_values = range(1, 11)  # Example range from 1 to 10\n",
    "    # for k in k_values:\n",
    "    #     kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    #     kmeans.fit(X_scaled)\n",
    "    #     inertia.append(kmeans.inertia_)\n",
    "\n",
    "    # # Plotting the Elbow Curve\n",
    "    # sns.set_theme()  # Using seaborn's default theme for the plot\n",
    "    # plt.figure(figsize=(8, 5))\n",
    "    # plt.plot(k_values, inertia, marker='o', linestyle='-', color='b')\n",
    "    # plt.title('Elbow Method For Optimal k')\n",
    "    # plt.xlabel('Number of clusters')\n",
    "    # plt.ylabel('Inertia')\n",
    "    # plt.xticks(k_values)\n",
    "    # plt.show()\n",
    "\n",
    "    # Based on the plot, let's say the elbow point is at k = optimal_k\n",
    "    optimal_k = 5  # This is an example, you should choose the optimal_k based on the plot\n",
    "\n",
    "    # Clustering with the optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    clusters = kmeans.labels_\n",
    "\n",
    "    # Evaluate the clustering\n",
    "    # silhouette_avg = silhouette_score(X_scaled, clusters)\n",
    "    # print(f'Silhouette Score for k={optimal_k}: {silhouette_avg}')\n",
    "\n",
    "    # Add cluster labels to the dataframe\n",
    "    df['cluster'] = clusters\n",
    "\n",
    "    return clusters\n",
    "\n",
    "# Use your dataframe here\n",
    "kmeans_clusters = cluster(training_data, X_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn.metrics import silhouette_score\n",
    "\n",
    "# def apply_dbscan_clustering(df, variables, scaler=StandardScaler(), eps=0.5, min_samples=5):\n",
    "#     # Scaling the data\n",
    "#     if lognormal_variables is not None:\n",
    "#         df = df.copy()\n",
    "#         for var in lognormal_variables:\n",
    "#             if var == 'CONTRACT_CREDIT_LOSS':\n",
    "#                 df[var] = np.log1p(np.abs(df[var])) * np.sign(df[var])\n",
    "#             else:\n",
    "#                 df[var] = np.log1p(df[var])\n",
    "\n",
    "#     X_scaled = scaler.fit_transform(df[variables])\n",
    "\n",
    "#     # DBSCAN clustering\n",
    "#     db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "#     clusters = db.fit_predict(X_scaled)\n",
    "\n",
    "#     # Silhouette score can be used if there is more than one cluster and noise\n",
    "#     # if len(set(clusters)) - (1 if -1 in clusters else 0) > 1:\n",
    "#     #     silhouette_avg = silhouette_score(X_scaled, clusters)\n",
    "#     #     print(f'Silhouette Score: {silhouette_avg}')\n",
    "\n",
    "#     # Add cluster labels to the dataframe\n",
    "#     # df['cluster'] = clusters\n",
    "#     return clusters\n",
    "\n",
    "# # Example usage:\n",
    "# dbscan_cluster = apply_dbscan_clustering(training_data, X_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from sklearn.metrics import silhouette_score\n",
    "\n",
    "# def apply_hierarchical_clustering(df, variables, scaler=StandardScaler(), n_clusters=None, distance_threshold=0):\n",
    "#     # Scaling the data\n",
    "#     if lognormal_variables is not None:\n",
    "#         df = df.copy()\n",
    "#         for var in lognormal_variables:\n",
    "#             if var == 'CONTRACT_CREDIT_LOSS':\n",
    "#                 df[var] = np.log1p(np.abs(df[var])) * np.sign(df[var])\n",
    "#             else:\n",
    "#                 df[var] = np.log1p(df[var])\n",
    "\n",
    "#     X_scaled = scaler.fit_transform(df[variables])\n",
    "\n",
    "#     # Agglomerative hierarchical clustering\n",
    "#     agg_clust = AgglomerativeClustering(n_clusters=n_clusters, distance_threshold=distance_threshold)\n",
    "#     clusters = agg_clust.fit_predict(X_scaled)\n",
    "\n",
    "#     # Silhouette score can be used if clusters are more than one\n",
    "#     # if len(set(clusters)) > 1:\n",
    "#     #     silhouette_avg = silhouette_score(X_scaled, clusters)\n",
    "#     #     print(f'Silhouette Score: {silhouette_avg}')\n",
    "\n",
    "#     # Add cluster labels to the dataframe\n",
    "#     df['cluster'] = clusters\n",
    "#     return df\n",
    "\n",
    "# # Example usage:\n",
    "# hierarchical_cluster = apply_hierarchical_clustering(training_data, X_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['KMEANS_CLUSTER'] = kmeans_clusters\n",
    "# training_data['DBSCAN_CLUSTER'] = dbscan_cluster\n",
    "# training_data['HIERARCHICAL_CLUSTER'] = hierarchical_cluster\n",
    "#  Apply the function to your dataframe for each column of interest\n",
    "training_data = calculate_default_probability(\n",
    "    training_data, 'KMEANS_CLUSTER', 'TARGET_EVENT_BINARY_2Y', 1, 'DEFAULT_PROB_BY_KMEANS_CLUSTER'\n",
    ")\n",
    "# training_data = calculate_default_probability(\n",
    "#     training_data, 'DBSCAN_CLUSTER', 'TARGET_EVENT_BINARY_2Y', 1, 'DEFAULT_PROB_BY_DBSCAN_CLUSTER'\n",
    "# )\n",
    "# training_data = calculate_default_probability(\n",
    "#     training_data, 'HIERARCHICAL_CLUSTER', 'TARGET_EVENT_BINARY_2Y', 1, 'DEFAULT_PROB_BY_HIERARCHICAL_CLUSTER'\n",
    "# )\n",
    "\n",
    "X_columns.extend(['DEFAULT_PROB_BY_KMEANS_CLUSTER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "def significant_features(df, variables, target, model1=LogisticRegression(), scaler=StandardScaler()):\n",
    "    if lognormal_variables is not None:\n",
    "        df = df.copy()\n",
    "        \n",
    "        for var in lognormal_variables:\n",
    "            if var == 'CONTRACT_CREDIT_LOSS':\n",
    "                df[var] = np.log1p(np.abs(df[var]))*np.sign(df[var])\n",
    "            else:\n",
    "                df[var] = np.log1p(df[var])\n",
    "\n",
    "    X_scaled = scaler.fit_transform(df[variables])\n",
    "\n",
    "    X = sm.add_constant(pd.DataFrame(X_scaled, columns=variables))\n",
    "    y = df[target]\n",
    "\n",
    "    # Fit the logistic regression model\n",
    "    model = sm.Logit(y, X).fit()\n",
    "\n",
    "    # Display the summary\n",
    "    print(model.summary())\n",
    "    # Get the p-values\n",
    "    p_values = model.pvalues\n",
    "\n",
    "    # Identify the non-significant variables\n",
    "    non_significant_vars = p_values[p_values > 0.05].index.tolist()\n",
    "\n",
    "    return model, non_significant_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def cut_exponential_tails(df, target):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Step 1: Estimate lambda for each row based on its 'target' value and 'ADJUSTED_TIME_TO_MATURITY'\n",
    "    df['LAMBDA_ESTIMATE'] = -np.log(1 - df[target])/730\n",
    "\n",
    "    # Step 2: Calculate new probabilities p_exp for each row based on its own lambda_estimate\n",
    "    df[target] = 1 - np.exp(-df['LAMBDA_ESTIMATE'] * df['ADJUSTED_TIME_TO_MATURITY'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def combined_probability(s):\n",
    "    return 1 - np.prod(1 - s.values)\n",
    "\n",
    "def create_submission_file(df_preds, target, example, filename='submission.csv', testing=False):\n",
    "    # Filter the data to only include BORROWER_IDs that are in the submission example\n",
    "    filtered_training_data = df_preds[df_preds['BORROWER_ID'].isin(example['BORROWER_ID'])]\n",
    "\n",
    "    # Print warning if the row count is off\n",
    "    if not testing and len(filtered_training_data) != 1564601:\n",
    "        print('WARNING: The filtered data does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "        # raise ValueError('WARNING: The submission file does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "    # Group by BORROWER_ID and calculate the combined probability\n",
    "\n",
    "    #######################x########################\n",
    "    #CUTTING TAILS DID NOT SEEM TO WORK\n",
    "    #######################x########################\n",
    "    # filtered_training_data = cut_exponential_tails(filtered_training_data, target)\n",
    "    grouped_data = filtered_training_data.groupby('BORROWER_ID')[target].apply(combined_probability).reset_index()\n",
    "\n",
    "    # Create the submission DataFrame\n",
    "    df_submission = pd.DataFrame()\n",
    "    df_submission['BORROWER_ID'] = grouped_data['BORROWER_ID']\n",
    "    df_submission['PRED'] = grouped_data[target]\n",
    "    print('Centering probabilities...')\n",
    "    # Center the probabilities around 1.48%\n",
    "    desired_mean = 0.0148  # 1.48% as a decimal\n",
    "    while (df_submission['PRED'].max() > 1 or df_submission['PRED'].min() < 0 or abs(df_submission['PRED'].mean() -0.0148) > 0.0005):\n",
    "        # print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "        df_submission['PRED'] = df_submission['PRED'].clip(lower=0, upper=1)\n",
    "        # print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "\n",
    "        current_mean = df_submission['PRED'].mean()\n",
    "        adjustment_factor = desired_mean  - current_mean\n",
    "        df_submission['PRED'] += adjustment_factor\n",
    "    print(df_submission['PRED'].max(), df_submission['PRED'].min(), df_submission['PRED'].mean())\n",
    "    # Save the submission file\n",
    "    if  not testing and filename is not None:\n",
    "        df_submission.to_csv(filename, index=False)\n",
    "    print(f'Saved file: {filename}')\n",
    "    # if abs(df_submission['PRED'].mean() -0.0148) > 0.0005:\n",
    "    #    raise ValueError('WARNING: mean is bad')\n",
    "        \n",
    "    # Print warning if the row count is off\n",
    "    if not testing and len(df_submission) != 1117674:\n",
    "        print('WARNING: The submission file does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "        # raise ValueError('WARNING: The submission file does not have the correct number of rows. Make sure you are not using the training data for submission.')\n",
    "        \n",
    "    return df_submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_2y_1y(df, variables, target, model1=LogisticRegression(), model2=LogisticRegression()):\n",
    "    df = df.copy()\n",
    "    start_date = pd.Timestamp('2015-01-01')\n",
    "    end_date = pd.Timestamp('2017-01-01')\n",
    "\n",
    "    # Mask for rows with CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN between start_date and end_date\n",
    "    mask_date_range = (df['CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN'] >= start_date) & (df['CONTRACT_DATE_OF_LOAN_AGREEMENT_JULIAN'] <= end_date)\n",
    "\n",
    "    df = df[mask_date_range]\n",
    "\n",
    "    probs = train_and_predict_two_halves(\n",
    "        df, \n",
    "        variables, \n",
    "        target, \n",
    "        model1=model1,\n",
    "        model2=model2,\n",
    "    )\n",
    "    df['2Y-1Y-PROBS'] = probs\n",
    "    \n",
    "    test_data = pd.read_csv('./data/1y-test.csv')\n",
    "\n",
    "\n",
    "    df_submission = create_submission_file(df, '2Y-1Y-PROBS', test_data, filename=None, testing=True)\n",
    "\n",
    "    merged_df = pd.merge(test_data, df_submission, on='BORROWER_ID')\n",
    "    true_labels = merged_df['TARGET_EVENT_BINARY_1Y']\n",
    "    predicted_probs = merged_df['PRED']\n",
    "    logloss = log_loss(true_labels, predicted_probs)\n",
    "\n",
    "    print(f'Log loss: {logloss}')\n",
    "        \n",
    "    return logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.015745\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Logit Regression Results                             \n",
      "==================================================================================\n",
      "Dep. Variable:     TARGET_EVENT_BINARY_2Y   No. Observations:              1601663\n",
      "Model:                              Logit   Df Residuals:                  1601611\n",
      "Method:                               MLE   Df Model:                           51\n",
      "Date:                    Thu, 02 Nov 2023   Pseudo R-squ.:                  0.6126\n",
      "Time:                            16:28:20   Log-Likelihood:                -25218.\n",
      "converged:                          False   LL-Null:                       -65100.\n",
      "Covariance Type:                nonrobust   LLR p-value:                     0.000\n",
      "=======================================================================================================\n",
      "                                          coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "const                                 -10.3448      4.178     -2.476      0.013     -18.533      -2.156\n",
      "CONTRACT_CREDIT_INTERMEDIARY            0.0250      0.004      5.676      0.000       0.016       0.034\n",
      "CONTRACT_CREDIT_LOSS                    4.7230      0.052     90.725      0.000       4.621       4.825\n",
      "CONTRACT_CURRENCY                      -0.0059      0.003     -2.250      0.024      -0.011      -0.001\n",
      "CONTRACT_DEPT_SERVICE_TO_INCOME        -0.1346      0.027     -4.916      0.000      -0.188      -0.081\n",
      "CONTRACT_INCOME                         0.2177      0.028      7.887      0.000       0.164       0.272\n",
      "CONTRACT_INSTALMENT_AMOUNT              0.2240      0.012     18.519      0.000       0.200       0.248\n",
      "CONTRACT_INSTALMENT_AMOUNT_2           -0.1490      0.023     -6.451      0.000      -0.194      -0.104\n",
      "CONTRACT_INTEREST_PERIOD               -0.0800      0.025     -3.215      0.001      -0.129      -0.031\n",
      "CONTRACT_INTEREST_RATE                  0.1948      0.018     10.573      0.000       0.159       0.231\n",
      "CONTRACT_LGD                           -0.4674      0.031    -15.099      0.000      -0.528      -0.407\n",
      "CONTRACT_LOAN_AMOUNT                   -0.1587      0.038     -4.143      0.000      -0.234      -0.084\n",
      "CONTRACT_LOAN_CONTRACT_TYPE             0.6539      0.055     11.915      0.000       0.546       0.761\n",
      "CONTRACT_LOAN_TO_VALUE_RATIO           -0.3627      0.037     -9.854      0.000      -0.435      -0.291\n",
      "CONTRACT_MARKET_VALUE                   0.5785      0.083      6.979      0.000       0.416       0.741\n",
      "CONTRACT_MORTGAGE_LENDING_VALUE        -0.8766      0.076    -11.481      0.000      -1.026      -0.727\n",
      "CONTRACT_MORTGAGE_TYPE                  0.2641      0.028      9.357      0.000       0.209       0.319\n",
      "CONTRACT_RISK_WEIGHTED_ASSETS           0.0179      0.004      4.709      0.000       0.010       0.025\n",
      "CONTRACT_TYPE_OF_INTEREST_REPAYMENT    -0.6589      0.013    -49.445      0.000      -0.685      -0.633\n",
      "BORROWER_BIRTH_YEAR                    -0.1420      0.042     -3.409      0.001      -0.224      -0.060\n",
      "BORROWER_CITIZENSHIP                   -0.0630      0.009     -7.056      0.000      -0.081      -0.046\n",
      "BORROWER_COUNTY                        -0.0022      0.013     -0.170      0.865      -0.027       0.023\n",
      "BORROWER_TYPE_OF_SETTLEMENT             0.0414      0.015      2.740      0.006       0.012       0.071\n",
      "LOAN_TYPE_1f951336                      4.8356    487.209      0.010      0.992    -950.077     959.748\n",
      "LOAN_TYPE_2f88e16c                      2.3369    304.281      0.008      0.994    -594.042     598.716\n",
      "LOAN_TYPE_47693941                     -0.0334    348.625  -9.59e-05      1.000    -683.327     683.260\n",
      "LOAN_TYPE_5a06241e                      2.1311    185.995      0.011      0.991    -362.413     366.675\n",
      "LOAN_TYPE_694cbaee                      0.8973     76.269      0.012      0.991    -148.586     150.381\n",
      "LOAN_TYPE_69f70539                      3.2310    329.704      0.010      0.992    -642.976     649.438\n",
      "LOAN_TYPE_7e2065f4                      3.7565    372.594      0.010      0.992    -726.514     734.027\n",
      "LOAN_TYPE_83910425                      1.2269    107.173      0.011      0.991    -208.828     211.282\n",
      "LOAN_TYPE_8fe006f1                      0.0087    247.441    3.5e-05      1.000    -484.967     484.984\n",
      "LOAN_TYPE_955ae3ef                      3.0791    302.736      0.010      0.992    -590.272     596.430\n",
      "LOAN_TYPE_95c4f8fb                      0.3132     30.847      0.010      0.992     -60.146      60.772\n",
      "LOAN_TYPE_b503a0de                      7.5792    789.183      0.010      0.992   -1539.191    1554.349\n",
      "LOAN_TYPE_cde77491                      0.0092    238.357   3.88e-05      1.000    -467.162     467.181\n",
      "LOAN_TYPE_cf07c2dd                      2.4650    243.306      0.010      0.992    -474.405     479.335\n",
      "LOAN_TYPE_d3aaffde                      3.8983    378.124      0.010      0.992    -737.212     745.008\n",
      "LOAN_TYPE_eab72d7a                      0.6447     62.010      0.010      0.992    -120.894     122.183\n",
      "LOAN_TYPE_f792971b                      0.0482     23.769      0.002      0.998     -46.538      46.635\n",
      "FREQ_TYPE_3265c5b7                     -0.3328      0.147     -2.269      0.023      -0.620      -0.045\n",
      "FREQ_TYPE_479a2e13                     -1.0212      0.378     -2.701      0.007      -1.762      -0.280\n",
      "FREQ_TYPE_87db11f5                     -0.7759      0.240     -3.238      0.001      -1.246      -0.306\n",
      "FREQ_TYPE_89efd382                     -0.0295      0.022     -1.311      0.190      -0.074       0.015\n",
      "FREQ_TYPE_ad534644                     -0.0800      0.045     -1.771      0.077      -0.168       0.009\n",
      "FREQ_TYPE_bd092d5a                     -0.0008      0.004     -0.183      0.855      -0.009       0.008\n",
      "BORROWER_LOAN_COUNT                    -0.1123      0.020     -5.736      0.000      -0.151      -0.074\n",
      "TOTAL_LOAN_AMOUNT                      -0.4742      0.039    -12.063      0.000      -0.551      -0.397\n",
      "TOTAL_INSTALLMENT_AMOUNT                0.1276      0.018      7.228      0.000       0.093       0.162\n",
      "DEFAULT_PROB_BY_BANK                    0.4572      0.018     25.066      0.000       0.421       0.493\n",
      "DEFAULT_PROB_BY_CONTRACT_TYPE           0.2126      0.011     18.490      0.000       0.190       0.235\n",
      "DEFAULT_PROB_BY_KMEANS_CLUSTER          0.3289      0.031     10.722      0.000       0.269       0.389\n",
      "=======================================================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.48 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "model, non_significant_vars = significant_features(training_data, X_columns, y_column,LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BORROWER_COUNTY',\n",
       " 'LOAN_TYPE_1f951336',\n",
       " 'LOAN_TYPE_2f88e16c',\n",
       " 'LOAN_TYPE_47693941',\n",
       " 'LOAN_TYPE_5a06241e',\n",
       " 'LOAN_TYPE_694cbaee',\n",
       " 'LOAN_TYPE_69f70539',\n",
       " 'LOAN_TYPE_7e2065f4',\n",
       " 'LOAN_TYPE_83910425',\n",
       " 'LOAN_TYPE_8fe006f1',\n",
       " 'LOAN_TYPE_955ae3ef',\n",
       " 'LOAN_TYPE_95c4f8fb',\n",
       " 'LOAN_TYPE_b503a0de',\n",
       " 'LOAN_TYPE_cde77491',\n",
       " 'LOAN_TYPE_cf07c2dd',\n",
       " 'LOAN_TYPE_d3aaffde',\n",
       " 'LOAN_TYPE_eab72d7a',\n",
       " 'LOAN_TYPE_f792971b',\n",
       " 'FREQ_TYPE_89efd382',\n",
       " 'FREQ_TYPE_ad534644',\n",
       " 'FREQ_TYPE_bd092d5a']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_significant_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns.remove('CONTRACT_CREDIT_INTERMEDIARY')\n",
    "X_columns.remove('BORROWER_COUNTY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTRACT_ID</th>\n",
       "      <th>BORROWER_ID</th>\n",
       "      <th>CONTRACT_BANK_ID</th>\n",
       "      <th>CONTRACT_CREDIT_INTERMEDIARY</th>\n",
       "      <th>CONTRACT_CREDIT_LOSS</th>\n",
       "      <th>CONTRACT_CURRENCY</th>\n",
       "      <th>CONTRACT_DATE_OF_LOAN_AGREEMENT</th>\n",
       "      <th>CONTRACT_DEPT_SERVICE_TO_INCOME</th>\n",
       "      <th>CONTRACT_FREQUENCY_TYPE</th>\n",
       "      <th>CONTRACT_INCOME</th>\n",
       "      <th>...</th>\n",
       "      <th>TOTAL_LOAN_AMOUNT</th>\n",
       "      <th>TOTAL_INSTALLMENT_AMOUNT_1</th>\n",
       "      <th>TOTAL_INSTALLMENT_AMOUNT_2</th>\n",
       "      <th>TOTAL_INSTALLMENT_AMOUNT</th>\n",
       "      <th>DEFAULT_PROB_BY_BANK</th>\n",
       "      <th>DEFAULT_PROB_BY_CONTRACT_TYPE</th>\n",
       "      <th>DEFAULT_PROB_BY_LOAN_TYPE</th>\n",
       "      <th>DEFAULT_PROB_BY_FREQUENCY_TYPE</th>\n",
       "      <th>KMEANS_CLUSTER</th>\n",
       "      <th>DEFAULT_PROB_BY_KMEANS_CLUSTER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MDE1zNJ</td>\n",
       "      <td>PRF52C7u</td>\n",
       "      <td>caa130b5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>359398.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457709</td>\n",
       "      <td>54.85</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>372461.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3319933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80717.0</td>\n",
       "      <td>80717.0</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>0.010327</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1rruIo4</td>\n",
       "      <td>MIYaoPq8</td>\n",
       "      <td>f789f8b0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3937.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457904</td>\n",
       "      <td>29.47</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>223276.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2533432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20311.0</td>\n",
       "      <td>20311.0</td>\n",
       "      <td>0.006072</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xhdrsu5x</td>\n",
       "      <td>0xFcqzYv</td>\n",
       "      <td>c2a40996</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18785.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457282</td>\n",
       "      <td>81.65</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>231007.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7024876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98662.0</td>\n",
       "      <td>98662.0</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.005552</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LrUfGq+</td>\n",
       "      <td>Y551bMvC</td>\n",
       "      <td>bd940aa2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3848.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457553</td>\n",
       "      <td>8.33</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>661272.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7998750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35616.0</td>\n",
       "      <td>35616.0</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fR2gPfST</td>\n",
       "      <td>A5NUFs2z</td>\n",
       "      <td>1d42bbf5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457406</td>\n",
       "      <td>0.00</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>120723</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12080.0</td>\n",
       "      <td>12080.0</td>\n",
       "      <td>0.006835</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601658</th>\n",
       "      <td>XladPB3d</td>\n",
       "      <td>0sMkSY/g</td>\n",
       "      <td>1d42bbf5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100075.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2458012</td>\n",
       "      <td>30.46</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>507273.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5990964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32582.0</td>\n",
       "      <td>32582.0</td>\n",
       "      <td>0.006835</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601659</th>\n",
       "      <td>q6UqBBI</td>\n",
       "      <td>ww9/NOGp</td>\n",
       "      <td>f789f8b0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14888.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457515</td>\n",
       "      <td>27.75</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>216392.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1420072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20080.0</td>\n",
       "      <td>20080.0</td>\n",
       "      <td>0.006072</td>\n",
       "      <td>0.010327</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601660</th>\n",
       "      <td>hIBR3bvU</td>\n",
       "      <td>MnSjeYaF</td>\n",
       "      <td>533b7c4a</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457680</td>\n",
       "      <td>0.00</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2453666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14947.0</td>\n",
       "      <td>14947.0</td>\n",
       "      <td>0.013201</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601661</th>\n",
       "      <td>nUmU4XLo</td>\n",
       "      <td>9cpBKGZ0</td>\n",
       "      <td>caa130b5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457345</td>\n",
       "      <td>10.66</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>173130.0</td>\n",
       "      <td>...</td>\n",
       "      <td>272804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17001.0</td>\n",
       "      <td>17001.0</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601662</th>\n",
       "      <td>muTG8saz</td>\n",
       "      <td>TPKz6u6z</td>\n",
       "      <td>1d42bbf5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12851.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2457948</td>\n",
       "      <td>50.13</td>\n",
       "      <td>479a2e13</td>\n",
       "      <td>115434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1107035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48694.0</td>\n",
       "      <td>48694.0</td>\n",
       "      <td>0.006835</td>\n",
       "      <td>0.010327</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1601663 rows √ó 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CONTRACT_ID BORROWER_ID CONTRACT_BANK_ID  \\\n",
       "0           MDE1zNJ    PRF52C7u         caa130b5   \n",
       "1           1rruIo4    MIYaoPq8         f789f8b0   \n",
       "2          xhdrsu5x    0xFcqzYv         c2a40996   \n",
       "3           LrUfGq+    Y551bMvC         bd940aa2   \n",
       "4          fR2gPfST    A5NUFs2z         1d42bbf5   \n",
       "...             ...         ...              ...   \n",
       "1601658    XladPB3d    0sMkSY/g         1d42bbf5   \n",
       "1601659     q6UqBBI    ww9/NOGp         f789f8b0   \n",
       "1601660    hIBR3bvU    MnSjeYaF         533b7c4a   \n",
       "1601661    nUmU4XLo    9cpBKGZ0         caa130b5   \n",
       "1601662    muTG8saz    TPKz6u6z         1d42bbf5   \n",
       "\n",
       "         CONTRACT_CREDIT_INTERMEDIARY  CONTRACT_CREDIT_LOSS  \\\n",
       "0                                 2.0              359398.0   \n",
       "1                                 2.0                3937.0   \n",
       "2                                 2.0               18785.0   \n",
       "3                                 1.0                3848.0   \n",
       "4                                 2.0                   0.0   \n",
       "...                               ...                   ...   \n",
       "1601658                           1.0              100075.0   \n",
       "1601659                           2.0               14888.0   \n",
       "1601660                           2.0                   0.0   \n",
       "1601661                           1.0                   0.0   \n",
       "1601662                           2.0               12851.0   \n",
       "\n",
       "         CONTRACT_CURRENCY  CONTRACT_DATE_OF_LOAN_AGREEMENT  \\\n",
       "0                       31                          2457709   \n",
       "1                       31                          2457904   \n",
       "2                       31                          2457282   \n",
       "3                       31                          2457553   \n",
       "4                       31                          2457406   \n",
       "...                    ...                              ...   \n",
       "1601658                 31                          2458012   \n",
       "1601659                 31                          2457515   \n",
       "1601660                 31                          2457680   \n",
       "1601661                 31                          2457345   \n",
       "1601662                 31                          2457948   \n",
       "\n",
       "         CONTRACT_DEPT_SERVICE_TO_INCOME CONTRACT_FREQUENCY_TYPE  \\\n",
       "0                                  54.85                479a2e13   \n",
       "1                                  29.47                479a2e13   \n",
       "2                                  81.65                479a2e13   \n",
       "3                                   8.33                479a2e13   \n",
       "4                                   0.00                479a2e13   \n",
       "...                                  ...                     ...   \n",
       "1601658                            30.46                479a2e13   \n",
       "1601659                            27.75                479a2e13   \n",
       "1601660                             0.00                479a2e13   \n",
       "1601661                            10.66                479a2e13   \n",
       "1601662                            50.13                479a2e13   \n",
       "\n",
       "         CONTRACT_INCOME  ...  TOTAL_LOAN_AMOUNT  TOTAL_INSTALLMENT_AMOUNT_1  \\\n",
       "0               372461.0  ...            3319933                         0.0   \n",
       "1               223276.0  ...            2533432                         0.0   \n",
       "2               231007.0  ...            7024876                         0.0   \n",
       "3               661272.0  ...            7998750                         0.0   \n",
       "4                    0.0  ...             120723                         0.0   \n",
       "...                  ...  ...                ...                         ...   \n",
       "1601658         507273.0  ...            5990964                         0.0   \n",
       "1601659         216392.0  ...            1420072                         0.0   \n",
       "1601660              0.0  ...            2453666                         0.0   \n",
       "1601661         173130.0  ...             272804                         0.0   \n",
       "1601662         115434.0  ...            1107035                         0.0   \n",
       "\n",
       "         TOTAL_INSTALLMENT_AMOUNT_2  TOTAL_INSTALLMENT_AMOUNT  \\\n",
       "0                           80717.0                   80717.0   \n",
       "1                           20311.0                   20311.0   \n",
       "2                           98662.0                   98662.0   \n",
       "3                           35616.0                   35616.0   \n",
       "4                           12080.0                   12080.0   \n",
       "...                             ...                       ...   \n",
       "1601658                     32582.0                   32582.0   \n",
       "1601659                     20080.0                   20080.0   \n",
       "1601660                     14947.0                   14947.0   \n",
       "1601661                     17001.0                   17001.0   \n",
       "1601662                     48694.0                   48694.0   \n",
       "\n",
       "         DEFAULT_PROB_BY_BANK  DEFAULT_PROB_BY_CONTRACT_TYPE  \\\n",
       "0                    0.012171                       0.010327   \n",
       "1                    0.006072                       0.000919   \n",
       "2                    0.000785                       0.005552   \n",
       "3                    0.000969                       0.000919   \n",
       "4                    0.006835                       0.004690   \n",
       "...                       ...                            ...   \n",
       "1601658              0.006835                       0.000919   \n",
       "1601659              0.006072                       0.010327   \n",
       "1601660              0.013201                       0.000919   \n",
       "1601661              0.012171                       0.004690   \n",
       "1601662              0.006835                       0.010327   \n",
       "\n",
       "         DEFAULT_PROB_BY_LOAN_TYPE  DEFAULT_PROB_BY_FREQUENCY_TYPE  \\\n",
       "0                         0.000957                        0.006482   \n",
       "1                         0.000957                        0.006482   \n",
       "2                         0.000957                        0.006482   \n",
       "3                         0.000677                        0.006482   \n",
       "4                         0.000957                        0.006482   \n",
       "...                            ...                             ...   \n",
       "1601658                   0.000677                        0.006482   \n",
       "1601659                   0.000957                        0.006482   \n",
       "1601660                   0.000957                        0.006482   \n",
       "1601661                   0.000957                        0.006482   \n",
       "1601662                   0.000957                        0.006482   \n",
       "\n",
       "        KMEANS_CLUSTER  DEFAULT_PROB_BY_KMEANS_CLUSTER  \n",
       "0                    2                        0.012785  \n",
       "1                    3                        0.001488  \n",
       "2                    3                        0.001488  \n",
       "3                    3                        0.001488  \n",
       "4                    0                        0.001511  \n",
       "...                ...                             ...  \n",
       "1601658              3                        0.001488  \n",
       "1601659              2                        0.012785  \n",
       "1601660              0                        0.001511  \n",
       "1601661              0                        0.001511  \n",
       "1601662              2                        0.012785  \n",
       "\n",
       "[1601663 rows x 78 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss on test set: 0.01784619252239841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\takat\\AppData\\Local\\Temp\\ipykernel_39148\\2283563472.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_unique[predicted_probs] = probs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centering probabilities...\n",
      "1.0 0.009078341354474752 0.014799999999999999\n",
      "Saved file: ./predictions/logistic-regression-one-model-one-contract.csv\n"
     ]
    }
   ],
   "source": [
    "predicted_probs = 'LOGISTIC_REG'\n",
    "\n",
    "training_data_unique = training_data.drop_duplicates(subset='CONTRACT_ID')\n",
    "\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data_unique, \n",
    "    X_columns, \n",
    "    y_column, \n",
    "    model1=LogisticRegression(max_iter=400, C=0.5, random_state=42),\n",
    ")\n",
    "\n",
    "training_data_unique[predicted_probs] = probs\n",
    "probs_df = pd.DataFrame({\n",
    "    'CONTRACT_ID': training_data_unique['CONTRACT_ID'],\n",
    "     predicted_probs: probs\n",
    "})\n",
    "training_data = training_data.merge(probs_df, on='CONTRACT_ID', how='left')\n",
    "# training_data[predicted_probs] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.601663e+06\n",
       "mean     6.747739e-03\n",
       "std      4.960275e-02\n",
       "min      4.888395e-15\n",
       "25%      1.281811e-06\n",
       "50%      1.220022e-04\n",
       "75%      4.924023e-04\n",
       "max      1.000000e+00\n",
       "Name: LOGISTIC_REG, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[predicted_probs].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(x):\n",
    "    if x<1.953286e-06:\n",
    "        return 10**(-10)\n",
    "    else:\n",
    "        return x\n",
    "training_data['CLIPPED'] = training_data[predicted_probs].apply(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centering probabilities...\n",
      "1.0 0.009078439876684437 0.014800000000000013\n",
      "Saved file: ./predictions/logistic-regression-one-model-one-contract-cluster-clip.csv\n"
     ]
    }
   ],
   "source": [
    "submission = create_submission_file(training_data, 'CLIPPED', data_submission_example, filename='./predictions/logistic-regression-one-model-one-contract-cluster-clip.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_and_predict_two_halves() got an unexpected keyword argument 'model2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\takat\\Documents\\Egyetem\\G√©pi tanul√°si esettanulm√°ny labor\\Defaults\\working-baseline.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/working-baseline.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predicted_probs \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRANDOM_FOREST\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/working-baseline.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m probs \u001b[39m=\u001b[39m train_and_predict_two_halves(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/working-baseline.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     training_data, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/working-baseline.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     X_columns, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/working-baseline.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     y_column, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/working-baseline.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     model1\u001b[39m=\u001b[39;49mRandomForestClassifier(n_estimators\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, max_depth\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/working-baseline.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model2\u001b[39m=\u001b[39;49mRandomForestClassifier(n_estimators\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, max_depth\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/working-baseline.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/working-baseline.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m training_data[predicted_probs] \u001b[39m=\u001b[39m probs\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/takat/Documents/Egyetem/G%C3%A9pi%20tanul%C3%A1si%20esettanulm%C3%A1ny%20labor/Defaults/working-baseline.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m submission \u001b[39m=\u001b[39m create_submission_file(training_data, predicted_probs,data_submission_example, filename\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./predictions/random-forrest-2y-exp-no-multicolinearity-lognormal-time-no-dates.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: train_and_predict_two_halves() got an unexpected keyword argument 'model2'"
     ]
    }
   ],
   "source": [
    "predicted_probs = 'RANDOM_FOREST'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    y_column, \n",
    "    model1=RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42),\n",
    "    model2=RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42),\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "submission = create_submission_file(training_data, predicted_probs,data_submission_example, filename='./predictions/random-forrest-2y-exp-no-multicolinearity-lognormal-time-no-dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probs = 'GRADIENT_BOOSTING_CLASSIFIER'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    y_column, \n",
    "    model1=GradientBoostingClassifier(random_state=42),\n",
    "    model2=GradientBoostingClassifier(random_state=42),\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "submission = create_submission_file(training_data, predicted_probs,data_submission_example, filename='./predictions/gbc-2y-exp-no-multicolinearity-lognormal-time-no-dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Model 1 on df2:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    795431\n",
      "           1       0.77      0.67      0.72      5401\n",
      "\n",
      "    accuracy                           1.00    800832\n",
      "   macro avg       0.88      0.83      0.86    800832\n",
      "weighted avg       1.00      1.00      1.00    800832\n",
      "\n",
      "Log Loss:\n",
      "0.008920031130332217\n",
      "Confusion Matrix:\n",
      "[[794339   1092]\n",
      " [  1781   3620]]\n",
      "Evaluation of Model 2 on df1:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    795363\n",
      "           1       0.76      0.65      0.70      5468\n",
      "\n",
      "    accuracy                           1.00    800831\n",
      "   macro avg       0.88      0.83      0.85    800831\n",
      "weighted avg       1.00      1.00      1.00    800831\n",
      "\n",
      "Log Loss:\n",
      "0.00899427534801325\n",
      "Confusion Matrix:\n",
      "[[794243   1120]\n",
      " [  1894   3574]]\n",
      "Centering probabilities...\n",
      "0.9933470937188168 0.011253558355142607 0.01479999999999997\n",
      "Saved file: ./predictions/xgboost-max_depth=3-2y-exp-no-multicolinearity-lognormal-time-no-dates.csv\n"
     ]
    }
   ],
   "source": [
    "predicted_probs = 'XGBOOST'\n",
    "probs = train_and_predict_two_halves(\n",
    "    training_data, \n",
    "    X_columns, \n",
    "    y_column, \n",
    "    model1= xgb.XGBClassifier(max_depth=3, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    model2= xgb.XGBClassifier(max_depth=3, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    ")\n",
    "training_data[predicted_probs] = probs\n",
    "submission = create_submission_file(training_data, predicted_probs, data_submission_example, filename='./predictions/xgboost-max_depth=3-2y-exp-no-multicolinearity-lognormal-time-no-dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Model 1 on df2:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    509478\n",
      "           1       0.60      0.39      0.47      5071\n",
      "\n",
      "    accuracy                           0.99    514549\n",
      "   macro avg       0.79      0.69      0.73    514549\n",
      "weighted avg       0.99      0.99      0.99    514549\n",
      "\n",
      "Log Loss:\n",
      "0.020154686217490652\n",
      "Confusion Matrix:\n",
      "[[508136   1342]\n",
      " [  3093   1978]]\n",
      "Evaluation of Model 2 on df1:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    509480\n",
      "           1       0.60      0.40      0.48      5069\n",
      "\n",
      "    accuracy                           0.99    514549\n",
      "   macro avg       0.80      0.70      0.74    514549\n",
      "weighted avg       0.99      0.99      0.99    514549\n",
      "\n",
      "Log Loss:\n",
      "0.02041090972095113\n",
      "Confusion Matrix:\n",
      "[[508121   1359]\n",
      " [  3032   2037]]\n",
      "Centering probabilities...\n",
      "1.0 0.0027108450789681583 0.014800000000000004\n",
      "Saved file: None\n",
      "Log loss: 0.02449711803718221\n",
      "LogisticRegression(max_iter=400, penalty=None, random_state=42)\n",
      "Logloss is  0.02449711803718221\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Model 1 on df2:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    509478\n",
      "           1       0.60      0.39      0.47      5071\n",
      "\n",
      "    accuracy                           0.99    514549\n",
      "   macro avg       0.79      0.69      0.73    514549\n",
      "weighted avg       0.99      0.99      0.99    514549\n",
      "\n",
      "Log Loss:\n",
      "0.020154686217490652\n",
      "Confusion Matrix:\n",
      "[[508136   1342]\n",
      " [  3093   1978]]\n",
      "Evaluation of Model 2 on df1:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    509480\n",
      "           1       0.60      0.40      0.48      5069\n",
      "\n",
      "    accuracy                           0.99    514549\n",
      "   macro avg       0.80      0.70      0.74    514549\n",
      "weighted avg       0.99      0.99      0.99    514549\n",
      "\n",
      "Log Loss:\n",
      "0.02041090972095113\n",
      "Confusion Matrix:\n",
      "[[508121   1359]\n",
      " [  3032   2037]]\n",
      "Centering probabilities...\n",
      "1.0 0.0027108450789681583 0.014800000000000004\n",
      "Saved file: None\n",
      "Log loss: 0.02449711803718221\n",
      "LogisticRegression(C=0.5, max_iter=400, penalty=None, random_state=42)\n",
      "Logloss is  0.02449711803718221\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "models =  [\n",
    "    (LogisticRegression(max_iter=400, penalty=None, C=1.0, random_state=42),\n",
    "     LogisticRegression(max_iter=400, penalty=None, C=1.0, random_state=42)),\n",
    "    \n",
    "   (LogisticRegression(max_iter=400, penalty=None, C=0.5, random_state=42),\n",
    "     LogisticRegression(max_iter=400, penalty=None, C=0.5, random_state=42)),\n",
    "]\n",
    "\n",
    "best_logloss = 100\n",
    "best_models = None\n",
    "for model in models:\n",
    "\n",
    "    logloss = test_model_2y_1y(\n",
    "        training_data, \n",
    "        X_columns, \n",
    "        'TARGET_EVENT_BINARY_1Y',\n",
    "        model1=model[0],\n",
    "        model2=model[1],\n",
    "    )\n",
    "    if logloss < best_logloss:\n",
    "        best_logloss = logloss\n",
    "        best_models = model\n",
    "    print(model[0])\n",
    "    print('Logloss is ', logloss)\n",
    "    print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_by_size = {}\n",
    "\n",
    "# Initialize a dictionary to hold covariance matrices by group size\n",
    "cov_matrices_by_size = {}\n",
    "\n",
    "# Group by 'BORROWER_ID' and iterate through the first 100 groups\n",
    "for idx, (name, group) in enumerate(training_data.groupby('BORROWER_ID')):\n",
    "    if idx % 100 == 0: \n",
    "        print('At index', idx)\n",
    "        \n",
    "    if idx == 50000:\n",
    "        break\n",
    "    \n",
    "    # Sort the group by CONTRACT_DATE_OF_LOAN_AGREEMENT\n",
    "    group = group.sort_values(by='CONTRACT_DATE_OF_LOAN_AGREEMENT')\n",
    "    \n",
    "    group_size = len(group)\n",
    "    if group_size not in groups_by_size:\n",
    "        groups_by_size[group_size] = []\n",
    "        \n",
    "    groups_by_size[group_size].append(group)\n",
    "\n",
    "# Initialize a new dictionary to hold the merged DataFrames by size\n",
    "merged_groups_by_size = {}\n",
    "\n",
    "for size, dfs in groups_by_size.items():\n",
    "    # Merge all DataFrames of the same size into a single DataFrame\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    # Store the merged DataFrame in the new dictionary\n",
    "    merged_groups_by_size[size] = merged_df\n",
    "\n",
    "# Now, merged_groups_by_size contains the merged DataFrames categorized by group size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\takat\\AppData\\Local\\Temp\\ipykernel_13332\\2391036120.py:12: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  cov_matrix = np.cov(merged_df['TARGET_EVENT_BINARY'].values.reshape(size,-1))\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2748: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\Users\\takat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2748: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Your code to generate groups_by_size and merged_groups_by_size\n",
    "\n",
    "cov_matrices_by_size = {}\n",
    "\n",
    "for size, merged_df in merged_groups_by_size.items():\n",
    "    if size > 1:  # Covariance matrix for single-element arrays doesn't make sense\n",
    "        try:\n",
    "            cov_matrix = np.cov(merged_df['TARGET_EVENT_BINARY'].values.reshape(size,-1))\n",
    "            if not np.isnan(cov_matrix).any():  # Check for NaN values\n",
    "                cov_matrices_by_size[size] = cov_matrix.tolist()  # Convert numpy array to list for JSON serialization\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while calculating the covariance matrix for size {size}: {e}\")\n",
    "\n",
    "# Ensure that the dictionary contains only JSON-serializable items\n",
    "serializable_cov_matrices_by_size = {k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in cov_matrices_by_size.items()}\n",
    "\n",
    "# Save to JSON\n",
    "with open('./data/cov_matrices_by_size.json', 'w') as f:\n",
    "    json.dump(serializable_cov_matrices_by_size, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: [[0.005868589752778426, -3.2357827075016176e-05],\n",
       "  [-3.2357827075016176e-05, 0.005451716043799604]],\n",
       " 3: [[0.007337144521720125, -2.6009019928111114e-05, -2.6009019928111046e-05],\n",
       "  [-2.6009019928111114e-05, 0.003507502116019551, -1.2385247584814735e-05],\n",
       "  [-2.6009019928111046e-05, -1.2385247584814735e-05, 0.003507502116019557]],\n",
       " 4: [[0.0023364421983921592,\n",
       "   -1.3695440787761734e-05,\n",
       "   -5.478176315104693e-06,\n",
       "   -8.217264472657053e-06],\n",
       "  [-1.3695440787761734e-05,\n",
       "   0.005820562334798763,\n",
       "   -1.3695440787761748e-05,\n",
       "   -2.054316118164268e-05],\n",
       "  [-5.478176315104693e-06,\n",
       "   -1.3695440787761748e-05,\n",
       "   0.0023364421983921558,\n",
       "   -8.217264472657026e-06],\n",
       "  [-8.217264472657053e-06,\n",
       "   -2.054316118164268e-05,\n",
       "   -8.217264472657026e-06,\n",
       "   0.003500554665351909]],\n",
       " 8: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       " 5: [[0.013199554069119256,\n",
       "   -0.00022296544035674517,\n",
       "   -0.00013377926421404707,\n",
       "   -4.4593088071349e-05,\n",
       "   -8.918617614269807e-05],\n",
       "  [-0.00022296544035674517,\n",
       "   0.016443701226309938,\n",
       "   -0.0001672240802675587,\n",
       "   -5.574136008918631e-05,\n",
       "   -0.00011148272017837272],\n",
       "  [-0.00013377926421404707,\n",
       "   -0.0001672240802675587,\n",
       "   0.009933110367893017,\n",
       "   -3.344481605351181e-05,\n",
       "   -6.688963210702361e-05],\n",
       "  [-4.4593088071349e-05,\n",
       "   -5.574136008918631e-05,\n",
       "   -3.344481605351181e-05,\n",
       "   0.003333333333333323,\n",
       "   -2.2296544035674506e-05],\n",
       "  [-8.918617614269807e-05,\n",
       "   -0.00011148272017837272,\n",
       "   -6.688963210702361e-05,\n",
       "   -2.2296544035674506e-05,\n",
       "   0.006644370122630978]],\n",
       " 6: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.020197769829581318, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       " 7: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.028571428571428577, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       " 9: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.11111111111111109, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_matrices_by_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "import json\n",
    "\n",
    "with open('cov_matrices_by_size.json', 'w') as f:\n",
    "    json.dump(cov_matrices_by_size, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
